{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to read images from FVC2006 DB2_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Sample precision and recall values for Algorithm A and Algorithm B\n",
    "precision_algoA = [0.95, 0.92, 0.89, 0.86, 0.82, 0.78]\n",
    "recall_algoA = [0.65, 0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "\n",
    "precision_algoB = [0.88, 0.85, 0.80, 0.75, 0.72, 0.68]\n",
    "recall_algoB = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "\n",
    "# Plot the precision-recall curve for Algorithm A\n",
    "plt.plot(recall_algoA, precision_algoA, label='Algorithm A')\n",
    "\n",
    "# Plot the precision-recall curve for Algorithm B\n",
    "plt.plot(recall_algoB, precision_algoB, label='Algorithm B')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "# Get the path to the images\n",
    "images_path = '/home/rs/21CS91R01/research/2023_ICVGIP-Code/datasets/FVC2006DB2_A'\n",
    "\n",
    "# List the images in the folder\n",
    "images = os.listdir(images_path)\n",
    "\n",
    "# # Iterate over the images\n",
    "# for image in images:\n",
    "#   # Read the image\n",
    "#   image = cv2.imread(os.path.join(images_path, image))\n",
    "\n",
    "#   # Display the image\n",
    "#   # cv2_imshow(image)\n",
    "#   cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 18:00:55.218365: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def random_distortion(image):\n",
    "    rows, cols = image.shape[:2]\n",
    "    # Define the distortion parameters\n",
    "    random_scale = np.random.uniform(0.8, 1.2)\n",
    "    random_angle = np.random.uniform(-15, 15)\n",
    "    random_translation = np.random.uniform(-10, 10, (2,))\n",
    "    # Apply the distortion\n",
    "    M = cv2.getRotationMatrix2D((cols/2, rows/2), random_angle, random_scale)\n",
    "    M[:, 2] += random_translation\n",
    "    distorted_image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    # print(distorted_image.shape)\n",
    "    return distorted_image\n",
    "\n",
    "def random_gaussian_blurring(image):\n",
    "    # Generate random blurring parameters\n",
    "    random_sigma = np.random.uniform(0, 2.0)\n",
    "    # Apply Gaussian blur\n",
    "    blurred_image = cv2.GaussianBlur(image, (0, 0), random_sigma)\n",
    "    # print(blurred_image.shape)\n",
    "    return blurred_image\n",
    "\n",
    "def random_rotation(image):\n",
    "    rows, cols = image.shape[:2]\n",
    "    # Generate random rotation angle\n",
    "    random_angle = np.random.uniform(-30, 30)\n",
    "    # Apply rotation\n",
    "    M = cv2.getRotationMatrix2D((cols/2, rows/2), random_angle, 1)\n",
    "    rotated_image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    # print(rotated_image.shape)\n",
    "    return rotated_image\n",
    "\n",
    "def random_scaling(image):\n",
    "    rows, cols = image.shape[:2]\n",
    "    # Generate random scaling factor\n",
    "    random_scale = np.random.uniform(0.8, 1.2)\n",
    "    # Apply scaling\n",
    "    scaled_image = cv2.resize(image, (int(cols*random_scale), int(rows*random_scale)))\n",
    "    scaled_image = cv2.resize(scaled_image, (cols, rows))\n",
    "    # print(scaled_image.shape)\n",
    "    return scaled_image\n",
    "\n",
    "def random_contrast(image):\n",
    "    # Generate random contrast factor\n",
    "    random_factor = np.random.uniform(0.5, 1.5)\n",
    "    # Apply contrast adjustment\n",
    "    adjusted_image = np.clip(image * random_factor, 0, 255).astype(np.uint8)\n",
    "    # print(adjusted_image.shape)\n",
    "    return adjusted_image\n",
    "\n",
    "def random_noise(image):\n",
    "    # Generate random noise parameters\n",
    "    random_mean = 0\n",
    "    random_std = np.random.uniform(0, 30)\n",
    "    # Apply Gaussian noise\n",
    "    noise = np.random.normal(random_mean, random_std, image.shape).astype(np.uint8)\n",
    "    noisy_image = np.clip(image + noise, 0, 255).astype(np.uint8)\n",
    "    # print(noisy_image.shape)\n",
    "    return noisy_image\n",
    "\n",
    "def random_morphology(image):\n",
    "    # Generate random kernel size for morphology operations\n",
    "    kernel_size = np.random.randint(2, 7)\n",
    "    # Generate random morphology operation\n",
    "    morph_op = np.random.choice([cv2.MORPH_OPEN, cv2.MORPH_CLOSE])\n",
    "    # Create the kernel\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "    # Apply morphology operation\n",
    "    morph_image = cv2.morphologyEx(image, morph_op, kernel)\n",
    "    # print(morph_image.shape)\n",
    "    return morph_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12_9\n",
      "12\n",
      "9\n",
      "94_12\n",
      "94\n",
      "12\n",
      "70_6\n",
      "70\n",
      "6\n",
      "35_6\n",
      "35\n",
      "6\n",
      "95_1\n",
      "95\n",
      "1\n",
      "128_3\n",
      "128\n",
      "3\n",
      "88_10\n",
      "88\n",
      "10\n",
      "23_11\n",
      "23\n",
      "11\n",
      "14_10\n",
      "14\n",
      "10\n",
      "100_4\n",
      "100\n",
      "4\n",
      "19_1\n",
      "19\n",
      "1\n",
      "121_11\n",
      "121\n",
      "11\n",
      "75_11\n",
      "75\n",
      "11\n",
      "59_5\n",
      "59\n",
      "5\n",
      "51_4\n",
      "51\n",
      "4\n",
      "137_12\n",
      "137\n",
      "12\n",
      "75_8\n",
      "75\n",
      "8\n",
      "133_10\n",
      "133\n",
      "10\n",
      "93_9\n",
      "93\n",
      "9\n",
      "51_10\n",
      "51\n",
      "10\n",
      "125_6\n",
      "125\n",
      "6\n",
      "38_11\n",
      "38\n",
      "11\n",
      "90_7\n",
      "90\n",
      "7\n",
      "9_11\n",
      "9\n",
      "11\n",
      "57_10\n",
      "57\n",
      "10\n",
      "62_1\n",
      "62\n",
      "1\n",
      "50_2\n",
      "50\n",
      "2\n",
      "78_11\n",
      "78\n",
      "11\n",
      "14_3\n",
      "14\n",
      "3\n",
      "99_6\n",
      "99\n",
      "6\n",
      "70_8\n",
      "70\n",
      "8\n",
      "1_7\n",
      "1\n",
      "7\n",
      "2_9\n",
      "2\n",
      "9\n",
      "42_12\n",
      "42\n",
      "12\n",
      "102_11\n",
      "102\n",
      "11\n",
      "82_2\n",
      "82\n",
      "2\n",
      "106_3\n",
      "106\n",
      "3\n",
      "136_12\n",
      "136\n",
      "12\n",
      "95_2\n",
      "95\n",
      "2\n",
      "59_2\n",
      "59\n",
      "2\n",
      "46_4\n",
      "46\n",
      "4\n",
      "19_10\n",
      "19\n",
      "10\n",
      "111_5\n",
      "111\n",
      "5\n",
      "21_1\n",
      "21\n",
      "1\n",
      "130_8\n",
      "130\n",
      "8\n",
      "26_3\n",
      "26\n",
      "3\n",
      "115_4\n",
      "115\n",
      "4\n",
      "127_6\n",
      "127\n",
      "6\n",
      "134_1\n",
      "134\n",
      "1\n",
      "138_6\n",
      "138\n",
      "6\n",
      "79_9\n",
      "79\n",
      "9\n",
      "73_8\n",
      "73\n",
      "8\n",
      "132_4\n",
      "132\n",
      "4\n",
      "78_7\n",
      "78\n",
      "7\n",
      "127_9\n",
      "127\n",
      "9\n",
      "134_2\n",
      "134\n",
      "2\n",
      "40_5\n",
      "40\n",
      "5\n",
      "41_5\n",
      "41\n",
      "5\n",
      "39_8\n",
      "39\n",
      "8\n",
      "31_2\n",
      "31\n",
      "2\n",
      "10_1\n",
      "10\n",
      "1\n",
      "79_4\n",
      "79\n",
      "4\n",
      "135_9\n",
      "135\n",
      "9\n",
      "96_2\n",
      "96\n",
      "2\n",
      "96_11\n",
      "96\n",
      "11\n",
      "106_1\n",
      "106\n",
      "1\n",
      "48_6\n",
      "48\n",
      "6\n",
      "97_1\n",
      "97\n",
      "1\n",
      "137_1\n",
      "137\n",
      "1\n",
      "17_1\n",
      "17\n",
      "1\n",
      "24_4\n",
      "24\n",
      "4\n",
      "124_11\n",
      "124\n",
      "11\n",
      "101_3\n",
      "101\n",
      "3\n",
      "93_1\n",
      "93\n",
      "1\n",
      "42_7\n",
      "42\n",
      "7\n",
      "92_5\n",
      "92\n",
      "5\n",
      "57_7\n",
      "57\n",
      "7\n",
      "27_2\n",
      "27\n",
      "2\n",
      "46_10\n",
      "46\n",
      "10\n",
      "128_8\n",
      "128\n",
      "8\n",
      "29_7\n",
      "29\n",
      "7\n",
      "49_10\n",
      "49\n",
      "10\n",
      "120_1\n",
      "120\n",
      "1\n",
      "129_1\n",
      "129\n",
      "1\n",
      "82_3\n",
      "82\n",
      "3\n",
      "56_7\n",
      "56\n",
      "7\n",
      "84_1\n",
      "84\n",
      "1\n",
      "126_7\n",
      "126\n",
      "7\n",
      "88_4\n",
      "88\n",
      "4\n",
      "16_2\n",
      "16\n",
      "2\n",
      "46_8\n",
      "46\n",
      "8\n",
      "118_6\n",
      "118\n",
      "6\n",
      "105_12\n",
      "105\n",
      "12\n",
      "105_1\n",
      "105\n",
      "1\n",
      "29_2\n",
      "29\n",
      "2\n",
      "59_1\n",
      "59\n",
      "1\n",
      "72_12\n",
      "72\n",
      "12\n",
      "89_3\n",
      "89\n",
      "3\n",
      "4_5\n",
      "4\n",
      "5\n",
      "125_7\n",
      "125\n",
      "7\n",
      "105_8\n",
      "105\n",
      "8\n",
      "55_6\n",
      "55\n",
      "6\n",
      "33_2\n",
      "33\n",
      "2\n",
      "64_4\n",
      "64\n",
      "4\n",
      "36_12\n",
      "36\n",
      "12\n",
      "81_10\n",
      "81\n",
      "10\n",
      "124_12\n",
      "124\n",
      "12\n",
      "24_11\n",
      "24\n",
      "11\n",
      "104_1\n",
      "104\n",
      "1\n",
      "4_1\n",
      "4\n",
      "1\n",
      "21_11\n",
      "21\n",
      "11\n",
      "6_12\n",
      "6\n",
      "12\n",
      "130_2\n",
      "130\n",
      "2\n",
      "93_6\n",
      "93\n",
      "6\n",
      "35_12\n",
      "35\n",
      "12\n",
      "9_4\n",
      "9\n",
      "4\n",
      "8_4\n",
      "8\n",
      "4\n",
      "132_12\n",
      "132\n",
      "12\n",
      "118_3\n",
      "118\n",
      "3\n",
      "66_4\n",
      "66\n",
      "4\n",
      "54_7\n",
      "54\n",
      "7\n",
      "113_4\n",
      "113\n",
      "4\n",
      "32_5\n",
      "32\n",
      "5\n",
      "67_4\n",
      "67\n",
      "4\n",
      "78_10\n",
      "78\n",
      "10\n",
      "18_1\n",
      "18\n",
      "1\n",
      "132_10\n",
      "132\n",
      "10\n",
      "48_7\n",
      "48\n",
      "7\n",
      "110_10\n",
      "110\n",
      "10\n",
      "35_2\n",
      "35\n",
      "2\n",
      "106_2\n",
      "106\n",
      "2\n",
      "15_12\n",
      "15\n",
      "12\n",
      "22_5\n",
      "22\n",
      "5\n",
      "83_12\n",
      "83\n",
      "12\n",
      "29_11\n",
      "29\n",
      "11\n",
      "1_9\n",
      "1\n",
      "9\n",
      "32_11\n",
      "32\n",
      "11\n",
      "99_4\n",
      "99\n",
      "4\n",
      "126_1\n",
      "126\n",
      "1\n",
      "9_3\n",
      "9\n",
      "3\n",
      "95_9\n",
      "95\n",
      "9\n",
      "86_5\n",
      "86\n",
      "5\n",
      "139_7\n",
      "139\n",
      "7\n",
      "90_1\n",
      "90\n",
      "1\n",
      "104_10\n",
      "104\n",
      "10\n",
      "24_10\n",
      "24\n",
      "10\n",
      "122_3\n",
      "122\n",
      "3\n",
      "59_3\n",
      "59\n",
      "3\n",
      "140_2\n",
      "140\n",
      "2\n",
      "103_7\n",
      "103\n",
      "7\n",
      "11_3\n",
      "11\n",
      "3\n",
      "67_11\n",
      "67\n",
      "11\n",
      "128_4\n",
      "128\n",
      "4\n",
      "66_9\n",
      "66\n",
      "9\n",
      "111_12\n",
      "111\n",
      "12\n",
      "50_9\n",
      "50\n",
      "9\n",
      "102_10\n",
      "102\n",
      "10\n",
      "49_7\n",
      "49\n",
      "7\n",
      "127_12\n",
      "127\n",
      "12\n",
      "39_10\n",
      "39\n",
      "10\n",
      "65_2\n",
      "65\n",
      "2\n",
      "87_8\n",
      "87\n",
      "8\n",
      "94_11\n",
      "94\n",
      "11\n",
      "40_2\n",
      "40\n",
      "2\n",
      "129_10\n",
      "129\n",
      "10\n",
      "81_3\n",
      "81\n",
      "3\n",
      "38_9\n",
      "38\n",
      "9\n",
      "41_6\n",
      "41\n",
      "6\n",
      "64_2\n",
      "64\n",
      "2\n",
      "64_6\n",
      "64\n",
      "6\n",
      "67_3\n",
      "67\n",
      "3\n",
      "6_8\n",
      "6\n",
      "8\n",
      "81_8\n",
      "81\n",
      "8\n",
      "13_4\n",
      "13\n",
      "4\n",
      "24_6\n",
      "24\n",
      "6\n",
      "113_10\n",
      "113\n",
      "10\n",
      "101_5\n",
      "101\n",
      "5\n",
      "122_11\n",
      "122\n",
      "11\n",
      "82_8\n",
      "82\n",
      "8\n",
      "28_1\n",
      "28\n",
      "1\n",
      "106_6\n",
      "106\n",
      "6\n",
      "58_7\n",
      "58\n",
      "7\n",
      "16_11\n",
      "16\n",
      "11\n",
      "21_6\n",
      "21\n",
      "6\n",
      "54_11\n",
      "54\n",
      "11\n",
      "127_7\n",
      "127\n",
      "7\n",
      "44_11\n",
      "44\n",
      "11\n",
      "64_3\n",
      "64\n",
      "3\n",
      "57_5\n",
      "57\n",
      "5\n",
      "103_10\n",
      "103\n",
      "10\n",
      "136_6\n",
      "136\n",
      "6\n",
      "132_6\n",
      "132\n",
      "6\n",
      "133_1\n",
      "133\n",
      "1\n",
      "25_4\n",
      "25\n",
      "4\n",
      "140_8\n",
      "140\n",
      "8\n",
      "41_1\n",
      "41\n",
      "1\n",
      "5_8\n",
      "5\n",
      "8\n",
      "68_12\n",
      "68\n",
      "12\n",
      "109_9\n",
      "109\n",
      "9\n",
      "40_6\n",
      "40\n",
      "6\n",
      "102_9\n",
      "102\n",
      "9\n",
      "3_5\n",
      "3\n",
      "5\n",
      "127_2\n",
      "127\n",
      "2\n",
      "51_5\n",
      "51\n",
      "5\n",
      "43_7\n",
      "43\n",
      "7\n",
      "87_3\n",
      "87\n",
      "3\n",
      "4_8\n",
      "4\n",
      "8\n",
      "137_6\n",
      "137\n",
      "6\n",
      "100_12\n",
      "100\n",
      "12\n",
      "26_7\n",
      "26\n",
      "7\n",
      "72_3\n",
      "72\n",
      "3\n",
      "106_7\n",
      "106\n",
      "7\n",
      "123_3\n",
      "123\n",
      "3\n",
      "92_2\n",
      "92\n",
      "2\n",
      "72_7\n",
      "72\n",
      "7\n",
      "80_1\n",
      "80\n",
      "1\n",
      "44_3\n",
      "44\n",
      "3\n",
      "134_7\n",
      "134\n",
      "7\n",
      "36_6\n",
      "36\n",
      "6\n",
      "85_8\n",
      "85\n",
      "8\n",
      "17_3\n",
      "17\n",
      "3\n",
      "61_11\n",
      "61\n",
      "11\n",
      "41_3\n",
      "41\n",
      "3\n",
      "132_5\n",
      "132\n",
      "5\n",
      "127_11\n",
      "127\n",
      "11\n",
      "26_8\n",
      "26\n",
      "8\n",
      "39_9\n",
      "39\n",
      "9\n",
      "125_5\n",
      "125\n",
      "5\n",
      "76_10\n",
      "76\n",
      "10\n",
      "18_3\n",
      "18\n",
      "3\n",
      "132_9\n",
      "132\n",
      "9\n",
      "57_1\n",
      "57\n",
      "1\n",
      "74_12\n",
      "74\n",
      "12\n",
      "88_5\n",
      "88\n",
      "5\n",
      "120_5\n",
      "120\n",
      "5\n",
      "46_2\n",
      "46\n",
      "2\n",
      "80_6\n",
      "80\n",
      "6\n",
      "137_3\n",
      "137\n",
      "3\n",
      "132_11\n",
      "132\n",
      "11\n",
      "103_1\n",
      "103\n",
      "1\n",
      "130_6\n",
      "130\n",
      "6\n",
      "129_12\n",
      "129\n",
      "12\n",
      "1_5\n",
      "1\n",
      "5\n",
      "5_3\n",
      "5\n",
      "3\n",
      "5_6\n",
      "5\n",
      "6\n",
      "92_4\n",
      "92\n",
      "4\n",
      "44_5\n",
      "44\n",
      "5\n",
      "50_8\n",
      "50\n",
      "8\n",
      "37_6\n",
      "37\n",
      "6\n",
      "39_4\n",
      "39\n",
      "4\n",
      "50_3\n",
      "50\n",
      "3\n",
      "62_4\n",
      "62\n",
      "4\n",
      "107_8\n",
      "107\n",
      "8\n",
      "24_7\n",
      "24\n",
      "7\n",
      "63_2\n",
      "63\n",
      "2\n",
      "62_6\n",
      "62\n",
      "6\n",
      "61_8\n",
      "61\n",
      "8\n",
      "108_3\n",
      "108\n",
      "3\n",
      "24_1\n",
      "24\n",
      "1\n",
      "75_1\n",
      "75\n",
      "1\n",
      "22_7\n",
      "22\n",
      "7\n",
      "22_11\n",
      "22\n",
      "11\n",
      "10_8\n",
      "10\n",
      "8\n",
      "80_10\n",
      "80\n",
      "10\n",
      "53_8\n",
      "53\n",
      "8\n",
      "18_8\n",
      "18\n",
      "8\n",
      "123_2\n",
      "123\n",
      "2\n",
      "48_10\n",
      "48\n",
      "10\n",
      "43_6\n",
      "43\n",
      "6\n",
      "43_12\n",
      "43\n",
      "12\n",
      "56_11\n",
      "56\n",
      "11\n",
      "7_7\n",
      "7\n",
      "7\n",
      "60_6\n",
      "60\n",
      "6\n",
      "19_6\n",
      "19\n",
      "6\n",
      "44_8\n",
      "44\n",
      "8\n",
      "88_3\n",
      "88\n",
      "3\n",
      "23_12\n",
      "23\n",
      "12\n",
      "128_12\n",
      "128\n",
      "12\n",
      "79_5\n",
      "79\n",
      "5\n",
      "35_5\n",
      "35\n",
      "5\n",
      "104_3\n",
      "104\n",
      "3\n",
      "14_12\n",
      "14\n",
      "12\n",
      "44_12\n",
      "44\n",
      "12\n",
      "65_3\n",
      "65\n",
      "3\n",
      "117_4\n",
      "117\n",
      "4\n",
      "34_8\n",
      "34\n",
      "8\n",
      "85_4\n",
      "85\n",
      "4\n",
      "89_2\n",
      "89\n",
      "2\n",
      "81_12\n",
      "81\n",
      "12\n",
      "124_9\n",
      "124\n",
      "9\n",
      "22_8\n",
      "22\n",
      "8\n",
      "118_1\n",
      "118\n",
      "1\n",
      "28_9\n",
      "28\n",
      "9\n",
      "79_7\n",
      "79\n",
      "7\n",
      "27_10\n",
      "27\n",
      "10\n",
      "28_4\n",
      "28\n",
      "4\n",
      "51_3\n",
      "51\n",
      "3\n",
      "137_4\n",
      "137\n",
      "4\n",
      "5_2\n",
      "5\n",
      "2\n",
      "88_12\n",
      "88\n",
      "12\n",
      "6_2\n",
      "6\n",
      "2\n",
      "128_11\n",
      "128\n",
      "11\n",
      "133_12\n",
      "133\n",
      "12\n",
      "32_12\n",
      "32\n",
      "12\n",
      "127_5\n",
      "127\n",
      "5\n",
      "13_1\n",
      "13\n",
      "1\n",
      "120_12\n",
      "120\n",
      "12\n",
      "40_10\n",
      "40\n",
      "10\n",
      "23_5\n",
      "23\n",
      "5\n",
      "60_8\n",
      "60\n",
      "8\n",
      "9_9\n",
      "9\n",
      "9\n",
      "125_3\n",
      "125\n",
      "3\n",
      "95_10\n",
      "95\n",
      "10\n",
      "98_10\n",
      "98\n",
      "10\n",
      "127_4\n",
      "127\n",
      "4\n",
      "17_11\n",
      "17\n",
      "11\n",
      "137_5\n",
      "137\n",
      "5\n",
      "96_7\n",
      "96\n",
      "7\n",
      "83_5\n",
      "83\n",
      "5\n",
      "69_5\n",
      "69\n",
      "5\n",
      "81_2\n",
      "81\n",
      "2\n",
      "33_10\n",
      "33\n",
      "10\n",
      "128_7\n",
      "128\n",
      "7\n",
      "95_8\n",
      "95\n",
      "8\n",
      "56_6\n",
      "56\n",
      "6\n",
      "135_11\n",
      "135\n",
      "11\n",
      "41_8\n",
      "41\n",
      "8\n",
      "87_1\n",
      "87\n",
      "1\n",
      "82_9\n",
      "82\n",
      "9\n",
      "47_4\n",
      "47\n",
      "4\n",
      "8_5\n",
      "8\n",
      "5\n",
      "114_2\n",
      "114\n",
      "2\n",
      "36_2\n",
      "36\n",
      "2\n",
      "109_1\n",
      "109\n",
      "1\n",
      "108_6\n",
      "108\n",
      "6\n",
      "87_4\n",
      "87\n",
      "4\n",
      "18_10\n",
      "18\n",
      "10\n",
      "110_7\n",
      "110\n",
      "7\n",
      "140_5\n",
      "140\n",
      "5\n",
      "136_8\n",
      "136\n",
      "8\n",
      "119_4\n",
      "119\n",
      "4\n",
      "52_9\n",
      "52\n",
      "9\n",
      "16_12\n",
      "16\n",
      "12\n",
      "104_9\n",
      "104\n",
      "9\n",
      "115_12\n",
      "115\n",
      "12\n",
      "115_5\n",
      "115\n",
      "5\n",
      "80_12\n",
      "80\n",
      "12\n",
      "117_9\n",
      "117\n",
      "9\n",
      "93_7\n",
      "93\n",
      "7\n",
      "62_9\n",
      "62\n",
      "9\n",
      "106_5\n",
      "106\n",
      "5\n",
      "120_11\n",
      "120\n",
      "11\n",
      "13_12\n",
      "13\n",
      "12\n",
      "107_7\n",
      "107\n",
      "7\n",
      "128_6\n",
      "128\n",
      "6\n",
      "91_3\n",
      "91\n",
      "3\n",
      "58_10\n",
      "58\n",
      "10\n",
      "80_5\n",
      "80\n",
      "5\n",
      "120_4\n",
      "120\n",
      "4\n",
      "37_8\n",
      "37\n",
      "8\n",
      "56_2\n",
      "56\n",
      "2\n",
      "100_2\n",
      "100\n",
      "2\n",
      "96_3\n",
      "96\n",
      "3\n",
      "85_6\n",
      "85\n",
      "6\n",
      "29_4\n",
      "29\n",
      "4\n",
      "19_3\n",
      "19\n",
      "3\n",
      "78_3\n",
      "78\n",
      "3\n",
      "131_9\n",
      "131\n",
      "9\n",
      "110_2\n",
      "110\n",
      "2\n",
      "18_12\n",
      "18\n",
      "12\n",
      "8_9\n",
      "8\n",
      "9\n",
      "105_5\n",
      "105\n",
      "5\n",
      "65_7\n",
      "65\n",
      "7\n",
      "63_9\n",
      "63\n",
      "9\n",
      "128_9\n",
      "128\n",
      "9\n",
      "12_8\n",
      "12\n",
      "8\n",
      "38_7\n",
      "38\n",
      "7\n",
      "13_7\n",
      "13\n",
      "7\n",
      "11_6\n",
      "11\n",
      "6\n",
      "102_6\n",
      "102\n",
      "6\n",
      "16_7\n",
      "16\n",
      "7\n",
      "84_9\n",
      "84\n",
      "9\n",
      "26_2\n",
      "26\n",
      "2\n",
      "21_9\n",
      "21\n",
      "9\n",
      "132_2\n",
      "132\n",
      "2\n",
      "36_4\n",
      "36\n",
      "4\n",
      "139_6\n",
      "139\n",
      "6\n",
      "33_7\n",
      "33\n",
      "7\n",
      "28_10\n",
      "28\n",
      "10\n",
      "42_6\n",
      "42\n",
      "6\n",
      "9_5\n",
      "9\n",
      "5\n",
      "55_2\n",
      "55\n",
      "2\n",
      "113_6\n",
      "113\n",
      "6\n",
      "118_4\n",
      "118\n",
      "4\n",
      "128_5\n",
      "128\n",
      "5\n",
      "65_10\n",
      "65\n",
      "10\n",
      "107_10\n",
      "107\n",
      "10\n",
      "34_7\n",
      "34\n",
      "7\n",
      "94_10\n",
      "94\n",
      "10\n",
      "53_2\n",
      "53\n",
      "2\n",
      "22_10\n",
      "22\n",
      "10\n",
      "48_2\n",
      "48\n",
      "2\n",
      "92_6\n",
      "92\n",
      "6\n",
      "140_4\n",
      "140\n",
      "4\n",
      "16_4\n",
      "16\n",
      "4\n",
      "7_3\n",
      "7\n",
      "3\n",
      "29_8\n",
      "29\n",
      "8\n",
      "90_10\n",
      "90\n",
      "10\n",
      "69_4\n",
      "69\n",
      "4\n",
      "66_5\n",
      "66\n",
      "5\n",
      "9_8\n",
      "9\n",
      "8\n",
      "10_7\n",
      "10\n",
      "7\n",
      "59_9\n",
      "59\n",
      "9\n",
      "130_7\n",
      "130\n",
      "7\n",
      "43_5\n",
      "43\n",
      "5\n",
      "120_9\n",
      "120\n",
      "9\n",
      "117_12\n",
      "117\n",
      "12\n",
      "17_7\n",
      "17\n",
      "7\n",
      "81_11\n",
      "81\n",
      "11\n",
      "4_4\n",
      "4\n",
      "4\n",
      "20_2\n",
      "20\n",
      "2\n",
      "93_5\n",
      "93\n",
      "5\n",
      "119_5\n",
      "119\n",
      "5\n",
      "20_1\n",
      "20\n",
      "1\n",
      "91_9\n",
      "91\n",
      "9\n",
      "6_4\n",
      "6\n",
      "4\n",
      "68_10\n",
      "68\n",
      "10\n",
      "100_1\n",
      "100\n",
      "1\n",
      "124_10\n",
      "124\n",
      "10\n",
      "11_1\n",
      "11\n",
      "1\n",
      "56_5\n",
      "56\n",
      "5\n",
      "7_2\n",
      "7\n",
      "2\n",
      "126_9\n",
      "126\n",
      "9\n",
      "73_10\n",
      "73\n",
      "10\n",
      "71_6\n",
      "71\n",
      "6\n",
      "31_9\n",
      "31\n",
      "9\n",
      "94_2\n",
      "94\n",
      "2\n",
      "98_5\n",
      "98\n",
      "5\n",
      "96_9\n",
      "96\n",
      "9\n",
      "104_2\n",
      "104\n",
      "2\n",
      "46_9\n",
      "46\n",
      "9\n",
      "121_1\n",
      "121\n",
      "1\n",
      "129_2\n",
      "129\n",
      "2\n",
      "61_3\n",
      "61\n",
      "3\n",
      "12_5\n",
      "12\n",
      "5\n",
      "70_9\n",
      "70\n",
      "9\n",
      "44_2\n",
      "44\n",
      "2\n",
      "124_1\n",
      "124\n",
      "1\n",
      "70_10\n",
      "70\n",
      "10\n",
      "76_6\n",
      "76\n",
      "6\n",
      "45_6\n",
      "45\n",
      "6\n",
      "62_12\n",
      "62\n",
      "12\n",
      "36_10\n",
      "36\n",
      "10\n",
      "15_6\n",
      "15\n",
      "6\n",
      "4_2\n",
      "4\n",
      "2\n",
      "87_11\n",
      "87\n",
      "11\n",
      "112_11\n",
      "112\n",
      "11\n",
      "26_5\n",
      "26\n",
      "5\n",
      "97_6\n",
      "97\n",
      "6\n",
      "72_11\n",
      "72\n",
      "11\n",
      "136_10\n",
      "136\n",
      "10\n",
      "101_8\n",
      "101\n",
      "8\n",
      "137_8\n",
      "137\n",
      "8\n",
      "35_4\n",
      "35\n",
      "4\n",
      "82_11\n",
      "82\n",
      "11\n",
      "102_5\n",
      "102\n",
      "5\n",
      "28_7\n",
      "28\n",
      "7\n",
      "125_12\n",
      "125\n",
      "12\n",
      "47_7\n",
      "47\n",
      "7\n",
      "30_11\n",
      "30\n",
      "11\n",
      "100_6\n",
      "100\n",
      "6\n",
      "75_9\n",
      "75\n",
      "9\n",
      "114_9\n",
      "114\n",
      "9\n",
      "54_3\n",
      "54\n",
      "3\n",
      "97_8\n",
      "97\n",
      "8\n",
      "66_1\n",
      "66\n",
      "1\n",
      "130_4\n",
      "130\n",
      "4\n",
      "104_7\n",
      "104\n",
      "7\n",
      "59_12\n",
      "59\n",
      "12\n",
      "42_1\n",
      "42\n",
      "1\n",
      "87_2\n",
      "87\n",
      "2\n",
      "2_10\n",
      "2\n",
      "10\n",
      "54_4\n",
      "54\n",
      "4\n",
      "137_2\n",
      "137\n",
      "2\n",
      "108_12\n",
      "108\n",
      "12\n",
      "121_4\n",
      "121\n",
      "4\n",
      "116_4\n",
      "116\n",
      "4\n",
      "89_8\n",
      "89\n",
      "8\n",
      "53_7\n",
      "53\n",
      "7\n",
      "86_8\n",
      "86\n",
      "8\n",
      "61_10\n",
      "61\n",
      "10\n",
      "133_5\n",
      "133\n",
      "5\n",
      "6_11\n",
      "6\n",
      "11\n",
      "58_6\n",
      "58\n",
      "6\n",
      "99_10\n",
      "99\n",
      "10\n",
      "138_10\n",
      "138\n",
      "10\n",
      "35_11\n",
      "35\n",
      "11\n",
      "104_8\n",
      "104\n",
      "8\n",
      "13_10\n",
      "13\n",
      "10\n",
      "61_1\n",
      "61\n",
      "1\n",
      "43_9\n",
      "43\n",
      "9\n",
      "39_11\n",
      "39\n",
      "11\n",
      "136_9\n",
      "136\n",
      "9\n",
      "47_1\n",
      "47\n",
      "1\n",
      "139_11\n",
      "139\n",
      "11\n",
      "44_6\n",
      "44\n",
      "6\n",
      "33_6\n",
      "33\n",
      "6\n",
      "109_4\n",
      "109\n",
      "4\n",
      "4_11\n",
      "4\n",
      "11\n",
      "101_4\n",
      "101\n",
      "4\n",
      "121_5\n",
      "121\n",
      "5\n",
      "63_11\n",
      "63\n",
      "11\n",
      "7_12\n",
      "7\n",
      "12\n",
      "51_7\n",
      "51\n",
      "7\n",
      "100_3\n",
      "100\n",
      "3\n",
      "134_8\n",
      "134\n",
      "8\n",
      "43_4\n",
      "43\n",
      "4\n",
      "133_3\n",
      "133\n",
      "3\n",
      "79_8\n",
      "79\n",
      "8\n",
      "97_2\n",
      "97\n",
      "2\n",
      "52_1\n",
      "52\n",
      "1\n",
      "100_10\n",
      "100\n",
      "10\n",
      "49_12\n",
      "49\n",
      "12\n",
      "1_1\n",
      "1\n",
      "1\n",
      "123_10\n",
      "123\n",
      "10\n",
      "64_9\n",
      "64\n",
      "9\n",
      "119_1\n",
      "119\n",
      "1\n",
      "79_1\n",
      "79\n",
      "1\n",
      "76_7\n",
      "76\n",
      "7\n",
      "59_7\n",
      "59\n",
      "7\n",
      "8_3\n",
      "8\n",
      "3\n",
      "92_7\n",
      "92\n",
      "7\n",
      "91_8\n",
      "91\n",
      "8\n",
      "138_3\n",
      "138\n",
      "3\n",
      "110_8\n",
      "110\n",
      "8\n",
      "60_3\n",
      "60\n",
      "3\n",
      "76_4\n",
      "76\n",
      "4\n",
      "108_11\n",
      "108\n",
      "11\n",
      "58_8\n",
      "58\n",
      "8\n",
      "96_4\n",
      "96\n",
      "4\n",
      "86_3\n",
      "86\n",
      "3\n",
      "57_8\n",
      "57\n",
      "8\n",
      "56_4\n",
      "56\n",
      "4\n",
      "128_10\n",
      "128\n",
      "10\n",
      "39_12\n",
      "39\n",
      "12\n",
      "73_11\n",
      "73\n",
      "11\n",
      "6_5\n",
      "6\n",
      "5\n",
      "58_4\n",
      "58\n",
      "4\n",
      "124_2\n",
      "124\n",
      "2\n",
      "71_12\n",
      "71\n",
      "12\n",
      "3_9\n",
      "3\n",
      "9\n",
      "23_4\n",
      "23\n",
      "4\n",
      "136_5\n",
      "136\n",
      "5\n",
      "65_4\n",
      "65\n",
      "4\n",
      "126_10\n",
      "126\n",
      "10\n",
      "111_10\n",
      "111\n",
      "10\n",
      "97_5\n",
      "97\n",
      "5\n",
      "22_6\n",
      "22\n",
      "6\n",
      "81_5\n",
      "81\n",
      "5\n",
      "30_7\n",
      "30\n",
      "7\n",
      "98_8\n",
      "98\n",
      "8\n",
      "20_9\n",
      "20\n",
      "9\n",
      "47_5\n",
      "47\n",
      "5\n",
      "49_5\n",
      "49\n",
      "5\n",
      "45_5\n",
      "45\n",
      "5\n",
      "96_5\n",
      "96\n",
      "5\n",
      "58_12\n",
      "58\n",
      "12\n",
      "100_11\n",
      "100\n",
      "11\n",
      "22_4\n",
      "22\n",
      "4\n",
      "64_1\n",
      "64\n",
      "1\n",
      "25_6\n",
      "25\n",
      "6\n",
      "134_12\n",
      "134\n",
      "12\n",
      "92_12\n",
      "92\n",
      "12\n",
      "31_7\n",
      "31\n",
      "7\n",
      "73_6\n",
      "73\n",
      "6\n",
      "121_8\n",
      "121\n",
      "8\n",
      "69_12\n",
      "69\n",
      "12\n",
      "31_3\n",
      "31\n",
      "3\n",
      "101_7\n",
      "101\n",
      "7\n",
      "58_5\n",
      "58\n",
      "5\n",
      "135_4\n",
      "135\n",
      "4\n",
      "30_4\n",
      "30\n",
      "4\n",
      "22_1\n",
      "22\n",
      "1\n",
      "23_10\n",
      "23\n",
      "10\n",
      "85_9\n",
      "85\n",
      "9\n",
      "125_8\n",
      "125\n",
      "8\n",
      "131_11\n",
      "131\n",
      "11\n",
      "83_6\n",
      "83\n",
      "6\n",
      "114_3\n",
      "114\n",
      "3\n",
      "73_1\n",
      "73\n",
      "1\n",
      "34_3\n",
      "34\n",
      "3\n",
      "8_7\n",
      "8\n",
      "7\n",
      "49_6\n",
      "49\n",
      "6\n",
      "85_12\n",
      "85\n",
      "12\n",
      "37_4\n",
      "37\n",
      "4\n",
      "71_3\n",
      "71\n",
      "3\n",
      "67_5\n",
      "67\n",
      "5\n",
      "122_12\n",
      "122\n",
      "12\n",
      "71_2\n",
      "71\n",
      "2\n",
      "27_6\n",
      "27\n",
      "6\n",
      "80_2\n",
      "80\n",
      "2\n",
      "9_2\n",
      "9\n",
      "2\n",
      "92_11\n",
      "92\n",
      "11\n",
      "37_5\n",
      "37\n",
      "5\n",
      "2_5\n",
      "2\n",
      "5\n",
      "126_11\n",
      "126\n",
      "11\n",
      "103_5\n",
      "103\n",
      "5\n",
      "70_3\n",
      "70\n",
      "3\n",
      "107_6\n",
      "107\n",
      "6\n",
      "26_10\n",
      "26\n",
      "10\n",
      "131_8\n",
      "131\n",
      "8\n",
      "45_2\n",
      "45\n",
      "2\n",
      "29_9\n",
      "29\n",
      "9\n",
      "108_4\n",
      "108\n",
      "4\n",
      "72_1\n",
      "72\n",
      "1\n",
      "76_3\n",
      "76\n",
      "3\n",
      "18_5\n",
      "18\n",
      "5\n",
      "37_7\n",
      "37\n",
      "7\n",
      "77_8\n",
      "77\n",
      "8\n",
      "78_5\n",
      "78\n",
      "5\n",
      "77_9\n",
      "77\n",
      "9\n",
      "124_8\n",
      "124\n",
      "8\n",
      "49_1\n",
      "49\n",
      "1\n",
      "86_4\n",
      "86\n",
      "4\n",
      "57_3\n",
      "57\n",
      "3\n",
      "119_9\n",
      "119\n",
      "9\n",
      "115_7\n",
      "115\n",
      "7\n",
      "110_5\n",
      "110\n",
      "5\n",
      "112_3\n",
      "112\n",
      "3\n",
      "24_3\n",
      "24\n",
      "3\n",
      "108_10\n",
      "108\n",
      "10\n",
      "74_5\n",
      "74\n",
      "5\n",
      "63_3\n",
      "63\n",
      "3\n",
      "20_4\n",
      "20\n",
      "4\n",
      "13_5\n",
      "13\n",
      "5\n",
      "54_6\n",
      "54\n",
      "6\n",
      "5_12\n",
      "5\n",
      "12\n",
      "47_12\n",
      "47\n",
      "12\n",
      "72_6\n",
      "72\n",
      "6\n",
      "104_12\n",
      "104\n",
      "12\n",
      "130_10\n",
      "130\n",
      "10\n",
      "2_1\n",
      "2\n",
      "1\n",
      "39_7\n",
      "39\n",
      "7\n",
      "63_5\n",
      "63\n",
      "5\n",
      "23_7\n",
      "23\n",
      "7\n",
      "84_11\n",
      "84\n",
      "11\n",
      "10_2\n",
      "10\n",
      "2\n",
      "123_1\n",
      "123\n",
      "1\n",
      "90_4\n",
      "90\n",
      "4\n",
      "99_5\n",
      "99\n",
      "5\n",
      "9_12\n",
      "9\n",
      "12\n",
      "116_6\n",
      "116\n",
      "6\n",
      "122_10\n",
      "122\n",
      "10\n",
      "4_6\n",
      "4\n",
      "6\n",
      "31_8\n",
      "31\n",
      "8\n",
      "116_12\n",
      "116\n",
      "12\n",
      "20_3\n",
      "20\n",
      "3\n",
      "116_9\n",
      "116\n",
      "9\n",
      "37_12\n",
      "37\n",
      "12\n",
      "89_7\n",
      "89\n",
      "7\n",
      "15_9\n",
      "15\n",
      "9\n",
      "74_1\n",
      "74\n",
      "1\n",
      "21_8\n",
      "21\n",
      "8\n",
      "53_5\n",
      "53\n",
      "5\n",
      "28_6\n",
      "28\n",
      "6\n",
      "70_5\n",
      "70\n",
      "5\n",
      "25_11\n",
      "25\n",
      "11\n",
      "36_9\n",
      "36\n",
      "9\n",
      "17_4\n",
      "17\n",
      "4\n",
      "39_2\n",
      "39\n",
      "2\n",
      "113_8\n",
      "113\n",
      "8\n",
      "109_3\n",
      "109\n",
      "3\n",
      "92_10\n",
      "92\n",
      "10\n",
      "110_12\n",
      "110\n",
      "12\n",
      "57_9\n",
      "57\n",
      "9\n",
      "90_11\n",
      "90\n",
      "11\n",
      "86_2\n",
      "86\n",
      "2\n",
      "131_10\n",
      "131\n",
      "10\n",
      "14_11\n",
      "14\n",
      "11\n",
      "60_10\n",
      "60\n",
      "10\n",
      "115_3\n",
      "115\n",
      "3\n",
      "63_4\n",
      "63\n",
      "4\n",
      "109_2\n",
      "109\n",
      "2\n",
      "74_2\n",
      "74\n",
      "2\n",
      "53_11\n",
      "53\n",
      "11\n",
      "33_3\n",
      "33\n",
      "3\n",
      "123_7\n",
      "123\n",
      "7\n",
      "102_7\n",
      "102\n",
      "7\n",
      "105_11\n",
      "105\n",
      "11\n",
      "52_5\n",
      "52\n",
      "5\n",
      "61_5\n",
      "61\n",
      "5\n",
      "52_4\n",
      "52\n",
      "4\n",
      "17_9\n",
      "17\n",
      "9\n",
      "22_9\n",
      "22\n",
      "9\n",
      "77_5\n",
      "77\n",
      "5\n",
      "51_12\n",
      "51\n",
      "12\n",
      "29_12\n",
      "29\n",
      "12\n",
      "11_10\n",
      "11\n",
      "10\n",
      "53_12\n",
      "53\n",
      "12\n",
      "53_3\n",
      "53\n",
      "3\n",
      "73_5\n",
      "73\n",
      "5\n",
      "76_1\n",
      "76\n",
      "1\n",
      "60_9\n",
      "60\n",
      "9\n",
      "139_4\n",
      "139\n",
      "4\n",
      "76_2\n",
      "76\n",
      "2\n",
      "1_10\n",
      "1\n",
      "10\n",
      "120_3\n",
      "120\n",
      "3\n",
      "85_1\n",
      "85\n",
      "1\n",
      "21_3\n",
      "21\n",
      "3\n",
      "61_9\n",
      "61\n",
      "9\n",
      "16_1\n",
      "16\n",
      "1\n",
      "76_11\n",
      "76\n",
      "11\n",
      "18_11\n",
      "18\n",
      "11\n",
      "44_1\n",
      "44\n",
      "1\n",
      "17_8\n",
      "17\n",
      "8\n",
      "133_8\n",
      "133\n",
      "8\n",
      "110_11\n",
      "110\n",
      "11\n",
      "97_11\n",
      "97\n",
      "11\n",
      "140_3\n",
      "140\n",
      "3\n",
      "99_2\n",
      "99\n",
      "2\n",
      "72_4\n",
      "72\n",
      "4\n",
      "86_11\n",
      "86\n",
      "11\n",
      "56_1\n",
      "56\n",
      "1\n",
      "38_3\n",
      "38\n",
      "3\n",
      "77_4\n",
      "77\n",
      "4\n",
      "10_5\n",
      "10\n",
      "5\n",
      "93_3\n",
      "93\n",
      "3\n",
      "3_2\n",
      "3\n",
      "2\n",
      "109_11\n",
      "109\n",
      "11\n",
      "132_1\n",
      "132\n",
      "1\n",
      "52_8\n",
      "52\n",
      "8\n",
      "26_4\n",
      "26\n",
      "4\n",
      "105_7\n",
      "105\n",
      "7\n",
      "68_9\n",
      "68\n",
      "9\n",
      "13_11\n",
      "13\n",
      "11\n",
      "102_8\n",
      "102\n",
      "8\n",
      "97_12\n",
      "97\n",
      "12\n",
      "90_2\n",
      "90\n",
      "2\n",
      "49_2\n",
      "49\n",
      "2\n",
      "132_7\n",
      "132\n",
      "7\n",
      "12_6\n",
      "12\n",
      "6\n",
      "1_11\n",
      "1\n",
      "11\n",
      "135_5\n",
      "135\n",
      "5\n",
      "35_3\n",
      "35\n",
      "3\n",
      "31_11\n",
      "31\n",
      "11\n",
      "131_12\n",
      "131\n",
      "12\n",
      "126_12\n",
      "126\n",
      "12\n",
      "59_8\n",
      "59\n",
      "8\n",
      "83_1\n",
      "83\n",
      "1\n",
      "123_9\n",
      "123\n",
      "9\n",
      "135_7\n",
      "135\n",
      "7\n",
      "112_2\n",
      "112\n",
      "2\n",
      "113_5\n",
      "113\n",
      "5\n",
      "4_7\n",
      "4\n",
      "7\n",
      "12_3\n",
      "12\n",
      "3\n",
      "88_8\n",
      "88\n",
      "8\n",
      "115_11\n",
      "115\n",
      "11\n",
      "68_8\n",
      "68\n",
      "8\n",
      "22_2\n",
      "22\n",
      "2\n",
      "55_9\n",
      "55\n",
      "9\n",
      "34_12\n",
      "34\n",
      "12\n",
      "46_12\n",
      "46\n",
      "12\n",
      "129_4\n",
      "129\n",
      "4\n",
      "85_10\n",
      "85\n",
      "10\n",
      "108_9\n",
      "108\n",
      "9\n",
      "102_4\n",
      "102\n",
      "4\n",
      "54_2\n",
      "54\n",
      "2\n",
      "36_5\n",
      "36\n",
      "5\n",
      "19_2\n",
      "19\n",
      "2\n",
      "25_12\n",
      "25\n",
      "12\n",
      "118_12\n",
      "118\n",
      "12\n",
      "119_7\n",
      "119\n",
      "7\n",
      "55_7\n",
      "55\n",
      "7\n",
      "122_1\n",
      "122\n",
      "1\n",
      "39_3\n",
      "39\n",
      "3\n",
      "65_12\n",
      "65\n",
      "12\n",
      "57_12\n",
      "57\n",
      "12\n",
      "129_11\n",
      "129\n",
      "11\n",
      "95_11\n",
      "95\n",
      "11\n",
      "50_10\n",
      "50\n",
      "10\n",
      "109_8\n",
      "109\n",
      "8\n",
      "134_11\n",
      "134\n",
      "11\n",
      "37_1\n",
      "37\n",
      "1\n",
      "8_10\n",
      "8\n",
      "10\n",
      "98_6\n",
      "98\n",
      "6\n",
      "42_3\n",
      "42\n",
      "3\n",
      "100_7\n",
      "100\n",
      "7\n",
      "10_4\n",
      "10\n",
      "4\n",
      "90_5\n",
      "90\n",
      "5\n",
      "13_8\n",
      "13\n",
      "8\n",
      "43_2\n",
      "43\n",
      "2\n",
      "140_1\n",
      "140\n",
      "1\n",
      "108_8\n",
      "108\n",
      "8\n",
      "4_3\n",
      "4\n",
      "3\n",
      "53_10\n",
      "53\n",
      "10\n",
      "75_7\n",
      "75\n",
      "7\n",
      "23_6\n",
      "23\n",
      "6\n",
      "37_9\n",
      "37\n",
      "9\n",
      "139_9\n",
      "139\n",
      "9\n",
      "134_5\n",
      "134\n",
      "5\n",
      "119_2\n",
      "119\n",
      "2\n",
      "114_4\n",
      "114\n",
      "4\n",
      "126_5\n",
      "126\n",
      "5\n",
      "110_6\n",
      "110\n",
      "6\n",
      "15_11\n",
      "15\n",
      "11\n",
      "72_8\n",
      "72\n",
      "8\n",
      "87_10\n",
      "87\n",
      "10\n",
      "15_1\n",
      "15\n",
      "1\n",
      "94_9\n",
      "94\n",
      "9\n",
      "119_12\n",
      "119\n",
      "12\n",
      "139_8\n",
      "139\n",
      "8\n",
      "50_1\n",
      "50\n",
      "1\n",
      "1_3\n",
      "1\n",
      "3\n",
      "89_4\n",
      "89\n",
      "4\n",
      "98_12\n",
      "98\n",
      "12\n",
      "55_3\n",
      "55\n",
      "3\n",
      "78_12\n",
      "78\n",
      "12\n",
      "17_10\n",
      "17\n",
      "10\n",
      "15_5\n",
      "15\n",
      "5\n",
      "68_1\n",
      "68\n",
      "1\n",
      "137_10\n",
      "137\n",
      "10\n",
      "31_1\n",
      "31\n",
      "1\n",
      "98_1\n",
      "98\n",
      "1\n",
      "92_1\n",
      "92\n",
      "1\n",
      "107_9\n",
      "107\n",
      "9\n",
      "136_7\n",
      "136\n",
      "7\n",
      "130_1\n",
      "130\n",
      "1\n",
      "124_6\n",
      "124\n",
      "6\n",
      "139_10\n",
      "139\n",
      "10\n",
      "3_6\n",
      "3\n",
      "6\n",
      "83_3\n",
      "83\n",
      "3\n",
      "49_4\n",
      "49\n",
      "4\n",
      "120_10\n",
      "120\n",
      "10\n",
      "96_1\n",
      "96\n",
      "1\n",
      "112_10\n",
      "112\n",
      "10\n",
      "136_2\n",
      "136\n",
      "2\n",
      "20_11\n",
      "20\n",
      "11\n",
      "137_9\n",
      "137\n",
      "9\n",
      "48_1\n",
      "48\n",
      "1\n",
      "40_8\n",
      "40\n",
      "8\n",
      "66_7\n",
      "66\n",
      "7\n",
      "27_11\n",
      "27\n",
      "11\n",
      "117_10\n",
      "117\n",
      "10\n",
      "84_5\n",
      "84\n",
      "5\n",
      "96_8\n",
      "96\n",
      "8\n",
      "97_10\n",
      "97\n",
      "10\n",
      "114_7\n",
      "114\n",
      "7\n",
      "50_4\n",
      "50\n",
      "4\n",
      "21_2\n",
      "21\n",
      "2\n",
      "14_8\n",
      "14\n",
      "8\n",
      "106_4\n",
      "106\n",
      "4\n",
      "111_7\n",
      "111\n",
      "7\n",
      "20_10\n",
      "20\n",
      "10\n",
      "73_7\n",
      "73\n",
      "7\n",
      "126_8\n",
      "126\n",
      "8\n",
      "77_6\n",
      "77\n",
      "6\n",
      "10_12\n",
      "10\n",
      "12\n",
      "121_9\n",
      "121\n",
      "9\n",
      "87_12\n",
      "87\n",
      "12\n",
      "102_12\n",
      "102\n",
      "12\n",
      "69_6\n",
      "69\n",
      "6\n",
      "85_2\n",
      "85\n",
      "2\n",
      "53_9\n",
      "53\n",
      "9\n",
      "65_5\n",
      "65\n",
      "5\n",
      "73_9\n",
      "73\n",
      "9\n",
      "26_11\n",
      "26\n",
      "11\n",
      "63_7\n",
      "63\n",
      "7\n",
      "138_11\n",
      "138\n",
      "11\n",
      "10_11\n",
      "10\n",
      "11\n",
      "27_12\n",
      "27\n",
      "12\n",
      "104_6\n",
      "104\n",
      "6\n",
      "70_2\n",
      "70\n",
      "2\n",
      "109_6\n",
      "109\n",
      "6\n",
      "10_3\n",
      "10\n",
      "3\n",
      "37_11\n",
      "37\n",
      "11\n",
      "125_4\n",
      "125\n",
      "4\n",
      "24_12\n",
      "24\n",
      "12\n",
      "66_2\n",
      "66\n",
      "2\n",
      "135_6\n",
      "135\n",
      "6\n",
      "121_12\n",
      "121\n",
      "12\n",
      "136_11\n",
      "136\n",
      "11\n",
      "97_3\n",
      "97\n",
      "3\n",
      "86_9\n",
      "86\n",
      "9\n",
      "7_8\n",
      "7\n",
      "8\n",
      "25_2\n",
      "25\n",
      "2\n",
      "66_12\n",
      "66\n",
      "12\n",
      "71_11\n",
      "71\n",
      "11\n",
      "59_11\n",
      "59\n",
      "11\n",
      "12_1\n",
      "12\n",
      "1\n",
      "3_12\n",
      "3\n",
      "12\n",
      "45_9\n",
      "45\n",
      "9\n",
      "3_1\n",
      "3\n",
      "1\n",
      "20_6\n",
      "20\n",
      "6\n",
      "98_7\n",
      "98\n",
      "7\n",
      "7_1\n",
      "7\n",
      "1\n",
      "112_5\n",
      "112\n",
      "5\n",
      "28_8\n",
      "28\n",
      "8\n",
      "8_2\n",
      "8\n",
      "2\n",
      "115_8\n",
      "115\n",
      "8\n",
      "121_2\n",
      "121\n",
      "2\n",
      "18_6\n",
      "18\n",
      "6\n",
      "18_4\n",
      "18\n",
      "4\n",
      "15_10\n",
      "15\n",
      "10\n",
      "117_1\n",
      "117\n",
      "1\n",
      "77_7\n",
      "77\n",
      "7\n",
      "29_6\n",
      "29\n",
      "6\n",
      "76_8\n",
      "76\n",
      "8\n",
      "140_11\n",
      "140\n",
      "11\n",
      "122_2\n",
      "122\n",
      "2\n",
      "27_8\n",
      "27\n",
      "8\n",
      "95_3\n",
      "95\n",
      "3\n",
      "74_11\n",
      "74\n",
      "11\n",
      "140_12\n",
      "140\n",
      "12\n",
      "107_12\n",
      "107\n",
      "12\n",
      "104_11\n",
      "104\n",
      "11\n",
      "67_1\n",
      "67\n",
      "1\n",
      "67_8\n",
      "67\n",
      "8\n",
      "67_12\n",
      "67\n",
      "12\n",
      "36_11\n",
      "36\n",
      "11\n",
      "107_11\n",
      "107\n",
      "11\n",
      "131_6\n",
      "131\n",
      "6\n",
      "56_12\n",
      "56\n",
      "12\n",
      "11_11\n",
      "11\n",
      "11\n",
      "74_7\n",
      "74\n",
      "7\n",
      "79_2\n",
      "79\n",
      "2\n",
      "48_9\n",
      "48\n",
      "9\n",
      "45_1\n",
      "45\n",
      "1\n",
      "91_12\n",
      "91\n",
      "12\n",
      "86_6\n",
      "86\n",
      "6\n",
      "80_3\n",
      "80\n",
      "3\n",
      "65_9\n",
      "65\n",
      "9\n",
      "134_10\n",
      "134\n",
      "10\n",
      "118_9\n",
      "118\n",
      "9\n",
      "57_2\n",
      "57\n",
      "2\n",
      "101_12\n",
      "101\n",
      "12\n",
      "14_2\n",
      "14\n",
      "2\n",
      "111_1\n",
      "111\n",
      "1\n",
      "46_11\n",
      "46\n",
      "11\n",
      "100_9\n",
      "100\n",
      "9\n",
      "138_9\n",
      "138\n",
      "9\n",
      "7_4\n",
      "7\n",
      "4\n",
      "28_12\n",
      "28\n",
      "12\n",
      "122_9\n",
      "122\n",
      "9\n",
      "38_1\n",
      "38\n",
      "1\n",
      "95_4\n",
      "95\n",
      "4\n",
      "35_10\n",
      "35\n",
      "10\n",
      "89_6\n",
      "89\n",
      "6\n",
      "55_10\n",
      "55\n",
      "10\n",
      "69_7\n",
      "69\n",
      "7\n",
      "52_7\n",
      "52\n",
      "7\n",
      "5_4\n",
      "5\n",
      "4\n",
      "4_12\n",
      "4\n",
      "12\n",
      "51_1\n",
      "51\n",
      "1\n",
      "75_2\n",
      "75\n",
      "2\n",
      "1_6\n",
      "1\n",
      "6\n",
      "61_7\n",
      "61\n",
      "7\n",
      "89_5\n",
      "89\n",
      "5\n",
      "93_4\n",
      "93\n",
      "4\n",
      "53_6\n",
      "53\n",
      "6\n",
      "12_7\n",
      "12\n",
      "7\n",
      "51_11\n",
      "51\n",
      "11\n",
      "127_1\n",
      "127\n",
      "1\n",
      "72_9\n",
      "72\n",
      "9\n",
      "42_11\n",
      "42\n",
      "11\n",
      "5_5\n",
      "5\n",
      "5\n",
      "71_9\n",
      "71\n",
      "9\n",
      "49_11\n",
      "49\n",
      "11\n",
      "82_6\n",
      "82\n",
      "6\n",
      "70_7\n",
      "70\n",
      "7\n",
      "99_11\n",
      "99\n",
      "11\n",
      "93_2\n",
      "93\n",
      "2\n",
      "15_8\n",
      "15\n",
      "8\n",
      "126_6\n",
      "126\n",
      "6\n",
      "69_9\n",
      "69\n",
      "9\n",
      "74_4\n",
      "74\n",
      "4\n",
      "106_11\n",
      "106\n",
      "11\n",
      "56_10\n",
      "56\n",
      "10\n",
      "98_4\n",
      "98\n",
      "4\n",
      "71_7\n",
      "71\n",
      "7\n",
      "104_4\n",
      "104\n",
      "4\n",
      "33_5\n",
      "33\n",
      "5\n",
      "50_11\n",
      "50\n",
      "11\n",
      "19_9\n",
      "19\n",
      "9\n",
      "137_7\n",
      "137\n",
      "7\n",
      "13_6\n",
      "13\n",
      "6\n",
      "102_2\n",
      "102\n",
      "2\n",
      "131_7\n",
      "131\n",
      "7\n",
      "30_1\n",
      "30\n",
      "1\n",
      "46_5\n",
      "46\n",
      "5\n",
      "116_11\n",
      "116\n",
      "11\n",
      "34_5\n",
      "34\n",
      "5\n",
      "26_12\n",
      "26\n",
      "12\n",
      "52_6\n",
      "52\n",
      "6\n",
      "54_12\n",
      "54\n",
      "12\n",
      "122_5\n",
      "122\n",
      "5\n",
      "17_2\n",
      "17\n",
      "2\n",
      "77_10\n",
      "77\n",
      "10\n",
      "91_6\n",
      "91\n",
      "6\n",
      "69_1\n",
      "69\n",
      "1\n",
      "81_7\n",
      "81\n",
      "7\n",
      "134_6\n",
      "134\n",
      "6\n",
      "76_5\n",
      "76\n",
      "5\n",
      "30_8\n",
      "30\n",
      "8\n",
      "25_1\n",
      "25\n",
      "1\n",
      "88_7\n",
      "88\n",
      "7\n",
      "31_12\n",
      "31\n",
      "12\n",
      "131_4\n",
      "131\n",
      "4\n",
      "103_12\n",
      "103\n",
      "12\n",
      "72_5\n",
      "72\n",
      "5\n",
      "33_12\n",
      "33\n",
      "12\n",
      "83_10\n",
      "83\n",
      "10\n",
      "77_3\n",
      "77\n",
      "3\n",
      "65_6\n",
      "65\n",
      "6\n",
      "109_10\n",
      "109\n",
      "10\n",
      "139_3\n",
      "139\n",
      "3\n",
      "11_7\n",
      "11\n",
      "7\n",
      "12_2\n",
      "12\n",
      "2\n",
      "55_4\n",
      "55\n",
      "4\n",
      "2_8\n",
      "2\n",
      "8\n",
      "123_4\n",
      "123\n",
      "4\n",
      "19_12\n",
      "19\n",
      "12\n",
      "84_12\n",
      "84\n",
      "12\n",
      "93_11\n",
      "93\n",
      "11\n",
      "59_6\n",
      "59\n",
      "6\n",
      "95_5\n",
      "95\n",
      "5\n",
      "64_11\n",
      "64\n",
      "11\n",
      "78_9\n",
      "78\n",
      "9\n",
      "34_11\n",
      "34\n",
      "11\n",
      "134_4\n",
      "134\n",
      "4\n",
      "63_10\n",
      "63\n",
      "10\n",
      "91_1\n",
      "91\n",
      "1\n",
      "17_5\n",
      "17\n",
      "5\n",
      "52_10\n",
      "52\n",
      "10\n",
      "112_1\n",
      "112\n",
      "1\n",
      "40_12\n",
      "40\n",
      "12\n",
      "40_9\n",
      "40\n",
      "9\n",
      "73_3\n",
      "73\n",
      "3\n",
      "39_6\n",
      "39\n",
      "6\n",
      "95_7\n",
      "95\n",
      "7\n",
      "16_10\n",
      "16\n",
      "10\n",
      "3_3\n",
      "3\n",
      "3\n",
      "134_3\n",
      "134\n",
      "3\n",
      "38_2\n",
      "38\n",
      "2\n",
      "109_7\n",
      "109\n",
      "7\n",
      "13_2\n",
      "13\n",
      "2\n",
      "105_9\n",
      "105\n",
      "9\n",
      "90_12\n",
      "90\n",
      "12\n",
      "27_5\n",
      "27\n",
      "5\n",
      "133_7\n",
      "133\n",
      "7\n",
      "99_1\n",
      "99\n",
      "1\n",
      "103_11\n",
      "103\n",
      "11\n",
      "101_11\n",
      "101\n",
      "11\n",
      "62_7\n",
      "62\n",
      "7\n",
      "68_6\n",
      "68\n",
      "6\n",
      "19_8\n",
      "19\n",
      "8\n",
      "36_3\n",
      "36\n",
      "3\n",
      "90_8\n",
      "90\n",
      "8\n",
      "135_3\n",
      "135\n",
      "3\n",
      "63_6\n",
      "63\n",
      "6\n",
      "57_6\n",
      "57\n",
      "6\n",
      "32_6\n",
      "32\n",
      "6\n",
      "125_10\n",
      "125\n",
      "10\n",
      "86_1\n",
      "86\n",
      "1\n",
      "81_9\n",
      "81\n",
      "9\n",
      "15_7\n",
      "15\n",
      "7\n",
      "33_11\n",
      "33\n",
      "11\n",
      "97_7\n",
      "97\n",
      "7\n",
      "84_6\n",
      "84\n",
      "6\n",
      "119_10\n",
      "119\n",
      "10\n",
      "8_12\n",
      "8\n",
      "12\n",
      "38_4\n",
      "38\n",
      "4\n",
      "115_10\n",
      "115\n",
      "10\n",
      "122_4\n",
      "122\n",
      "4\n",
      "94_7\n",
      "94\n",
      "7\n",
      "126_4\n",
      "126\n",
      "4\n",
      "98_9\n",
      "98\n",
      "9\n",
      "58_2\n",
      "58\n",
      "2\n",
      "120_8\n",
      "120\n",
      "8\n",
      "119_8\n",
      "119\n",
      "8\n",
      "60_12\n",
      "60\n",
      "12\n",
      "95_6\n",
      "95\n",
      "6\n",
      "135_8\n",
      "135\n",
      "8\n",
      "107_4\n",
      "107\n",
      "4\n",
      "2_4\n",
      "2\n",
      "4\n",
      "121_3\n",
      "121\n",
      "3\n",
      "129_8\n",
      "129\n",
      "8\n",
      "78_6\n",
      "78\n",
      "6\n",
      "67_6\n",
      "67\n",
      "6\n",
      "5_1\n",
      "5\n",
      "1\n",
      "17_12\n",
      "17\n",
      "12\n",
      "119_6\n",
      "119\n",
      "6\n",
      "47_11\n",
      "47\n",
      "11\n",
      "7_9\n",
      "7\n",
      "9\n",
      "63_1\n",
      "63\n",
      "1\n",
      "33_4\n",
      "33\n",
      "4\n",
      "47_6\n",
      "47\n",
      "6\n",
      "25_9\n",
      "25\n",
      "9\n",
      "138_8\n",
      "138\n",
      "8\n",
      "93_12\n",
      "93\n",
      "12\n",
      "94_8\n",
      "94\n",
      "8\n",
      "48_3\n",
      "48\n",
      "3\n",
      "110_1\n",
      "110\n",
      "1\n",
      "35_9\n",
      "35\n",
      "9\n",
      "20_12\n",
      "20\n",
      "12\n",
      "125_1\n",
      "125\n",
      "1\n",
      "2_12\n",
      "2\n",
      "12\n",
      "40_1\n",
      "40\n",
      "1\n",
      "111_3\n",
      "111\n",
      "3\n",
      "28_2\n",
      "28\n",
      "2\n",
      "11_8\n",
      "11\n",
      "8\n",
      "129_7\n",
      "129\n",
      "7\n",
      "133_6\n",
      "133\n",
      "6\n",
      "5_7\n",
      "5\n",
      "7\n",
      "94_3\n",
      "94\n",
      "3\n",
      "4_9\n",
      "4\n",
      "9\n",
      "129_6\n",
      "129\n",
      "6\n",
      "10_6\n",
      "10\n",
      "6\n",
      "24_8\n",
      "24\n",
      "8\n",
      "114_1\n",
      "114\n",
      "1\n",
      "125_9\n",
      "125\n",
      "9\n",
      "65_11\n",
      "65\n",
      "11\n",
      "98_11\n",
      "98\n",
      "11\n",
      "118_11\n",
      "118\n",
      "11\n",
      "96_6\n",
      "96\n",
      "6\n",
      "30_10\n",
      "30\n",
      "10\n",
      "19_7\n",
      "19\n",
      "7\n",
      "87_7\n",
      "87\n",
      "7\n",
      "41_9\n",
      "41\n",
      "9\n",
      "92_8\n",
      "92\n",
      "8\n",
      "93_8\n",
      "93\n",
      "8\n",
      "58_9\n",
      "58\n",
      "9\n",
      "62_8\n",
      "62\n",
      "8\n",
      "25_10\n",
      "25\n",
      "10\n",
      "86_7\n",
      "86\n",
      "7\n",
      "8_11\n",
      "8\n",
      "11\n",
      "7_5\n",
      "7\n",
      "5\n",
      "88_1\n",
      "88\n",
      "1\n",
      "110_3\n",
      "110\n",
      "3\n",
      "76_9\n",
      "76\n",
      "9\n",
      "128_1\n",
      "128\n",
      "1\n",
      "34_4\n",
      "34\n",
      "4\n",
      "1_8\n",
      "1\n",
      "8\n",
      "60_5\n",
      "60\n",
      "5\n",
      "94_1\n",
      "94\n",
      "1\n",
      "23_8\n",
      "23\n",
      "8\n",
      "41_10\n",
      "41\n",
      "10\n",
      "83_8\n",
      "83\n",
      "8\n",
      "6_10\n",
      "6\n",
      "10\n",
      "69_10\n",
      "69\n",
      "10\n",
      "81_4\n",
      "81\n",
      "4\n",
      "38_8\n",
      "38\n",
      "8\n",
      "19_11\n",
      "19\n",
      "11\n",
      "124_7\n",
      "124\n",
      "7\n",
      "74_8\n",
      "74\n",
      "8\n",
      "21_4\n",
      "21\n",
      "4\n",
      "123_5\n",
      "123\n",
      "5\n",
      "22_12\n",
      "22\n",
      "12\n",
      "94_5\n",
      "94\n",
      "5\n",
      "28_11\n",
      "28\n",
      "11\n",
      "44_10\n",
      "44\n",
      "10\n",
      "118_2\n",
      "118\n",
      "2\n",
      "13_3\n",
      "13\n",
      "3\n",
      "110_9\n",
      "110\n",
      "9\n",
      "55_12\n",
      "55\n",
      "12\n",
      "116_7\n",
      "116\n",
      "7\n",
      "114_5\n",
      "114\n",
      "5\n",
      "43_8\n",
      "43\n",
      "8\n",
      "134_9\n",
      "134\n",
      "9\n",
      "68_5\n",
      "68\n",
      "5\n",
      "49_3\n",
      "49\n",
      "3\n",
      "125_2\n",
      "125\n",
      "2\n",
      "45_3\n",
      "45\n",
      "3\n",
      "113_11\n",
      "113\n",
      "11\n",
      "117_8\n",
      "117\n",
      "8\n",
      "123_12\n",
      "123\n",
      "12\n",
      "107_3\n",
      "107\n",
      "3\n",
      "9_1\n",
      "9\n",
      "1\n",
      "41_12\n",
      "41\n",
      "12\n",
      "129_3\n",
      "129\n",
      "3\n",
      "101_2\n",
      "101\n",
      "2\n",
      "2_7\n",
      "2\n",
      "7\n",
      "123_6\n",
      "123\n",
      "6\n",
      "70_12\n",
      "70\n",
      "12\n",
      "91_5\n",
      "91\n",
      "5\n",
      "36_1\n",
      "36\n",
      "1\n",
      "68_7\n",
      "68\n",
      "7\n",
      "116_3\n",
      "116\n",
      "3\n",
      "29_5\n",
      "29\n",
      "5\n",
      "42_8\n",
      "42\n",
      "8\n",
      "100_8\n",
      "100\n",
      "8\n",
      "69_2\n",
      "69\n",
      "2\n",
      "101_10\n",
      "101\n",
      "10\n",
      "80_4\n",
      "80\n",
      "4\n",
      "130_9\n",
      "130\n",
      "9\n",
      "131_2\n",
      "131\n",
      "2\n",
      "112_8\n",
      "112\n",
      "8\n",
      "31_10\n",
      "31\n",
      "10\n",
      "74_10\n",
      "74\n",
      "10\n",
      "27_1\n",
      "27\n",
      "1\n",
      "66_11\n",
      "66\n",
      "11\n",
      "42_4\n",
      "42\n",
      "4\n",
      "138_7\n",
      "138\n",
      "7\n",
      "133_2\n",
      "133\n",
      "2\n",
      "69_11\n",
      "69\n",
      "11\n",
      "52_12\n",
      "52\n",
      "12\n",
      "79_10\n",
      "79\n",
      "10\n",
      "88_9\n",
      "88\n",
      "9\n",
      "34_9\n",
      "34\n",
      "9\n",
      "35_8\n",
      "35\n",
      "8\n",
      "82_10\n",
      "82\n",
      "10\n",
      "29_3\n",
      "29\n",
      "3\n",
      "61_12\n",
      "61\n",
      "12\n",
      "127_3\n",
      "127\n",
      "3\n",
      "31_6\n",
      "31\n",
      "6\n",
      "82_7\n",
      "82\n",
      "7\n",
      "67_7\n",
      "67\n",
      "7\n",
      "52_2\n",
      "52\n",
      "2\n",
      "91_7\n",
      "91\n",
      "7\n",
      "64_7\n",
      "64\n",
      "7\n",
      "87_9\n",
      "87\n",
      "9\n",
      "81_6\n",
      "81\n",
      "6\n",
      "42_9\n",
      "42\n",
      "9\n",
      "11_12\n",
      "11\n",
      "12\n",
      "48_4\n",
      "48\n",
      "4\n",
      "80_9\n",
      "80\n",
      "9\n",
      "133_11\n",
      "133\n",
      "11\n",
      "14_9\n",
      "14\n",
      "9\n",
      "93_10\n",
      "93\n",
      "10\n",
      "107_1\n",
      "107\n",
      "1\n",
      "96_12\n",
      "96\n",
      "12\n",
      "25_5\n",
      "25\n",
      "5\n",
      "23_1\n",
      "23\n",
      "1\n",
      "109_5\n",
      "109\n",
      "5\n",
      "45_4\n",
      "45\n",
      "4\n",
      "49_8\n",
      "49\n",
      "8\n",
      "54_1\n",
      "54\n",
      "1\n",
      "84_8\n",
      "84\n",
      "8\n",
      "113_3\n",
      "113\n",
      "3\n",
      "34_6\n",
      "34\n",
      "6\n",
      "113_1\n",
      "113\n",
      "1\n",
      "71_1\n",
      "71\n",
      "1\n",
      "98_3\n",
      "98\n",
      "3\n",
      "46_6\n",
      "46\n",
      "6\n",
      "1_4\n",
      "1\n",
      "4\n",
      "3_7\n",
      "3\n",
      "7\n",
      "139_1\n",
      "139\n",
      "1\n",
      "44_4\n",
      "44\n",
      "4\n",
      "103_6\n",
      "103\n",
      "6\n",
      "9_6\n",
      "9\n",
      "6\n",
      "72_10\n",
      "72\n",
      "10\n",
      "110_4\n",
      "110\n",
      "4\n",
      "35_1\n",
      "35\n",
      "1\n",
      "107_5\n",
      "107\n",
      "5\n",
      "98_2\n",
      "98\n",
      "2\n",
      "71_10\n",
      "71\n",
      "10\n",
      "16_8\n",
      "16\n",
      "8\n",
      "117_11\n",
      "117\n",
      "11\n",
      "83_7\n",
      "83\n",
      "7\n",
      "54_10\n",
      "54\n",
      "10\n",
      "82_5\n",
      "82\n",
      "5\n",
      "21_10\n",
      "21\n",
      "10\n",
      "32_1\n",
      "32\n",
      "1\n",
      "120_2\n",
      "120\n",
      "2\n",
      "111_11\n",
      "111\n",
      "11\n",
      "82_12\n",
      "82\n",
      "12\n",
      "41_7\n",
      "41\n",
      "7\n",
      "45_8\n",
      "45\n",
      "8\n",
      "113_9\n",
      "113\n",
      "9\n",
      "30_12\n",
      "30\n",
      "12\n",
      "40_11\n",
      "40\n",
      "11\n",
      "14_1\n",
      "14\n",
      "1\n",
      "78_1\n",
      "78\n",
      "1\n",
      "85_11\n",
      "85\n",
      "11\n",
      "106_12\n",
      "106\n",
      "12\n",
      "11_2\n",
      "11\n",
      "2\n",
      "117_2\n",
      "117\n",
      "2\n",
      "131_5\n",
      "131\n",
      "5\n",
      "85_7\n",
      "85\n",
      "7\n",
      "138_4\n",
      "138\n",
      "4\n",
      "111_4\n",
      "111\n",
      "4\n",
      "4_10\n",
      "4\n",
      "10\n",
      "111_8\n",
      "111\n",
      "8\n",
      "118_5\n",
      "118\n",
      "5\n",
      "21_7\n",
      "21\n",
      "7\n",
      "121_10\n",
      "121\n",
      "10\n",
      "71_8\n",
      "71\n",
      "8\n",
      "48_8\n",
      "48\n",
      "8\n",
      "106_10\n",
      "106\n",
      "10\n",
      "3_4\n",
      "3\n",
      "4\n",
      "63_8\n",
      "63\n",
      "8\n",
      "5_10\n",
      "5\n",
      "10\n",
      "66_10\n",
      "66\n",
      "10\n",
      "61_6\n",
      "61\n",
      "6\n",
      "138_12\n",
      "138\n",
      "12\n",
      "29_1\n",
      "29\n",
      "1\n",
      "71_4\n",
      "71\n",
      "4\n",
      "120_7\n",
      "120\n",
      "7\n",
      "119_3\n",
      "119\n",
      "3\n",
      "65_1\n",
      "65\n",
      "1\n",
      "41_2\n",
      "41\n",
      "2\n",
      "65_8\n",
      "65\n",
      "8\n",
      "87_5\n",
      "87\n",
      "5\n",
      "125_11\n",
      "125\n",
      "11\n",
      "27_9\n",
      "27\n",
      "9\n",
      "57_4\n",
      "57\n",
      "4\n",
      "91_10\n",
      "91\n",
      "10\n",
      "112_12\n",
      "112\n",
      "12\n",
      "80_8\n",
      "80\n",
      "8\n",
      "30_9\n",
      "30\n",
      "9\n",
      "103_4\n",
      "103\n",
      "4\n",
      "41_11\n",
      "41\n",
      "11\n",
      "51_6\n",
      "51\n",
      "6\n",
      "12_4\n",
      "12\n",
      "4\n",
      "54_5\n",
      "54\n",
      "5\n",
      "117_3\n",
      "117\n",
      "3\n",
      "38_10\n",
      "38\n",
      "10\n",
      "111_9\n",
      "111\n",
      "9\n",
      "7_10\n",
      "7\n",
      "10\n",
      "139_2\n",
      "139\n",
      "2\n",
      "97_4\n",
      "97\n",
      "4\n",
      "111_2\n",
      "111\n",
      "2\n",
      "14_7\n",
      "14\n",
      "7\n",
      "105_3\n",
      "105\n",
      "3\n",
      "25_3\n",
      "25\n",
      "3\n",
      "15_3\n",
      "15\n",
      "3\n",
      "119_11\n",
      "119\n",
      "11\n",
      "52_11\n",
      "52\n",
      "11\n",
      "46_1\n",
      "46\n",
      "1\n",
      "62_10\n",
      "62\n",
      "10\n",
      "72_2\n",
      "72\n",
      "2\n",
      "12_11\n",
      "12\n",
      "11\n",
      "133_9\n",
      "133\n",
      "9\n",
      "7_6\n",
      "7\n",
      "6\n",
      "114_12\n",
      "114\n",
      "12\n",
      "31_5\n",
      "31\n",
      "5\n",
      "74_6\n",
      "74\n",
      "6\n",
      "67_10\n",
      "67\n",
      "10\n",
      "136_1\n",
      "136\n",
      "1\n",
      "61_2\n",
      "61\n",
      "2\n",
      "89_9\n",
      "89\n",
      "9\n",
      "83_9\n",
      "83\n",
      "9\n",
      "83_11\n",
      "83\n",
      "11\n",
      "124_4\n",
      "124\n",
      "4\n",
      "117_5\n",
      "117\n",
      "5\n",
      "63_12\n",
      "63\n",
      "12\n",
      "70_4\n",
      "70\n",
      "4\n",
      "77_12\n",
      "77\n",
      "12\n",
      "60_2\n",
      "60\n",
      "2\n",
      "32_8\n",
      "32\n",
      "8\n",
      "20_8\n",
      "20\n",
      "8\n",
      "106_9\n",
      "106\n",
      "9\n",
      "13_9\n",
      "13\n",
      "9\n",
      "138_5\n",
      "138\n",
      "5\n",
      "77_1\n",
      "77\n",
      "1\n",
      "62_5\n",
      "62\n",
      "5\n",
      "58_11\n",
      "58\n",
      "11\n",
      "49_9\n",
      "49\n",
      "9\n",
      "66_3\n",
      "66\n",
      "3\n",
      "5_9\n",
      "5\n",
      "9\n",
      "106_8\n",
      "106\n",
      "8\n",
      "77_2\n",
      "77\n",
      "2\n",
      "26_1\n",
      "26\n",
      "1\n",
      "27_4\n",
      "27\n",
      "4\n",
      "90_9\n",
      "90\n",
      "9\n",
      "99_3\n",
      "99\n",
      "3\n",
      "45_11\n",
      "45\n",
      "11\n",
      "48_12\n",
      "48\n",
      "12\n",
      "116_5\n",
      "116\n",
      "5\n",
      "122_7\n",
      "122\n",
      "7\n",
      "40_7\n",
      "40\n",
      "7\n",
      "57_11\n",
      "57\n",
      "11\n",
      "79_3\n",
      "79\n",
      "3\n",
      "107_2\n",
      "107\n",
      "2\n",
      "40_3\n",
      "40\n",
      "3\n",
      "102_3\n",
      "102\n",
      "3\n",
      "124_5\n",
      "124\n",
      "5\n",
      "46_7\n",
      "46\n",
      "7\n",
      "114_8\n",
      "114\n",
      "8\n",
      "68_11\n",
      "68\n",
      "11\n",
      "118_8\n",
      "118\n",
      "8\n",
      "90_6\n",
      "90\n",
      "6\n",
      "42_10\n",
      "42\n",
      "10\n",
      "81_1\n",
      "81\n",
      "1\n",
      "88_2\n",
      "88\n",
      "2\n",
      "116_2\n",
      "116\n",
      "2\n",
      "70_1\n",
      "70\n",
      "1\n",
      "89_11\n",
      "89\n",
      "11\n",
      "20_5\n",
      "20\n",
      "5\n",
      "47_8\n",
      "47\n",
      "8\n",
      "62_2\n",
      "62\n",
      "2\n",
      "78_2\n",
      "78\n",
      "2\n",
      "66_6\n",
      "66\n",
      "6\n",
      "36_8\n",
      "36\n",
      "8\n",
      "64_10\n",
      "64\n",
      "10\n",
      "89_12\n",
      "89\n",
      "12\n",
      "99_9\n",
      "99\n",
      "9\n",
      "103_8\n",
      "103\n",
      "8\n",
      "105_4\n",
      "105\n",
      "4\n",
      "124_3\n",
      "124\n",
      "3\n",
      "135_12\n",
      "135\n",
      "12\n",
      "122_8\n",
      "122\n",
      "8\n",
      "60_11\n",
      "60\n",
      "11\n",
      "59_4\n",
      "59\n",
      "4\n",
      "96_10\n",
      "96\n",
      "10\n",
      "70_11\n",
      "70\n",
      "11\n",
      "59_10\n",
      "59\n",
      "10\n",
      "130_5\n",
      "130\n",
      "5\n",
      "43_1\n",
      "43\n",
      "1\n",
      "19_4\n",
      "19\n",
      "4\n",
      "74_9\n",
      "74\n",
      "9\n",
      "82_1\n",
      "82\n",
      "1\n",
      "129_5\n",
      "129\n",
      "5\n",
      "79_12\n",
      "79\n",
      "12\n",
      "44_7\n",
      "44\n",
      "7\n",
      "139_12\n",
      "139\n",
      "12\n",
      "84_10\n",
      "84\n",
      "10\n",
      "102_1\n",
      "102\n",
      "1\n",
      "79_11\n",
      "79\n",
      "11\n",
      "97_9\n",
      "97\n",
      "9\n",
      "37_3\n",
      "37\n",
      "3\n",
      "127_10\n",
      "127\n",
      "10\n",
      "48_11\n",
      "48\n",
      "11\n",
      "53_1\n",
      "53\n",
      "1\n",
      "128_2\n",
      "128\n",
      "2\n",
      "16_9\n",
      "16\n",
      "9\n",
      "99_7\n",
      "99\n",
      "7\n",
      "108_7\n",
      "108\n",
      "7\n",
      "6_7\n",
      "6\n",
      "7\n",
      "8_6\n",
      "8\n",
      "6\n",
      "56_8\n",
      "56\n",
      "8\n",
      "135_1\n",
      "135\n",
      "1\n",
      "10_10\n",
      "10\n",
      "10\n",
      "32_7\n",
      "32\n",
      "7\n",
      "130_12\n",
      "130\n",
      "12\n",
      "26_9\n",
      "26\n",
      "9\n",
      "86_12\n",
      "86\n",
      "12\n",
      "123_8\n",
      "123\n",
      "8\n",
      "108_2\n",
      "108\n",
      "2\n",
      "83_4\n",
      "83\n",
      "4\n",
      "132_3\n",
      "132\n",
      "3\n",
      "84_7\n",
      "84\n",
      "7\n",
      "23_9\n",
      "23\n",
      "9\n",
      "68_2\n",
      "68\n",
      "2\n",
      "52_3\n",
      "52\n",
      "3\n",
      "58_3\n",
      "58\n",
      "3\n",
      "32_4\n",
      "32\n",
      "4\n",
      "10_9\n",
      "10\n",
      "9\n",
      "140_6\n",
      "140\n",
      "6\n",
      "73_2\n",
      "73\n",
      "2\n",
      "5_11\n",
      "5\n",
      "11\n",
      "103_9\n",
      "103\n",
      "9\n",
      "28_3\n",
      "28\n",
      "3\n",
      "80_7\n",
      "80\n",
      "7\n",
      "75_4\n",
      "75\n",
      "4\n",
      "16_3\n",
      "16\n",
      "3\n",
      "11_9\n",
      "11\n",
      "9\n",
      "116_1\n",
      "116\n",
      "1\n",
      "41_4\n",
      "41\n",
      "4\n",
      "84_2\n",
      "84\n",
      "2\n",
      "43_10\n",
      "43\n",
      "10\n",
      "139_5\n",
      "139\n",
      "5\n",
      "47_3\n",
      "47\n",
      "3\n",
      "60_7\n",
      "60\n",
      "7\n",
      "120_6\n",
      "120\n",
      "6\n",
      "131_1\n",
      "131\n",
      "1\n",
      "118_7\n",
      "118\n",
      "7\n",
      "25_8\n",
      "25\n",
      "8\n",
      "15_2\n",
      "15\n",
      "2\n",
      "64_8\n",
      "64\n",
      "8\n",
      "64_12\n",
      "64\n",
      "12\n",
      "30_2\n",
      "30\n",
      "2\n",
      "74_3\n",
      "74\n",
      "3\n",
      "34_1\n",
      "34\n",
      "1\n",
      "76_12\n",
      "76\n",
      "12\n",
      "27_7\n",
      "27\n",
      "7\n",
      "21_12\n",
      "21\n",
      "12\n",
      "79_6\n",
      "79\n",
      "6\n",
      "33_8\n",
      "33\n",
      "8\n",
      "71_5\n",
      "71\n",
      "5\n",
      "99_12\n",
      "99\n",
      "12\n",
      "32_3\n",
      "32\n",
      "3\n",
      "75_5\n",
      "75\n",
      "5\n",
      "1_12\n",
      "1\n",
      "12\n",
      "24_5\n",
      "24\n",
      "5\n",
      "126_2\n",
      "126\n",
      "2\n",
      "55_11\n",
      "55\n",
      "11\n",
      "38_5\n",
      "38\n",
      "5\n",
      "45_7\n",
      "45\n",
      "7\n",
      "73_12\n",
      "73\n",
      "12\n",
      "114_10\n",
      "114\n",
      "10\n",
      "62_3\n",
      "62\n",
      "3\n",
      "48_5\n",
      "48\n",
      "5\n",
      "16_5\n",
      "16\n",
      "5\n",
      "115_2\n",
      "115\n",
      "2\n",
      "47_10\n",
      "47\n",
      "10\n",
      "115_9\n",
      "115\n",
      "9\n",
      "114_11\n",
      "114\n",
      "11\n",
      "82_4\n",
      "82\n",
      "4\n",
      "37_10\n",
      "37\n",
      "10\n",
      "112_4\n",
      "112\n",
      "4\n",
      "45_10\n",
      "45\n",
      "10\n",
      "113_2\n",
      "113\n",
      "2\n",
      "56_3\n",
      "56\n",
      "3\n",
      "75_6\n",
      "75\n",
      "6\n",
      "101_1\n",
      "101\n",
      "1\n",
      "6_9\n",
      "6\n",
      "9\n",
      "88_6\n",
      "88\n",
      "6\n",
      "132_8\n",
      "132\n",
      "8\n",
      "105_6\n",
      "105\n",
      "6\n",
      "19_5\n",
      "19\n",
      "5\n",
      "44_9\n",
      "44\n",
      "9\n",
      "3_8\n",
      "3\n",
      "8\n",
      "31_4\n",
      "31\n",
      "4\n",
      "2_3\n",
      "2\n",
      "3\n",
      "137_11\n",
      "137\n",
      "11\n",
      "105_10\n",
      "105\n",
      "10\n",
      "105_2\n",
      "105\n",
      "2\n",
      "78_8\n",
      "78\n",
      "8\n",
      "6_6\n",
      "6\n",
      "6\n",
      "50_7\n",
      "50\n",
      "7\n",
      "67_2\n",
      "67\n",
      "2\n",
      "55_1\n",
      "55\n",
      "1\n",
      "112_6\n",
      "112\n",
      "6\n",
      "85_3\n",
      "85\n",
      "3\n",
      "99_8\n",
      "99\n",
      "8\n",
      "26_6\n",
      "26\n",
      "6\n",
      "2_11\n",
      "2\n",
      "11\n",
      "130_11\n",
      "130\n",
      "11\n",
      "117_6\n",
      "117\n",
      "6\n",
      "30_6\n",
      "30\n",
      "6\n",
      "109_12\n",
      "109\n",
      "12\n",
      "40_4\n",
      "40\n",
      "4\n",
      "58_1\n",
      "58\n",
      "1\n",
      "84_3\n",
      "84\n",
      "3\n",
      "23_2\n",
      "23\n",
      "2\n",
      "89_1\n",
      "89\n",
      "1\n",
      "24_2\n",
      "24\n",
      "2\n",
      "42_2\n",
      "42\n",
      "2\n",
      "34_2\n",
      "34\n",
      "2\n",
      "112_7\n",
      "112\n",
      "7\n",
      "94_4\n",
      "94\n",
      "4\n",
      "131_3\n",
      "131\n",
      "3\n",
      "50_12\n",
      "50\n",
      "12\n",
      "111_6\n",
      "111\n",
      "6\n",
      "3_11\n",
      "3\n",
      "11\n",
      "60_1\n",
      "60\n",
      "1\n",
      "14_5\n",
      "14\n",
      "5\n",
      "94_6\n",
      "94\n",
      "6\n",
      "23_3\n",
      "23\n",
      "3\n",
      "51_2\n",
      "51\n",
      "2\n",
      "17_6\n",
      "17\n",
      "6\n",
      "18_2\n",
      "18\n",
      "2\n",
      "39_1\n",
      "39\n",
      "1\n",
      "101_6\n",
      "101\n",
      "6\n",
      "91_4\n",
      "91\n",
      "4\n",
      "113_7\n",
      "113\n",
      "7\n",
      "77_11\n",
      "77\n",
      "11\n",
      "37_2\n",
      "37\n",
      "2\n",
      "112_9\n",
      "112\n",
      "9\n",
      "68_3\n",
      "68\n",
      "3\n",
      "80_11\n",
      "80\n",
      "11\n",
      "16_6\n",
      "16\n",
      "6\n",
      "47_9\n",
      "47\n",
      "9\n",
      "39_5\n",
      "39\n",
      "5\n",
      "68_4\n",
      "68\n",
      "4\n",
      "54_9\n",
      "54\n",
      "9\n",
      "123_11\n",
      "123\n",
      "11\n",
      "135_10\n",
      "135\n",
      "10\n",
      "25_7\n",
      "25\n",
      "7\n",
      "56_9\n",
      "56\n",
      "9\n",
      "43_3\n",
      "43\n",
      "3\n",
      "103_2\n",
      "103\n",
      "2\n",
      "55_5\n",
      "55\n",
      "5\n",
      "43_11\n",
      "43\n",
      "11\n",
      "118_10\n",
      "118\n",
      "10\n",
      "140_9\n",
      "140\n",
      "9\n",
      "50_5\n",
      "50\n",
      "5\n",
      "108_1\n",
      "108\n",
      "1\n",
      "100_5\n",
      "100\n",
      "5\n",
      "104_5\n",
      "104\n",
      "5\n",
      "129_9\n",
      "129\n",
      "9\n",
      "115_1\n",
      "115\n",
      "1\n",
      "114_6\n",
      "114\n",
      "6\n",
      "69_8\n",
      "69\n",
      "8\n",
      "133_4\n",
      "133\n",
      "4\n",
      "113_12\n",
      "113\n",
      "12\n",
      "92_9\n",
      "92\n",
      "9\n",
      "89_10\n",
      "89\n",
      "10\n",
      "75_10\n",
      "75\n",
      "10\n",
      "92_3\n",
      "92\n",
      "3\n",
      "38_12\n",
      "38\n",
      "12\n",
      "47_2\n",
      "47\n",
      "2\n",
      "87_6\n",
      "87\n",
      "6\n",
      "6_1\n",
      "6\n",
      "1\n",
      "33_1\n",
      "33\n",
      "1\n",
      "35_7\n",
      "35\n",
      "7\n",
      "7_11\n",
      "7\n",
      "11\n",
      "50_6\n",
      "50\n",
      "6\n",
      "30_5\n",
      "30\n",
      "5\n",
      "46_3\n",
      "46\n",
      "3\n",
      "45_12\n",
      "45\n",
      "12\n",
      "32_9\n",
      "32\n",
      "9\n",
      "126_3\n",
      "126\n",
      "3\n",
      "66_8\n",
      "66\n",
      "8\n",
      "12_12\n",
      "12\n",
      "12\n",
      "28_5\n",
      "28\n",
      "5\n",
      "27_3\n",
      "27\n",
      "3\n",
      "127_8\n",
      "127\n",
      "8\n",
      "42_5\n",
      "42\n",
      "5\n",
      "30_3\n",
      "30\n",
      "3\n",
      "60_4\n",
      "60\n",
      "4\n",
      "24_9\n",
      "24\n",
      "9\n",
      "22_3\n",
      "22\n",
      "3\n",
      "85_5\n",
      "85\n",
      "5\n",
      "32_2\n",
      "32\n",
      "2\n",
      "136_3\n",
      "136\n",
      "3\n",
      "18_7\n",
      "18\n",
      "7\n",
      "75_12\n",
      "75\n",
      "12\n",
      "55_8\n",
      "55\n",
      "8\n",
      "75_3\n",
      "75\n",
      "3\n",
      "140_10\n",
      "140\n",
      "10\n",
      "11_5\n",
      "11\n",
      "5\n",
      "130_3\n",
      "130\n",
      "3\n",
      "8_8\n",
      "8\n",
      "8\n",
      "61_4\n",
      "61\n",
      "4\n",
      "69_3\n",
      "69\n",
      "3\n",
      "38_6\n",
      "38\n",
      "6\n",
      "51_8\n",
      "51\n",
      "8\n",
      "64_5\n",
      "64\n",
      "5\n",
      "1_2\n",
      "1\n",
      "2\n",
      "51_9\n",
      "51\n",
      "9\n",
      "140_7\n",
      "140\n",
      "7\n",
      "95_12\n",
      "95\n",
      "12\n",
      "11_4\n",
      "11\n",
      "4\n",
      "115_6\n",
      "115\n",
      "6\n",
      "34_10\n",
      "34\n",
      "10\n",
      "86_10\n",
      "86\n",
      "10\n",
      "78_4\n",
      "78\n",
      "4\n",
      "121_7\n",
      "121\n",
      "7\n",
      "3_10\n",
      "3\n",
      "10\n",
      "18_9\n",
      "18\n",
      "9\n",
      "14_4\n",
      "14\n",
      "4\n",
      "138_1\n",
      "138\n",
      "1\n",
      "62_11\n",
      "62\n",
      "11\n",
      "6_3\n",
      "6\n",
      "3\n",
      "9_10\n",
      "9\n",
      "10\n",
      "91_2\n",
      "91\n",
      "2\n",
      "90_3\n",
      "90\n",
      "3\n",
      "88_11\n",
      "88\n",
      "11\n",
      "91_11\n",
      "91\n",
      "11\n",
      "108_5\n",
      "108\n",
      "5\n",
      "32_10\n",
      "32\n",
      "10\n",
      "67_9\n",
      "67\n",
      "9\n",
      "103_3\n",
      "103\n",
      "3\n",
      "2_2\n",
      "2\n",
      "2\n",
      "101_9\n",
      "101\n",
      "9\n",
      "2_6\n",
      "2\n",
      "6\n",
      "73_4\n",
      "73\n",
      "4\n",
      "15_4\n",
      "15\n",
      "4\n",
      "136_4\n",
      "136\n",
      "4\n",
      "116_8\n",
      "116\n",
      "8\n",
      "117_7\n",
      "117\n",
      "7\n",
      "84_4\n",
      "84\n",
      "4\n",
      "138_2\n",
      "138\n",
      "2\n",
      "21_5\n",
      "21\n",
      "5\n",
      "9_7\n",
      "9\n",
      "7\n",
      "20_7\n",
      "20\n",
      "7\n",
      "122_6\n",
      "122\n",
      "6\n",
      "36_7\n",
      "36\n",
      "7\n",
      "12_10\n",
      "12\n",
      "10\n",
      "33_9\n",
      "33\n",
      "9\n",
      "121_6\n",
      "121\n",
      "6\n",
      "29_10\n",
      "29\n",
      "10\n",
      "135_2\n",
      "135\n",
      "2\n",
      "83_2\n",
      "83\n",
      "2\n",
      "14_6\n",
      "14\n",
      "6\n",
      "54_8\n",
      "54\n",
      "8\n",
      "116_10\n",
      "116\n",
      "10\n",
      "53_4\n",
      "53\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# processed_images_path = '/home/rs/21CS91R01/research/2023_ICVGIP-Code/datasets/processed_images_path'\n",
    "# if not os.path.exists(processed_images_path):\n",
    "#     os.mkdir(processed_images_path)\n",
    "\n",
    "# List to store preprocessed images\n",
    "preprocessed_images_train = []\n",
    "labels_train = []\n",
    "\n",
    "preprocessed_images_test = []\n",
    "labels_test = []\n",
    "\n",
    "# Iterate over the images\n",
    "for image_name in os.listdir(images_path):\n",
    "    if image_name.endswith('.bmp'):\n",
    "        # Extract the label from the image name without considering variations\n",
    "        image_name_ex = image_name.split('.')[0]\n",
    "        print(image_name_ex)\n",
    "        image_no = image_name_ex.split('_')[0]\n",
    "        print(image_no)\n",
    "        index =image_name_ex.split('_')[1]\n",
    "        print(index)\n",
    "\n",
    "\n",
    "        # Read the image\n",
    "        image_path = os.path.join(images_path, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "     \n",
    "        # Resize the image\n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        # Convert the image to grayscale\n",
    "        # gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Normalize the image\n",
    "        normalized_image = resized_image\n",
    "        # distorted_image = random_distortion(resized_image)\n",
    "        # blurred_image = random_gaussian_blurring(resized_image)\n",
    "        # rotated_image = random_rotation(resized_image)\n",
    "        # scaled_image = random_scaling(resized_image)\n",
    "        # noisy_image = random_noise(resized_image)\n",
    "        # adjusted_image = random_contrast(resized_image)\n",
    "        # morphed_image = random_morphology(resized_image)\n",
    "\n",
    "\n",
    "        # Save the image\n",
    "        # processed_image_path = os.path.join(processed_images_path, image_name)\n",
    "        # cv2.imwrite(processed_image_path, normalized_image)\n",
    "\n",
    "        # Add the preprocessed image and label to the lists\n",
    "        if (int(index) >= 5 and int(index) <= 12):\n",
    "            preprocessed_images_train.append(np.array(normalized_image))\n",
    "            # preprocessed_images_train.append(np.array(distorted_image))\n",
    "            # preprocessed_images_train.append(np.array(blurred_image))\n",
    "            # preprocessed_images_train.append(np.array(rotated_image))\n",
    "            # preprocessed_images_train.append(np.array(scaled_image))\n",
    "            # preprocessed_images_train.append(np.array(adjusted_image))\n",
    "            # preprocessed_images_train.append(np.array(noisy_image))\n",
    "            # preprocessed_images_train.append(np.array(morphed_image))\n",
    "            labels_train.append(image_no)\n",
    "            # for _ in range(7):\n",
    "            #     labels_train.append(image_no)\n",
    "       \n",
    "         \n",
    "        else:\n",
    "            preprocessed_images_test.append(normalized_image)\n",
    "            labels_test.append(image_no)\n",
    "            # preprocessed_images_test.extend([distorted_image, blurred_image, rotated_image, scaled_image,adjusted_image,noisy_image,morphed_image])\n",
    "            # for _ in range(7):\n",
    "            #     labels_test.append(image_no)\n",
    "    # break\n",
    "\n",
    "        \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120, 224, 224, 3)\n",
      "(559, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of preprocessed images to a NumPy array\n",
    "\n",
    "preprocessed_images_train = np.array(preprocessed_images_train)\n",
    "preprocessed_images_test = np.array(preprocessed_images_test)\n",
    "print(preprocessed_images_train.shape)\n",
    "print(preprocessed_images_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120,)\n",
      "(559,)\n"
     ]
    }
   ],
   "source": [
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "print(labels_train.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "-========\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "numerical_labels_train = label_encoder.fit_transform(labels_train)\n",
    "numerical_labels_test = label_encoder.fit_transform(labels_test)\n",
    "\n",
    "\n",
    "# Convert numerical labels to one-hot encoded format\n",
    "one_hot_labels_train = to_categorical(numerical_labels_train)\n",
    "one_hot_labels_test = to_categorical(numerical_labels_test)\n",
    "\n",
    "print(one_hot_labels_train)\n",
    "print(\"-========\")\n",
    "print(one_hot_labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 16\n",
    "margin = 1  # Margin for constrastive loss.\n",
    "x_train = preprocessed_images_train\n",
    "y_train = one_hot_labels_train\n",
    "x_test = preprocessed_images_test\n",
    "y_test = one_hot_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]]], dtype=uint8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape=(1120, 224, 224, 3)\n",
      "y_train.shape=(1120, 140)\n",
      "x_test.shape=(559, 224, 224, 3)\n",
      "y_test.shape=(559, 140)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{x_test.shape=}\")\n",
    "print(f\"{y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    # The first two layers with 32 filters of window size 3x3\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224,224,3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = createModel()\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(x_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=50,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(100, activation='softmax')  # Assuming there are 10 unique labels\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(preprocessed_images_train, one_hot_labels_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Extract features from the preprocessed images using the trained CNN model\n",
    "features = model.predict(preprocessed_images_test)\n",
    "\n",
    "# Print the shape of the extracted features\n",
    "print(\"Extracted Features Shape:\", features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 193ms/step - loss: 15.6867 - accuracy: 0.0300\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.paperspace.com/attention-mechanisms-in-computer-vision-cbam/\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\n",
    "from keras import backend as K\n",
    "from keras.activations import sigmoid\n",
    "\n",
    "def attach_attention_module(net, attention_module):\n",
    "  if attention_module == 'se_block': # SE_block\n",
    "    net = se_block(net)\n",
    "  elif attention_module == 'cbam_block': # CBAM_block\n",
    "    net = cbam_block(net)\n",
    "  else:\n",
    "    raise Exception(\"'{}' is not supported attention module!\".format(attention_module))\n",
    "\n",
    "  return net\n",
    "\n",
    "def se_block(input_feature, ratio=8):\n",
    "\t\"\"\"Contains the implementation of Squeeze-and-Excitation(SE) block.\n",
    "\tAs described in https://arxiv.org/abs/1709.01507.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\n",
    "\tse_feature = GlobalAveragePooling2D()(input_feature)\n",
    "\tse_feature = Reshape((1, 1, channel))(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel)\n",
    "\tse_feature = Dense(channel // ratio,\n",
    "\t\t\t\t\t   activation='relu',\n",
    "\t\t\t\t\t   kernel_initializer='he_normal',\n",
    "\t\t\t\t\t   use_bias=True,\n",
    "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel//ratio)\n",
    "\tse_feature = Dense(channel,\n",
    "\t\t\t\t\t   activation='sigmoid',\n",
    "\t\t\t\t\t   kernel_initializer='he_normal',\n",
    "\t\t\t\t\t   use_bias=True,\n",
    "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel)\n",
    "\tif K.image_data_format() == 'channels_first':\n",
    "\t\tse_feature = Permute((3, 1, 2))(se_feature)\n",
    "\n",
    "\tse_feature = multiply([input_feature, se_feature])\n",
    "\treturn se_feature\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "\t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "\tAs described in https://arxiv.org/abs/1807.06521.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
    "\tcbam_feature = spatial_attention(cbam_feature)\n",
    "\treturn cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\t\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\t\n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\t\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "\tkernel_size = 7\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tchannel = input_feature.shape[1]\n",
    "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
    "\telse:\n",
    "\t\tchannel = input_feature.shape[-1]\n",
    "\t\tcbam_feature = input_feature\n",
    "\t\n",
    "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert avg_pool.shape[-1] == 1\n",
    "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert max_pool.shape[-1] == 1\n",
    "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\tassert concat.shape[-1] == 2\n",
    "\tcbam_feature = Conv2D(filters = 1,\n",
    "\t\t\t\t\tkernel_size=kernel_size,\n",
    "\t\t\t\t\tstrides=1,\n",
    "\t\t\t\t\tpadding='same',\n",
    "\t\t\t\t\tactivation='sigmoid',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tuse_bias=False)(concat)\t\n",
    "\tassert cbam_feature.shape[-1] == 1\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\t\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\t\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "Exception encountered when calling layer \"lambda\" (type Lambda).\n\nname 'K' is not defined\n\nCall arguments received by layer \"lambda\" (type Lambda):\n   inputs=tf.Tensor(shape=(None, 224, 224, 16), dtype=float32)\n   mask=None\n   training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2174658/2362015634.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the model using TensorFlow/Keras's load_model function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Now you can use the loaded_model to make predictions or continue training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mcbam_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mavg_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbam_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mavg_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mmax_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbam_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: Exception encountered when calling layer \"lambda\" (type Lambda).\n\nname 'K' is not defined\n\nCall arguments received by layer \"lambda\" (type Lambda):\n   inputs=tf.Tensor(shape=(None, 224, 224, 16), dtype=float32)\n   mask=None\n   training=None"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the path where the model was saved\n",
    "model_path = '/home/rs/21CS91R01/research/2023_ICVGIP-Code/baseline1/models/resnet_cbam.h5'\n",
    "\n",
    "# Load the model using TensorFlow/Keras's load_model function\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Now you can use the loaded_model to make predictions or continue training\n",
    "\n",
    "# Score trained model.\n",
    "scores = loaded_model.evaluate(x_test, y_test)\n",
    "print(\"Test loss:\", scores[0])\n",
    "print(\"Test accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet v1\n",
    "This is a revised implementation from Cifar10 ResNet example in Keras:\n",
    "(https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py)\n",
    "[a] Deep Residual Learning for Image Recognition\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "        x= Dropout(0.5)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=140, attention_module=None):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            # attention_module\n",
    "            if attention_module is not None:\n",
    "                y = attach_attention_module(y, attention_module)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "            # x= Dropout(0.5)(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    # print(y.shape)https://stackoverflow.com/questions/69802548/how-to-print-keras-tensor-values/69806679#69806679\n",
    "    \n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "data_augmentation = False\n",
    "num_classes = 10\n",
    "subtract_pixel_mean = True  # Subtracting pixel mean improves accuracy\n",
    "base_model = 'resnet20'\n",
    "# Choose what attention_module to use: cbam_block / se_block / None\n",
    "attention_module = 'se_block'\n",
    "model_type = base_model if attention_module==None else base_model+'_'+attention_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1120, 224, 224, 3)\n",
      "1120 train samples\n",
      "559 test samples\n",
      "y_train shape: (1120, 140)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Input\n",
    "depth = 20 # For ResNet, specify the depth (e.g. ResNet50: depth=50)\n",
    "model = resnet_v1(input_shape=input_shape, depth=depth, attention_module='cbam_block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 224, 224, 16  448         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 224, 224, 16  64         ['conv2d_3[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 224, 224, 16  0           ['batch_normalization_3[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 224, 224, 16  2320        ['activation_2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 224, 224, 16  64         ['conv2d_4[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 224, 224, 16  0           ['batch_normalization_4[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 224, 224, 16  2320        ['activation_3[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 224, 224, 16  64         ['conv2d_5[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 16)          0           ['batch_normalization_5[0][0]']  \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 16)          0           ['batch_normalization_5[0][0]']  \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 16)     0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1, 16)     0           ['global_max_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1, 1, 2)      34          ['reshape[0][0]',                \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1, 1, 16)     48          ['dense[0][0]',                  \n",
      "                                                                  'dense[1][0]']                  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 1, 1, 16)     0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_1[1][0]']                \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 1, 1, 16)     0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 224, 224, 16  0           ['batch_normalization_5[0][0]',  \n",
      "                                )                                 'activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 224, 224, 1)  0           ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 224, 224, 1)  0           ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 224, 224, 2)  0           ['lambda[0][0]',                 \n",
      "                                                                  'lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 224, 224, 1)  98          ['concatenate[0][0]']            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 224, 224, 16  0           ['multiply[0][0]',               \n",
      "                                )                                 'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 224, 224, 16  0           ['activation_2[0][0]',           \n",
      "                                )                                 'multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 224, 224, 16  0           ['add_1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 224, 224, 16  2320        ['activation_5[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 224, 224, 16  64         ['conv2d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 224, 224, 16  0           ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 224, 224, 16  2320        ['activation_6[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 224, 224, 16  64         ['conv2d_8[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 16)          0           ['batch_normalization_7[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_1 (Global  (None, 16)          0           ['batch_normalization_7[0][0]']  \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 1, 16)     0           ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 1, 1, 16)     0           ['global_max_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1, 1, 2)      34          ['reshape_2[0][0]',              \n",
      "                                                                  'reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1, 1, 16)     48          ['dense_2[0][0]',                \n",
      "                                                                  'dense_2[1][0]']                \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 1, 1, 16)     0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_3[1][0]']                \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 1, 1, 16)     0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 224, 224, 16  0           ['batch_normalization_7[0][0]',  \n",
      "                                )                                 'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 224, 224, 1)  0           ['multiply_2[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 224, 224, 1)  0           ['multiply_2[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 224, 224, 2)  0           ['lambda_2[0][0]',               \n",
      "                                                                  'lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 224, 224, 1)  98          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 224, 224, 16  0           ['multiply_2[0][0]',             \n",
      "                                )                                 'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 224, 224, 16  0           ['activation_5[0][0]',           \n",
      "                                )                                 'multiply_3[0][0]']             \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 224, 224, 16  0           ['add_3[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 224, 224, 16  2320        ['activation_8[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 224, 224, 16  64         ['conv2d_10[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 224, 224, 16  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 224, 224, 16  2320        ['activation_9[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 224, 224, 16  64         ['conv2d_11[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 16)          0           ['batch_normalization_9[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_2 (Global  (None, 16)          0           ['batch_normalization_9[0][0]']  \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 1, 1, 16)     0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1, 1, 16)     0           ['global_max_pooling2d_2[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1, 1, 2)      34          ['reshape_4[0][0]',              \n",
      "                                                                  'reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1, 1, 16)     48          ['dense_4[0][0]',                \n",
      "                                                                  'dense_4[1][0]']                \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 1, 1, 16)     0           ['dense_5[0][0]',                \n",
      "                                                                  'dense_5[1][0]']                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 1, 1, 16)     0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply_4 (Multiply)          (None, 224, 224, 16  0           ['batch_normalization_9[0][0]',  \n",
      "                                )                                 'activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 224, 224, 1)  0           ['multiply_4[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 224, 224, 1)  0           ['multiply_4[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 224, 224, 2)  0           ['lambda_4[0][0]',               \n",
      "                                                                  'lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 224, 224, 1)  98          ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_5 (Multiply)          (None, 224, 224, 16  0           ['multiply_4[0][0]',             \n",
      "                                )                                 'conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 224, 224, 16  0           ['activation_8[0][0]',           \n",
      "                                )                                 'multiply_5[0][0]']             \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 224, 224, 16  0           ['add_5[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 112, 112, 32  4640        ['activation_11[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 112, 112, 32  128        ['conv2d_13[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 112, 112, 32  0           ['batch_normalization_10[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 112, 112, 32  9248        ['activation_12[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 112, 112, 32  128        ['conv2d_14[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 32)          0           ['batch_normalization_11[0][0]'] \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_3 (Global  (None, 32)          0           ['batch_normalization_11[0][0]'] \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 1, 1, 32)     0           ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 1, 1, 32)     0           ['global_max_pooling2d_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1, 1, 4)      132         ['reshape_6[0][0]',              \n",
      "                                                                  'reshape_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1, 1, 32)     160         ['dense_6[0][0]',                \n",
      "                                                                  'dense_6[1][0]']                \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 1, 1, 32)     0           ['dense_7[0][0]',                \n",
      "                                                                  'dense_7[1][0]']                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 1, 1, 32)     0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply_6 (Multiply)          (None, 112, 112, 32  0           ['batch_normalization_11[0][0]', \n",
      "                                )                                 'activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 112, 112, 1)  0           ['multiply_6[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 112, 112, 1)  0           ['multiply_6[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 112, 112, 2)  0           ['lambda_6[0][0]',               \n",
      "                                                                  'lambda_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 112, 112, 1)  98          ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 112, 112, 32  544         ['activation_11[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " multiply_7 (Multiply)          (None, 112, 112, 32  0           ['multiply_6[0][0]',             \n",
      "                                )                                 'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 112, 112, 32  0           ['conv2d_15[0][0]',              \n",
      "                                )                                 'multiply_7[0][0]']             \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 112, 112, 32  0           ['add_7[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 112, 112, 32  9248        ['activation_14[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 112, 112, 32  128        ['conv2d_17[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 112, 112, 32  0           ['batch_normalization_12[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 112, 112, 32  9248        ['activation_15[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 112, 112, 32  128        ['conv2d_18[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4 (Gl  (None, 32)          0           ['batch_normalization_13[0][0]'] \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_4 (Global  (None, 32)          0           ['batch_normalization_13[0][0]'] \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)            (None, 1, 1, 32)     0           ['global_average_pooling2d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)            (None, 1, 1, 32)     0           ['global_max_pooling2d_4[0][0]'] \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1, 1, 4)      132         ['reshape_8[0][0]',              \n",
      "                                                                  'reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1, 1, 32)     160         ['dense_8[0][0]',                \n",
      "                                                                  'dense_8[1][0]']                \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 1, 1, 32)     0           ['dense_9[0][0]',                \n",
      "                                                                  'dense_9[1][0]']                \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 1, 1, 32)     0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply_8 (Multiply)          (None, 112, 112, 32  0           ['batch_normalization_13[0][0]', \n",
      "                                )                                 'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_8 (Lambda)              (None, 112, 112, 1)  0           ['multiply_8[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_9 (Lambda)              (None, 112, 112, 1)  0           ['multiply_8[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 112, 112, 2)  0           ['lambda_8[0][0]',               \n",
      "                                                                  'lambda_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 112, 112, 1)  98          ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_9 (Multiply)          (None, 112, 112, 32  0           ['multiply_8[0][0]',             \n",
      "                                )                                 'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 112, 112, 32  0           ['activation_14[0][0]',          \n",
      "                                )                                 'multiply_9[0][0]']             \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 112, 112, 32  0           ['add_9[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 112, 112, 32  9248        ['activation_17[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 112, 112, 32  128        ['conv2d_20[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 112, 112, 32  0           ['batch_normalization_14[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 112, 112, 32  9248        ['activation_18[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 112, 112, 32  128        ['conv2d_21[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_5 (Gl  (None, 32)          0           ['batch_normalization_15[0][0]'] \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_5 (Global  (None, 32)          0           ['batch_normalization_15[0][0]'] \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)           (None, 1, 1, 32)     0           ['global_average_pooling2d_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " reshape_11 (Reshape)           (None, 1, 1, 32)     0           ['global_max_pooling2d_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1, 1, 4)      132         ['reshape_10[0][0]',             \n",
      "                                                                  'reshape_11[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1, 1, 32)     160         ['dense_10[0][0]',               \n",
      "                                                                  'dense_10[1][0]']               \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 1, 1, 32)     0           ['dense_11[0][0]',               \n",
      "                                                                  'dense_11[1][0]']               \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 1, 1, 32)     0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_10 (Multiply)         (None, 112, 112, 32  0           ['batch_normalization_15[0][0]', \n",
      "                                )                                 'activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_10 (Lambda)             (None, 112, 112, 1)  0           ['multiply_10[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_11 (Lambda)             (None, 112, 112, 1)  0           ['multiply_10[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 112, 112, 2)  0           ['lambda_10[0][0]',              \n",
      "                                                                  'lambda_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 112, 112, 1)  98          ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_11 (Multiply)         (None, 112, 112, 32  0           ['multiply_10[0][0]',            \n",
      "                                )                                 'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 112, 112, 32  0           ['activation_17[0][0]',          \n",
      "                                )                                 'multiply_11[0][0]']            \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 112, 112, 32  0           ['add_11[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 56, 56, 64)   18496       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 56, 56, 64)  256         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 56, 56, 64)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 56, 56, 64)  256         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_6 (Gl  (None, 64)          0           ['batch_normalization_17[0][0]'] \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_6 (Global  (None, 64)          0           ['batch_normalization_17[0][0]'] \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " reshape_12 (Reshape)           (None, 1, 1, 64)     0           ['global_average_pooling2d_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " reshape_13 (Reshape)           (None, 1, 1, 64)     0           ['global_max_pooling2d_6[0][0]'] \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1, 1, 8)      520         ['reshape_12[0][0]',             \n",
      "                                                                  'reshape_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1, 1, 64)     576         ['dense_12[0][0]',               \n",
      "                                                                  'dense_12[1][0]']               \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 1, 1, 64)     0           ['dense_13[0][0]',               \n",
      "                                                                  'dense_13[1][0]']               \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 1, 1, 64)     0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_12 (Multiply)         (None, 56, 56, 64)   0           ['batch_normalization_17[0][0]', \n",
      "                                                                  'activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_12 (Lambda)             (None, 56, 56, 1)    0           ['multiply_12[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_13 (Lambda)             (None, 56, 56, 1)    0           ['multiply_12[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 56, 56, 2)    0           ['lambda_12[0][0]',              \n",
      "                                                                  'lambda_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 56, 56, 1)    98          ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 56, 56, 64)   2112        ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_13 (Multiply)         (None, 56, 56, 64)   0           ['multiply_12[0][0]',            \n",
      "                                                                  'conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 56, 56, 64)   0           ['conv2d_25[0][0]',              \n",
      "                                                                  'multiply_13[0][0]']            \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 56, 56, 64)   0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 56, 56, 64)  256         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 56, 56, 64)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 56, 56, 64)  256         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_7 (Gl  (None, 64)          0           ['batch_normalization_19[0][0]'] \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_7 (Global  (None, 64)          0           ['batch_normalization_19[0][0]'] \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " reshape_14 (Reshape)           (None, 1, 1, 64)     0           ['global_average_pooling2d_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " reshape_15 (Reshape)           (None, 1, 1, 64)     0           ['global_max_pooling2d_7[0][0]'] \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1, 1, 8)      520         ['reshape_14[0][0]',             \n",
      "                                                                  'reshape_15[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 1, 1, 64)     576         ['dense_14[0][0]',               \n",
      "                                                                  'dense_14[1][0]']               \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 1, 1, 64)     0           ['dense_15[0][0]',               \n",
      "                                                                  'dense_15[1][0]']               \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 1, 1, 64)     0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_14 (Multiply)         (None, 56, 56, 64)   0           ['batch_normalization_19[0][0]', \n",
      "                                                                  'activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_14 (Lambda)             (None, 56, 56, 1)    0           ['multiply_14[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_15 (Lambda)             (None, 56, 56, 1)    0           ['multiply_14[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 56, 56, 2)    0           ['lambda_14[0][0]',              \n",
      "                                                                  'lambda_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 56, 56, 1)    98          ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_15 (Multiply)         (None, 56, 56, 64)   0           ['multiply_14[0][0]',            \n",
      "                                                                  'conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 56, 56, 64)   0           ['activation_23[0][0]',          \n",
      "                                                                  'multiply_15[0][0]']            \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 56, 56, 64)   0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 56, 56, 64)  256         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 56, 56, 64)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 56, 56, 64)  256         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_8 (Gl  (None, 64)          0           ['batch_normalization_21[0][0]'] \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_8 (Global  (None, 64)          0           ['batch_normalization_21[0][0]'] \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " reshape_16 (Reshape)           (None, 1, 1, 64)     0           ['global_average_pooling2d_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " reshape_17 (Reshape)           (None, 1, 1, 64)     0           ['global_max_pooling2d_8[0][0]'] \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 1, 1, 8)      520         ['reshape_16[0][0]',             \n",
      "                                                                  'reshape_17[0][0]']             \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1, 1, 64)     576         ['dense_16[0][0]',               \n",
      "                                                                  'dense_16[1][0]']               \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 1, 1, 64)     0           ['dense_17[0][0]',               \n",
      "                                                                  'dense_17[1][0]']               \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 1, 1, 64)     0           ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_16 (Multiply)         (None, 56, 56, 64)   0           ['batch_normalization_21[0][0]', \n",
      "                                                                  'activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_16 (Lambda)             (None, 56, 56, 1)    0           ['multiply_16[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_17 (Lambda)             (None, 56, 56, 1)    0           ['multiply_16[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 56, 56, 2)    0           ['lambda_16[0][0]',              \n",
      "                                                                  'lambda_17[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 56, 56, 1)    98          ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_17 (Multiply)         (None, 56, 56, 64)   0           ['multiply_16[0][0]',            \n",
      "                                                                  'conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 56, 56, 64)   0           ['activation_26[0][0]',          \n",
      "                                                                  'multiply_17[0][0]']            \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 56, 56, 64)   0           ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 7, 7, 64)    0           ['activation_29[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3136)         0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 140)          439180      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 718,264\n",
      "Trainable params: 716,888\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 543s 30s/step - loss: 5.7436 - accuracy: 0.0063 - val_loss: 5.1383 - val_accuracy: 0.0072\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 500s 29s/step - loss: 4.7108 - accuracy: 0.0598 - val_loss: 5.3857 - val_accuracy: 0.0072\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 483s 27s/step - loss: 3.4366 - accuracy: 0.2723 - val_loss: 19.3630 - val_accuracy: 0.0072\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 490s 28s/step - loss: 1.9856 - accuracy: 0.5241 - val_loss: 15.8641 - val_accuracy: 0.0143\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 440s 25s/step - loss: 1.0970 - accuracy: 0.7366 - val_loss: 13.6311 - val_accuracy: 0.0161\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 491s 28s/step - loss: 0.6182 - accuracy: 0.8661 - val_loss: 14.1396 - val_accuracy: 0.0483\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 502s 28s/step - loss: 0.3863 - accuracy: 0.9402 - val_loss: 12.2375 - val_accuracy: 0.0877\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 519s 29s/step - loss: 0.2980 - accuracy: 0.9607 - val_loss: 7.9916 - val_accuracy: 0.1306\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 528s 30s/step - loss: 0.2596 - accuracy: 0.9714 - val_loss: 4.4325 - val_accuracy: 0.3256\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 449s 25s/step - loss: 0.2014 - accuracy: 0.9920 - val_loss: 4.4263 - val_accuracy: 0.3059\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 443s 25s/step - loss: 0.1741 - accuracy: 0.9973 - val_loss: 3.4430 - val_accuracy: 0.3864\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 454s 25s/step - loss: 0.1653 - accuracy: 0.9964 - val_loss: 3.6027 - val_accuracy: 0.3936\n",
      "Epoch 13/50\n",
      " 1/18 [>.............................] - ETA: 5:08 - loss: 0.1576 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "resnet_model = model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=50,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 741s 40s/step - loss: 28.0828 - accuracy: 0.0394\n",
      "Test loss: 28.082801818847656\n",
      "Test accuracy: 0.03935599327087402\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.datacamp.com/tutorial/convolutional-neural-networks-python\n",
    "accuracy = fashion_train_dropout.history['acc']\n",
    "val_accuracy = fashion_train_dropout.history['val_acc']\n",
    "loss = fashion_train_dropout.history['loss']\n",
    "val_loss = fashion_train_dropout.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Hyper-Parameters\n",
    "\n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "optimizer = Adam(learning_rate=lr) #lr = 1e-4,\n",
    "# optimizer = RMSprop(learning_rate=lr) #lr = 1e-5\n",
    "# optimizer = SGD(learning_rate=lr) #lr = 1e-2\n",
    "# optimizer = tfa.optimizers.SGDW(learning_rate=lr, momentum=0.9, nesterov=True, weight_decay=1e-4) #lr = 1e-3\n",
    "batch_size = 50\n",
    "input_shape = x_test.shape\n",
    "input_shape = (224,224,3)\n",
    "output_num_units = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Layer, Conv2D, MaxPooling2D, BatchNormalization, ZeroPadding2D\n",
    "from tensorflow.keras.metrics import Precision, Recall, TruePositives\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, \\\n",
    "EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.nn import local_response_normalization #, conv2d\n",
    "\n",
    "# import tensorflow_probability as tfp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from typing import Any, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Localization(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 filters_1: int, \n",
    "                 filters_2: int, \n",
    "                 fc_units: int, \n",
    "                 kernel_size=(5,5),\n",
    "                 pool_size=(2,2),\n",
    "                 name='localization', \n",
    "                 **kwargs):\n",
    "        super(Localization, self).__init__(**kwargs)\n",
    "        self.filters_1 = filters_1\n",
    "        self.filters_2 = filters_2\n",
    "        self.fc_units = fc_units\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.network = keras.Sequential(\n",
    "            [\n",
    "                MaxPooling2D(pool_size=pool_size, name=name+'_mp_1'),\n",
    "                Conv2D(filters=filters_1, \n",
    "                       kernel_size=kernel_size, \n",
    "                       padding='same', \n",
    "                       strides=1, \n",
    "                       activation='relu',\n",
    "                       name=name+'_c_1'),\n",
    "                MaxPooling2D(pool_size=pool_size, name=name+'_mp_2'),\n",
    "                Conv2D(filters=filters_2, \n",
    "                       kernel_size=kernel_size, \n",
    "                       padding='same', \n",
    "                       strides=1, \n",
    "                       activation='relu',\n",
    "                       name=name+'_c_2'),\n",
    "                MaxPooling2D(pool_size=pool_size, name=name+'_mp_3'),\n",
    "                Flatten(name=name+'_fl'),\n",
    "                Dense(fc_units, activation='relu', name=name+'_d_1'),\n",
    "                Dense(6, activation=None, \n",
    "                      bias_initializer=tf.keras.initializers.constant\\\n",
    "                      ([1.0, 0.0, 0.0, 0.0, 1.0, 0.0]), \n",
    "                      kernel_initializer='zeros',\n",
    "                      name=name+'_d_2'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(\"Building Localization Network with input shape:\", input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [None, 6]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        theta = self.network(inputs)\n",
    "        theta = tf.keras.layers.Reshape((2, 3))(theta)\n",
    "        return theta\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Localization, self).get_config()\n",
    "        config.update({\n",
    "            'filters_1': self.filters_1,\n",
    "            'filters_2': self.filters_2,\n",
    "            'fc_units': self.fc_units,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'pool_size': self.pool_size,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearInterpolation(tf.keras.layers.Layer):\n",
    "    def __init__(self, height=48, width=48):\n",
    "        super(BilinearInterpolation, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [None, self.height, self.width, 1]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'height': self.height,\n",
    "            'width': self.width,\n",
    "        }\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        print(\"Building Bilinear Interpolation Layer with input shape:\", input_shape)\n",
    "\n",
    "    def advance_indexing(self, inputs, x, y):\n",
    "        '''\n",
    "        Utility function to get pixel value for coordinate\n",
    "        vectors x and y from a  4D tensor image.\n",
    "        '''        \n",
    "        shape = tf.shape(inputs)\n",
    "        batch_size, _, _ = shape[0], shape[1], shape[2]\n",
    "        \n",
    "        batch_idx = tf.range(0, batch_size)\n",
    "        batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
    "        b = tf.tile(batch_idx, (1, self.height, self.width))\n",
    "        indices = tf.stack([b, y, x], 3)\n",
    "        return tf.gather_nd(inputs, indices)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        images, theta = inputs\n",
    "        sampling_grid = self.grid_generator(batch=tf.shape(images)[0])\n",
    "        return self.interpolate(images, sampling_grid, theta)\n",
    "\n",
    "    def grid_generator(self, batch):\n",
    "        '''\n",
    "        This function returns a sampling grid, which when\n",
    "        used with the bilinear sampler on the input feature\n",
    "        map, will create an output feature map that is an\n",
    "        affine transformation of the input feature map.\n",
    "        '''\n",
    "        # create normalized 2D grid\n",
    "        x = tf.linspace(-1, 1, self.width)\n",
    "        y = tf.linspace(-1, 1, self.height)\n",
    "        # x and y are selected in the range of -1 to 1 so the the transformation happens considering the center\n",
    "        # of the image as the origin. The images will be later scaled up.\n",
    "        xx, yy = tf.meshgrid(x, y)\n",
    "            \n",
    "        # flatten\n",
    "        xx = tf.reshape(xx, (-1,))\n",
    "        yy = tf.reshape(yy, (-1,))\n",
    "\n",
    "        # reshape to [x_t, y_t , 1] - (homogeneous form)\n",
    "        homogenous_coordinates = tf.stack([xx, yy, tf.ones_like(xx)])\n",
    "        # # repeat grid num_batch times\n",
    "        # sampling_grid = np.resize(sampling_grid, (num_batch, 3, H*W))\n",
    "        # repeat grid num_batch times\n",
    "        homogenous_coordinates = tf.expand_dims(homogenous_coordinates, axis=0)\n",
    "        homogenous_coordinates = tf.tile(homogenous_coordinates, tf.stack([batch, 1, 1]))\n",
    "        # homogenous_coordinates = tf.tile(homogenous_coordinates, [batch, 1, 1])\n",
    "\n",
    "        # cast to float32 (required for matmul)\n",
    "        homogenous_coordinates = tf.cast(homogenous_coordinates, dtype=tf.float32)\n",
    "\n",
    "        return homogenous_coordinates\n",
    "    \n",
    "    def interpolate(self, images, grid, theta):\n",
    "        '''\n",
    "        Performs bilinear sampling of the input images according to the\n",
    "        normalized coordinates provided by the sampling grid. Note that\n",
    "        the sampling is done identically for each channel of the input.\n",
    "        '''\n",
    "\n",
    "        with tf.name_scope(\"Transformation\"):\n",
    "            # transform the sampling grid - batch multiply\n",
    "            transformed = tf.matmul(theta, grid)\n",
    "            # batch grid has shape (num_batch, 2, H*W)\n",
    "\n",
    "            # reshape to (num_batch, H, W, 2)\n",
    "            transformed = tf.transpose(transformed, perm=[0, 2, 1])\n",
    "            transformed = tf.reshape(transformed, [-1, self.height, self.width, 2])\n",
    "                \n",
    "            x_transformed = transformed[:, :, :, 0]\n",
    "            y_transformed = transformed[:, :, :, 1]\n",
    "                \n",
    "            # rescale x and y to [0, W-1/H-1]\n",
    "            x = ((x_transformed + 1.) * tf.cast(self.width, dtype=tf.float32)) * 0.5\n",
    "            y = ((y_transformed + 1.) * tf.cast(self.height, dtype=tf.float32)) * 0.5\n",
    "\n",
    "        with tf.name_scope(\"VariableCasting\"):\n",
    "            # grab 4 nearest corner points for each (x_i, y_i)\n",
    "            x0 = tf.cast(tf.math.floor(x), dtype=tf.int32)\n",
    "            x1 = x0 + 1\n",
    "            y0 = tf.cast(tf.math.floor(y), dtype=tf.int32)\n",
    "            y1 = y0 + 1\n",
    "\n",
    "            # clip to range [0, H-1/W-1] to not violate img boundaries\n",
    "            x0 = tf.clip_by_value(x0, 0, self.width-1)\n",
    "            x1 = tf.clip_by_value(x1, 0, self.width-1)\n",
    "            y0 = tf.clip_by_value(y0, 0, self.height-1)\n",
    "            y1 = tf.clip_by_value(y1, 0, self.height-1)\n",
    "            x = tf.clip_by_value(x, 0, tf.cast(self.width, dtype=tf.float32)-1.0)\n",
    "            y = tf.clip_by_value(y, 0, tf.cast(self.height, dtype=tf.float32)-1)\n",
    "\n",
    "        with tf.name_scope(\"AdvanceIndexing\"):\n",
    "            # get pixel value at corner coords\n",
    "            Ia = self.advance_indexing(images, x0, y0)\n",
    "            Ib = self.advance_indexing(images, x0, y1)\n",
    "            Ic = self.advance_indexing(images, x1, y0)\n",
    "            Id = self.advance_indexing(images, x1, y1)\n",
    "\n",
    "        with tf.name_scope(\"Interpolation\"):\n",
    "            # recast as float for delta calculation\n",
    "            x0 = tf.cast(x0, dtype=tf.float32)\n",
    "            x1 = tf.cast(x1, dtype=tf.float32)\n",
    "            y0 = tf.cast(y0, dtype=tf.float32)\n",
    "            y1 = tf.cast(y1, dtype=tf.float32)\n",
    "                            \n",
    "            # calculate deltas\n",
    "            wa = (x1-x) * (y1-y)\n",
    "            wb = (x1-x) * (y-y0)\n",
    "            wc = (x-x0) * (y1-y)\n",
    "            wd = (x-x0) * (y-y0)\n",
    "\n",
    "            # add dimension for addition\n",
    "            wa = tf.expand_dims(wa, axis=3)\n",
    "            wb = tf.expand_dims(wb, axis=3)\n",
    "            wc = tf.expand_dims(wc, axis=3)\n",
    "            wd = tf.expand_dims(wd, axis=3)\n",
    "                        \n",
    "        return tf.math.add_n([wa*Ia + wb*Ib + wc*Ic + wd*Id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_block(filters, kernel_size, pool_size=(2, 2), name='zcmn'):\n",
    "    def layer_fn(x):\n",
    "        zeropad = ZeroPadding2D(padding=2, \n",
    "                                name=name+'_zp')(x)\n",
    "        conv = Conv2D(filters=filters, \n",
    "                    kernel_size=kernel_size, \n",
    "                    padding='valid', \n",
    "                    strides=1, \n",
    "                    activation='relu', \n",
    "                    name=name+'_c')(zeropad)   \n",
    "        maxpool = MaxPooling2D(pool_size=pool_size, \n",
    "                                strides=2, \n",
    "                                padding='valid',\n",
    "                                name=name+'_mp')(conv)       \n",
    "        norm = local_response_normalization(maxpool, name=name+'_lrn')\n",
    "\n",
    "        return norm\n",
    "    return layer_fn\n",
    "\n",
    "def STN(input_shape=input_shape, num_classes=output_num_units):\n",
    "    image = Input(shape=input_shape)\n",
    "    theta = Localization(250, 250, 250, name='localization_0')(image)\n",
    "    spatial_transformer_network_1 = BilinearInterpolation(height=input_shape[0], \n",
    "                                                          width=input_shape[1])([image, theta])    \n",
    "    conv_1 = conv_block(200, (7,7), name='zcmn_0')(spatial_transformer_network_1)\n",
    "    theta = Localization(150, 200, 300, name='localization_1')(conv_1)\n",
    "    spatial_transformer_network_2 = BilinearInterpolation(height=23, \n",
    "                                                          width=23)([conv_1, theta])        \n",
    "    conv_2 = conv_block(250, (4,4), name='zcmn_1')(spatial_transformer_network_2)\n",
    "    theta = Localization(150, 200, 300, name='localization_2')(conv_2)\n",
    "    spatial_transformer_network_3 = BilinearInterpolation(height=12, \n",
    "                                                          width=12)([conv_2, theta])         \n",
    "    conv_3 = conv_block(350, (4,4), name='zcmn_2')(spatial_transformer_network_3)\n",
    "    flatten = Flatten()(conv_3)\n",
    "    dense = Dense(units= 400, activation='relu')(flatten)\n",
    "    predictions = Dense(units=num_classes, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(inputs=image, outputs=predictions)\n",
    "    return model\n",
    " \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Localization Network with input shape: (None, 224, 224, 3)\n",
      "Building Bilinear Interpolation Layer with input shape: [TensorShape([None, 224, 224, 3]), TensorShape([None, 2, 3])]\n",
      "Building Localization Network with input shape: (None, 111, 111, 200)\n",
      "Building Bilinear Interpolation Layer with input shape: [TensorShape([None, 111, 111, 200]), TensorShape([None, 2, 3])]\n",
      "Building Localization Network with input shape: (None, 12, 12, 250)\n",
      "Building Bilinear Interpolation Layer with input shape: [TensorShape([None, 12, 12, 250]), TensorShape([None, 2, 3])]\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " localization (Localization)    (None, 2, 3)         50583506    ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " bilinear_interpolation (Biline  (None, 224, 224, 3)  0          ['input_12[0][0]',               \n",
      " arInterpolation)                                                 'localization[0][0]']           \n",
      "                                                                                                  \n",
      " zcmn_0_zp (ZeroPadding2D)      (None, 228, 228, 3)  0           ['bilinear_interpolation[0][0]'] \n",
      "                                                                                                  \n",
      " zcmn_0_c (Conv2D)              (None, 222, 222, 20  29600       ['zcmn_0_zp[0][0]']              \n",
      "                                0)                                                                \n",
      "                                                                                                  \n",
      " zcmn_0_mp (MaxPooling2D)       (None, 111, 111, 20  0           ['zcmn_0_c[0][0]']               \n",
      "                                0)                                                                \n",
      "                                                                                                  \n",
      " tf.nn.local_response_normaliza  (None, 111, 111, 20  0          ['zcmn_0_mp[0][0]']              \n",
      " tion_9 (TFOpLambda)            0)                                                                \n",
      "                                                                                                  \n",
      " localization_1 (Localization)  (None, 2, 3)         11642456    ['tf.nn.local_response_normalizat\n",
      "                                                                 ion_9[0][0]']                    \n",
      "                                                                                                  \n",
      " bilinear_interpolation_1 (Bili  (None, 23, 23, 200)  0          ['tf.nn.local_response_normalizat\n",
      " nearInterpolation)                                              ion_9[0][0]',                    \n",
      "                                                                  'localization_1[0][0]']         \n",
      "                                                                                                  \n",
      " zcmn_1_zp (ZeroPadding2D)      (None, 27, 27, 200)  0           ['bilinear_interpolation_1[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " zcmn_1_c (Conv2D)              (None, 24, 24, 250)  800250      ['zcmn_1_zp[0][0]']              \n",
      "                                                                                                  \n",
      " zcmn_1_mp (MaxPooling2D)       (None, 12, 12, 250)  0           ['zcmn_1_c[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.local_response_normaliza  (None, 12, 12, 250)  0          ['zcmn_1_mp[0][0]']              \n",
      " tion_10 (TFOpLambda)                                                                             \n",
      "                                                                                                  \n",
      " localization_2 (Localization)  (None, 2, 3)         1749956     ['tf.nn.local_response_normalizat\n",
      "                                                                 ion_10[0][0]']                   \n",
      "                                                                                                  \n",
      " bilinear_interpolation_2 (Bili  (None, 12, 12, 250)  0          ['tf.nn.local_response_normalizat\n",
      " nearInterpolation)                                              ion_10[0][0]',                   \n",
      "                                                                  'localization_2[0][0]']         \n",
      "                                                                                                  \n",
      " zcmn_2_zp (ZeroPadding2D)      (None, 16, 16, 250)  0           ['bilinear_interpolation_2[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " zcmn_2_c (Conv2D)              (None, 13, 13, 350)  1400350     ['zcmn_2_zp[0][0]']              \n",
      "                                                                                                  \n",
      " zcmn_2_mp (MaxPooling2D)       (None, 6, 6, 350)    0           ['zcmn_2_c[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.local_response_normaliza  (None, 6, 6, 350)   0           ['zcmn_2_mp[0][0]']              \n",
      " tion_11 (TFOpLambda)                                                                             \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 12600)        0           ['tf.nn.local_response_normalizat\n",
      "                                                                 ion_11[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 400)          5040400     ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 140)          56140       ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 71,302,658\n",
      "Trainable params: 71,302,658\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = STN()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 244s 10s/step - loss: 4.9449 - accuracy: 8.9286e-04\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 236s 10s/step - loss: 4.9417 - accuracy: 0.0063\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 236s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 237s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 235s 10s/step - loss: 4.9417 - accuracy: 0.0054\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 238s 10s/step - loss: 4.9417 - accuracy: 0.0054\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 237s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 236s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 235s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 238s 10s/step - loss: 4.9417 - accuracy: 0.0036\n"
     ]
    }
   ],
   "source": [
    "  #Train model\n",
    "hist = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    epochs=10, \n",
    "                  \n",
    "                    batch_size=batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper-Parameters\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "optimizer = Adam(learning_rate=lr) #lr = 1e-3\n",
    "# optimizer = RMSprop(learning_rate=lr) #lr = 1e-3\n",
    "# optimizer = SGD(learning_rate=lr) #1e-1 \n",
    "# optimizer = tfa.optimizers.SGDW(learning_rate=lr, momentum=0.9, nesterov=True, weight_decay=1e-4) #lr = 1e-2\n",
    "\n",
    "batch_size = 50\n",
    "num_classes = 140\n",
    "input_shape = x_test.shape\n",
    "input_shape = (224,224,3)\n",
    "# input_shape = (48, 48, 3)\n",
    "image_size = input_shape[0]  # We'll resize input images to this size\n",
    "patch_size = 4 # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 32\n",
    "num_heads = 6\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 4\n",
    "mlp_head_units = [512, 256]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Patches(Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'patch_size': self.patch_size,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projection = Dense(units=projection_dim)\n",
    "        self.position_embedding = Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'num_patches': self.num_patches,\n",
    "            'projection_dim': self.projection_dim,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fanisspr/traffic-sign-recognition/blob/master/ipynb/CNN_with_STNs-DFG.ipynb\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomRotation(factor=0.02),\n",
    "        tf.keras.layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = Flatten()(representation)\n",
    "    representation = Dropout(0.5)(representation) \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = Dense(num_classes, activation='softmax')(features)\n",
    "    # Create the Keras model.\n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 224, 224, 3)  0          ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " patches_1 (Patches)            (None, None, 48)     0           ['data_augmentation[0][0]']      \n",
      "                                                                                                  \n",
      " patch_encoder_1 (PatchEncoder)  (None, 3136, 32)    101920      ['patches_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 3136, 32)    64          ['patch_encoder_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 3136, 32)    25184       ['layer_normalization_9[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 3136, 32)     0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'patch_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 3136, 32)    64          ['add_8[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 3136, 64)     2112        ['layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 3136, 64)     0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 3136, 32)     2080        ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 3136, 32)     0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 3136, 32)     0           ['dropout_16[0][0]',             \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 3136, 32)    64          ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 3136, 32)    25184       ['layer_normalization_11[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 3136, 32)     0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 3136, 32)    64          ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 3136, 64)     2112        ['layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 3136, 64)     0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 3136, 32)     2080        ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 3136, 32)     0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 3136, 32)     0           ['dropout_18[0][0]',             \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 3136, 32)    64          ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 3136, 32)    25184       ['layer_normalization_13[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 3136, 32)     0           ['multi_head_attention_6[0][0]', \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 3136, 32)    64          ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 3136, 64)     2112        ['layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 3136, 64)     0           ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 3136, 32)     2080        ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 3136, 32)     0           ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 3136, 32)     0           ['dropout_20[0][0]',             \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 3136, 32)    64          ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 3136, 32)    25184       ['layer_normalization_15[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 3136, 32)     0           ['multi_head_attention_7[0][0]', \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 3136, 32)    64          ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 3136, 64)     2112        ['layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 3136, 64)     0           ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 3136, 32)     2080        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 3136, 32)     0           ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 3136, 32)     0           ['dropout_22[0][0]',             \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 3136, 32)    64          ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 100352)       0           ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 100352)       0           ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 512)          51380736    ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 512)          0           ['dense_35[0][0]']               \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 256)          131328      ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 256)          0           ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 140)          35980       ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 51,768,044\n",
      "Trainable params: 51,768,044\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_vit_classifier()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 15:45:43.115093: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (mklcpu) ran out of memory trying to allocate 10.99GiB (rounded to 11801395200)requested by op model_9/multi_head_attention_5/einsum/Einsum\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-06-29 15:45:43.115187: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for mklcpu\n",
      "2023-06-29 15:45:43.115214: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115229: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115243: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115256: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115276: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 57, Chunks in use: 57. 265.0KiB allocated for chunks. 265.0KiB in use in bin. 257.5KiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115294: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 72, Chunks in use: 72. 632.5KiB allocated for chunks. 632.5KiB in use in bin. 590.1KiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115310: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 107, Chunks in use: 107. 2.58MiB allocated for chunks. 2.58MiB in use in bin. 2.54MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115327: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 9, Chunks in use: 8. 364.0KiB allocated for chunks. 318.8KiB in use in bin. 247.8KiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115344: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 9, Chunks in use: 9. 738.2KiB allocated for chunks. 738.2KiB in use in bin. 654.6KiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115359: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 22, Chunks in use: 22. 4.05MiB allocated for chunks. 4.05MiB in use in bin. 3.52MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115373: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 13, Chunks in use: 12. 4.17MiB allocated for chunks. 3.78MiB in use in bin. 3.30MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115390: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 26, Chunks in use: 25. 15.48MiB allocated for chunks. 14.80MiB in use in bin. 13.49MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115407: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 16, Chunks in use: 16. 18.38MiB allocated for chunks. 18.38MiB in use in bin. 15.27MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115423: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 23, Chunks in use: 23. 63.93MiB allocated for chunks. 63.93MiB in use in bin. 54.65MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115440: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 49, Chunks in use: 47. 311.36MiB allocated for chunks. 297.03MiB in use in bin. 279.69MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115457: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 21, Chunks in use: 21. 206.72MiB allocated for chunks. 206.72MiB in use in bin. 172.54MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115475: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 15, Chunks in use: 15. 314.76MiB allocated for chunks. 314.76MiB in use in bin. 296.94MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115492: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 8, Chunks in use: 8. 303.09MiB allocated for chunks. 303.09MiB in use in bin. 288.31MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115522: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 7, Chunks in use: 7. 803.91MiB allocated for chunks. 803.91MiB in use in bin. 803.91MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115537: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 8, Chunks in use: 8. 1.54GiB allocated for chunks. 1.54GiB in use in bin. 1.26GiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115549: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 13, Chunks in use: 10. 59.26GiB allocated for chunks. 37.66GiB in use in bin. 37.29GiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115560: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 10.99GiB was 256.00MiB, Chunk State: \n",
      "2023-06-29 15:45:43.115578: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 5.01GiB | Requested Size: 588.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 10.99GiB | Requested Size: 10.99GiB | in_use: 1 | bin_num: -1\n",
      "2023-06-29 15:45:43.115591: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 7.78GiB | Requested Size: 588.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 114.84MiB | Requested Size: 114.84MiB | in_use: 1 | bin_num: -1\n",
      "2023-06-29 15:45:43.115603: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 8.82GiB | Requested Size: 588.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 10.99GiB | Requested Size: 10.99GiB | in_use: 1 | bin_num: -1\n",
      "2023-06-29 15:45:43.115611: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 2097152\n",
      "2023-06-29 15:45:43.115623: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e41c0 of size 4096 next 82\n",
      "2023-06-29 15:45:43.115632: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e51c0 of size 4096 next 83\n",
      "2023-06-29 15:45:43.115641: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e61c0 of size 4096 next 87\n",
      "2023-06-29 15:45:43.115649: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e71c0 of size 4096 next 88\n",
      "2023-06-29 15:45:43.115658: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e81c0 of size 12288 next 91\n",
      "2023-06-29 15:45:43.115667: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59eb1c0 of size 8192 next 92\n",
      "2023-06-29 15:45:43.115675: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59ed1c0 of size 8192 next 95\n",
      "2023-06-29 15:45:43.115684: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59ef1c0 of size 8192 next 96\n",
      "2023-06-29 15:45:43.115692: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59f11c0 of size 8192 next 97\n",
      "2023-06-29 15:45:43.115701: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59f31c0 of size 13824 next 2\n",
      "2023-06-29 15:45:43.115710: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59f67c0 of size 16384 next 125\n",
      "2023-06-29 15:45:43.115719: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fa7c0 of size 4096 next 74\n",
      "2023-06-29 15:45:43.115727: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fb7c0 of size 4096 next 69\n",
      "2023-06-29 15:45:43.115736: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fc7c0 of size 4096 next 76\n",
      "2023-06-29 15:45:43.115744: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fd7c0 of size 4096 next 80\n",
      "2023-06-29 15:45:43.115753: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fe7c0 of size 4096 next 81\n",
      "2023-06-29 15:45:43.115762: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59ff7c0 of size 4864 next 41\n",
      "2023-06-29 15:45:43.115771: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a00ac0 of size 4096 next 44\n",
      "2023-06-29 15:45:43.115779: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a01ac0 of size 4096 next 48\n",
      "2023-06-29 15:45:43.115788: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a02ac0 of size 4096 next 49\n",
      "2023-06-29 15:45:43.115797: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a03ac0 of size 4096 next 45\n",
      "2023-06-29 15:45:43.115813: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a04ac0 of size 4096 next 51\n",
      "2023-06-29 15:45:43.115822: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a05ac0 of size 4096 next 59\n",
      "2023-06-29 15:45:43.115831: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a06ac0 of size 4096 next 6\n",
      "2023-06-29 15:45:43.115840: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a07ac0 of size 4096 next 7\n",
      "2023-06-29 15:45:43.115848: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a08ac0 of size 4096 next 36\n",
      "2023-06-29 15:45:43.115856: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a09ac0 of size 4096 next 39\n",
      "2023-06-29 15:45:43.115864: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0aac0 of size 4096 next 3\n",
      "2023-06-29 15:45:43.115873: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0bac0 of size 4096 next 64\n",
      "2023-06-29 15:45:43.115881: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0cac0 of size 4096 next 66\n",
      "2023-06-29 15:45:43.115889: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0dac0 of size 4096 next 67\n",
      "2023-06-29 15:45:43.115897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0eac0 of size 4096 next 1\n",
      "2023-06-29 15:45:43.115906: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0fac0 of size 65536 next 4\n",
      "2023-06-29 15:45:43.115914: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a1fac0 of size 4096 next 57\n",
      "2023-06-29 15:45:43.115923: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a20ac0 of size 143360 next 99\n",
      "2023-06-29 15:45:43.115931: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a43ac0 of size 147456 next 8\n",
      "2023-06-29 15:45:43.115940: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a67ac0 of size 65536 next 126\n",
      "2023-06-29 15:45:43.115948: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a77ac0 of size 81920 next 9\n",
      "2023-06-29 15:45:43.115957: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a8bac0 of size 65536 next 12\n",
      "2023-06-29 15:45:43.115965: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a9bac0 of size 229376 next 13\n",
      "2023-06-29 15:45:43.115974: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5ad3ac0 of size 147456 next 14\n",
      "2023-06-29 15:45:43.115982: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5af7ac0 of size 8192 next 17\n",
      "2023-06-29 15:45:43.115991: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5af9ac0 of size 8192 next 104\n",
      "2023-06-29 15:45:43.115999: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5afbac0 of size 8192 next 111\n",
      "2023-06-29 15:45:43.116007: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5afdac0 of size 8192 next 107\n",
      "2023-06-29 15:45:43.116015: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5affac0 of size 8192 next 112\n",
      "2023-06-29 15:45:43.116023: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b01ac0 of size 8192 next 113\n",
      "2023-06-29 15:45:43.116032: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b03ac0 of size 8192 next 109\n",
      "2023-06-29 15:45:43.116040: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b05ac0 of size 8192 next 119\n",
      "2023-06-29 15:45:43.116048: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b07ac0 of size 8192 next 101\n",
      "2023-06-29 15:45:43.116057: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b09ac0 of size 8192 next 115\n",
      "2023-06-29 15:45:43.116065: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b0bac0 of size 24576 next 365\n",
      "2023-06-29 15:45:43.116074: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b11ac0 of size 40960 next 15\n",
      "2023-06-29 15:45:43.116083: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b1bac0 of size 147456 next 16\n",
      "2023-06-29 15:45:43.116091: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b3fac0 of size 262144 next 26\n",
      "2023-06-29 15:45:43.116100: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b7fac0 of size 411392 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116108: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 4194304\n",
      "2023-06-29 15:45:43.116116: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5be4200 of size 8192 next 117\n",
      "2023-06-29 15:45:43.116125: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5be6200 of size 8192 next 120\n",
      "2023-06-29 15:45:43.116133: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5be8200 of size 8192 next 121\n",
      "2023-06-29 15:45:43.116141: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bea200 of size 8192 next 122\n",
      "2023-06-29 15:45:43.116149: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bec200 of size 8192 next 123\n",
      "2023-06-29 15:45:43.116158: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bee200 of size 37632 next 124\n",
      "2023-06-29 15:45:43.116166: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bf7500 of size 6912 next 40\n",
      "2023-06-29 15:45:43.116174: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bf9000 of size 8192 next 348\n",
      "2023-06-29 15:45:43.116183: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bfb000 of size 8192 next 147\n",
      "2023-06-29 15:45:43.116191: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bfd000 of size 6144 next 38\n",
      "2023-06-29 15:45:43.116200: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bfe800 of size 6400 next 114\n",
      "2023-06-29 15:45:43.116209: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5c00100 of size 35840 next 106\n",
      "2023-06-29 15:45:43.116217: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5c08d00 of size 111872 next 22\n",
      "2023-06-29 15:45:43.116225: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5c24200 of size 262144 next 19\n",
      "2023-06-29 15:45:43.116234: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5c64200 of size 1966080 next 27\n",
      "2023-06-29 15:45:43.116243: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5e44200 of size 262144 next 25\n",
      "2023-06-29 15:45:43.116252: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5e84200 of size 655360 next 28\n",
      "2023-06-29 15:45:43.116261: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5f24200 of size 786432 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116270: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 8388608\n",
      "2023-06-29 15:45:43.116278: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6005780 of size 262144 next 32\n",
      "2023-06-29 15:45:43.116287: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6045780 of size 1179648 next 35\n",
      "2023-06-29 15:45:43.116297: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6165780 of size 327680 next 33\n",
      "2023-06-29 15:45:43.116306: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 61b5780 of size 589824 next 34\n",
      "2023-06-29 15:45:43.116314: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6245780 of size 524288 next 42\n",
      "2023-06-29 15:45:43.116322: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 62c5780 of size 2621440 next 46\n",
      "2023-06-29 15:45:43.116332: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6545780 of size 1048576 next 47\n",
      "2023-06-29 15:45:43.116341: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6645780 of size 1835008 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116349: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 16777216\n",
      "2023-06-29 15:45:43.116358: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 686ba40 of size 224000 next 232\n",
      "2023-06-29 15:45:43.116366: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68a2540 of size 240128 next 152\n",
      "2023-06-29 15:45:43.116375: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68dcf40 of size 24576 next 352\n",
      "2023-06-29 15:45:43.116383: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68e2f40 of size 24576 next 324\n",
      "2023-06-29 15:45:43.116391: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68e8f40 of size 24576 next 294\n",
      "2023-06-29 15:45:43.116400: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68eef40 of size 24576 next 297\n",
      "2023-06-29 15:45:43.116408: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68f4f40 of size 24576 next 277\n",
      "2023-06-29 15:45:43.116416: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68faf40 of size 24576 next 296\n",
      "2023-06-29 15:45:43.116424: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6900f40 of size 24576 next 176\n",
      "2023-06-29 15:45:43.116432: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6906f40 of size 24576 next 376\n",
      "2023-06-29 15:45:43.116440: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 690cf40 of size 24576 next 23\n",
      "2023-06-29 15:45:43.116449: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6912f40 of size 24576 next 298\n",
      "2023-06-29 15:45:43.116457: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6918f40 of size 24576 next 281\n",
      "2023-06-29 15:45:43.116465: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 691ef40 of size 24576 next 379\n",
      "2023-06-29 15:45:43.116473: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6924f40 of size 24576 next 312\n",
      "2023-06-29 15:45:43.116481: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 692af40 of size 40960 next 361\n",
      "2023-06-29 15:45:43.116490: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6934f40 of size 224000 next 100\n",
      "2023-06-29 15:45:43.116498: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 696ba40 of size 524288 next 103\n",
      "2023-06-29 15:45:43.116507: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 69eba40 of size 884736 next 89\n",
      "2023-06-29 15:45:43.116515: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ac3a40 of size 81920 next 181\n",
      "2023-06-29 15:45:43.116524: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ad7a40 of size 4608 next 183\n",
      "2023-06-29 15:45:43.116532: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ad8c40 of size 4608 next 187\n",
      "2023-06-29 15:45:43.116540: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ad9e40 of size 4608 next 188\n",
      "2023-06-29 15:45:43.116549: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6adb040 of size 6656 next 168\n",
      "2023-06-29 15:45:43.116558: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6adca40 of size 24576 next 334\n",
      "2023-06-29 15:45:43.116566: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ae2a40 of size 24576 next 291\n",
      "2023-06-29 15:45:43.116574: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ae8a40 of size 8192 next 370\n",
      "2023-06-29 15:45:43.116583: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6aeaa40 of size 8192 next 309\n",
      "2023-06-29 15:45:43.116591: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6aeca40 of size 8192 next 171\n",
      "2023-06-29 15:45:43.116599: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6aeea40 of size 4096 next 220\n",
      "2023-06-29 15:45:43.116608: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6aefa40 of size 4096 next 223\n",
      "2023-06-29 15:45:43.116616: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6af0a40 of size 4096 next 224\n",
      "2023-06-29 15:45:43.116624: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6af1a40 of size 6144 next 198\n",
      "2023-06-29 15:45:43.116632: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6af3240 of size 18432 next 202\n",
      "2023-06-29 15:45:43.116642: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6af7a40 of size 28672 next 185\n",
      "2023-06-29 15:45:43.116651: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6afea40 of size 32768 next 186\n",
      "2023-06-29 15:45:43.116661: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b06a40 of size 9216 next 196\n",
      "2023-06-29 15:45:43.116670: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b08e40 of size 9216 next 197\n",
      "2023-06-29 15:45:43.116679: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b0b240 of size 9216 next 199\n",
      "2023-06-29 15:45:43.116687: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b0d640 of size 9216 next 200\n",
      "2023-06-29 15:45:43.116696: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b0fa40 of size 4096 next 216\n",
      "2023-06-29 15:45:43.116705: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b10a40 of size 4096 next 218\n",
      "2023-06-29 15:45:43.116714: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b11a40 of size 4096 next 219\n",
      "2023-06-29 15:45:43.116722: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b12a40 of size 4096 next 144\n",
      "2023-06-29 15:45:43.116732: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b13a40 of size 223232 next 162\n",
      "2023-06-29 15:45:43.116741: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b4a240 of size 8192 next 184\n",
      "2023-06-29 15:45:43.116750: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b4c240 of size 4608 next 191\n",
      "2023-06-29 15:45:43.116759: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b4d440 of size 4608 next 192\n",
      "2023-06-29 15:45:43.116768: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b4e640 of size 21504 next 146\n",
      "2023-06-29 15:45:43.116778: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b53a40 of size 589824 next 105\n",
      "2023-06-29 15:45:43.116787: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6be3a40 of size 2916352 next 60\n",
      "2023-06-29 15:45:43.116798: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6eaba40 of size 2621440 next 54\n",
      "2023-06-29 15:45:43.116807: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 712ba40 of size 2359296 next 55\n",
      "2023-06-29 15:45:43.116817: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 736ba40 of size 1048576 next 62\n",
      "2023-06-29 15:45:43.116827: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 746ba40 of size 1310720 next 61\n",
      "2023-06-29 15:45:43.116837: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 75aba40 of size 2883584 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116846: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 33554432\n",
      "2023-06-29 15:45:43.116856: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 79e0640 of size 1048576 next 71\n",
      "2023-06-29 15:45:43.116865: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7ae0640 of size 3670016 next 70\n",
      "2023-06-29 15:45:43.116875: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7e60640 of size 5767168 next 86\n",
      "2023-06-29 15:45:43.116885: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 83e0640 of size 1310720 next 84\n",
      "2023-06-29 15:45:43.116895: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 8520640 of size 2359296 next 85\n",
      "2023-06-29 15:45:43.116905: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 8760640 of size 2097152 next 98\n",
      "2023-06-29 15:45:43.116914: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 8960640 of size 4718592 next 79\n",
      "2023-06-29 15:45:43.116924: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 8de0640 of size 12582912 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116933: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 67108864\n",
      "2023-06-29 15:45:43.116942: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 9b3e380 of size 9437184 next 72\n",
      "2023-06-29 15:45:43.116952: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at a43e380 of size 9437184 next 63\n",
      "2023-06-29 15:45:43.116962: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at ad3e380 of size 9437184 next 139\n",
      "2023-06-29 15:45:43.116972: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at b63e380 of size 3145728 next 116\n",
      "2023-06-29 15:45:43.116982: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at b93e380 of size 5242880 next 108\n",
      "2023-06-29 15:45:43.116992: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at be3e380 of size 9437184 next 110\n",
      "2023-06-29 15:45:43.117001: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at c73e380 of size 9437184 next 90\n",
      "2023-06-29 15:45:43.117011: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d03e380 of size 100352 next 31\n",
      "2023-06-29 15:45:43.117020: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d056b80 of size 143360 next 77\n",
      "2023-06-29 15:45:43.117029: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d079b80 of size 935936 next 5\n",
      "2023-06-29 15:45:43.117038: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d15e380 of size 1179648 next 78\n",
      "2023-06-29 15:45:43.117048: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d27e380 of size 2359296 next 75\n",
      "2023-06-29 15:45:43.117057: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d4be380 of size 524288 next 94\n",
      "2023-06-29 15:45:43.117065: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d53e380 of size 524288 next 102\n",
      "2023-06-29 15:45:43.117074: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d5be380 of size 294912 next 153\n",
      "2023-06-29 15:45:43.117083: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d606380 of size 2195456 next 73\n",
      "2023-06-29 15:45:43.117092: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d81e380 of size 3276800 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117100: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 33076834304\n",
      "2023-06-29 15:45:43.117109: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6964772040 of size 11801395200 next 452\n",
      "2023-06-29 15:45:43.117118: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6c23e22040 of size 11801395200 next 449\n",
      "2023-06-29 15:45:43.117127: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f6ee34d2040 of size 9474043904 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117135: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 17179869184\n",
      "2023-06-29 15:45:43.117143: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7137ffb040 of size 11801395200 next 468\n",
      "2023-06-29 15:45:43.117152: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f73f76ab040 of size 5378473984 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117160: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 8589934592\n",
      "2023-06-29 15:45:43.117169: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7547ffd040 of size 120422400 next 472\n",
      "2023-06-29 15:45:43.117178: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f754f2d5040 of size 120422400 next 469\n",
      "2023-06-29 15:45:43.117186: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f75565ad040 of size 8349089792 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117194: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 4294967296\n",
      "2023-06-29 15:45:43.117203: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f774ffff040 of size 2950348800 next 453\n",
      "2023-06-29 15:45:43.117212: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f77ffdab040 of size 20070400 next 441\n",
      "2023-06-29 15:45:43.117221: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78010cf040 of size 20070400 next 454\n",
      "2023-06-29 15:45:43.117230: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78023f3040 of size 20070400 next 462\n",
      "2023-06-29 15:45:43.117238: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7803717040 of size 40140800 next 463\n",
      "2023-06-29 15:45:43.117247: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7805d5f040 of size 40140800 next 461\n",
      "2023-06-29 15:45:43.117256: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78083a7040 of size 40140800 next 460\n",
      "2023-06-29 15:45:43.117264: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f780a9ef040 of size 40140800 next 457\n",
      "2023-06-29 15:45:43.117272: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f780d037040 of size 20070400 next 464\n",
      "2023-06-29 15:45:43.117281: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f780e35b040 of size 20070400 next 456\n",
      "2023-06-29 15:45:43.117289: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f780f67f040 of size 10035200 next 451\n",
      "2023-06-29 15:45:43.117299: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7810011040 of size 20070400 next 467\n",
      "2023-06-29 15:45:43.117307: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7811335040 of size 20070400 next 466\n",
      "2023-06-29 15:45:43.117315: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7812659040 of size 26869760 next 363\n",
      "2023-06-29 15:45:43.117324: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7813ff9040 of size 4199936 next 336\n",
      "2023-06-29 15:45:43.117334: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78143fa640 of size 20160000 next 24\n",
      "2023-06-29 15:45:43.117343: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7815734440 of size 296750080 next 305\n",
      "2023-06-29 15:45:43.117352: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7827235040 of size 5600000 next 356\n",
      "2023-06-29 15:45:43.117361: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f782778c340 of size 205520896 next 368\n",
      "2023-06-29 15:45:43.117370: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7833b8c340 of size 205520896 next 272\n",
      "2023-06-29 15:45:43.117379: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f783ff8c340 of size 7526400 next 438\n",
      "2023-06-29 15:45:43.117388: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78406b9b40 of size 7526400 next 443\n",
      "2023-06-29 15:45:43.117396: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7840de7340 of size 30105600 next 450\n",
      "2023-06-29 15:45:43.117405: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7842a9d340 of size 20070400 next 444\n",
      "2023-06-29 15:45:43.117413: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7843dc1340 of size 20070400 next 447\n",
      "2023-06-29 15:45:43.117422: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78450e5340 of size 183606528 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117430: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 2147483648\n",
      "2023-06-29 15:45:43.117439: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a3fff040 of size 7526400 next 432\n",
      "2023-06-29 15:45:43.117447: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a472c840 of size 7526400 next 414\n",
      "2023-06-29 15:45:43.117456: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a4e5a040 of size 7526400 next 424\n",
      "2023-06-29 15:45:43.117464: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a5587840 of size 7526400 next 425\n",
      "2023-06-29 15:45:43.117473: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a5cb5040 of size 7526400 next 407\n",
      "2023-06-29 15:45:43.117481: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a63e2840 of size 7526400 next 413\n",
      "2023-06-29 15:45:43.117490: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a6b10040 of size 7526400 next 415\n",
      "2023-06-29 15:45:43.117498: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a723d840 of size 7526400 next 409\n",
      "2023-06-29 15:45:43.117506: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a796b040 of size 7526400 next 403\n",
      "2023-06-29 15:45:43.117515: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a8098840 of size 7526400 next 406\n",
      "2023-06-29 15:45:43.117523: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a87c6040 of size 7526400 next 431\n",
      "2023-06-29 15:45:43.117531: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a8ef3840 of size 35569408 next 271\n",
      "2023-06-29 15:45:43.117540: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ab0df740 of size 4860416 next 269\n",
      "2023-06-29 15:45:43.117549: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ab582140 of size 3561728 next 157\n",
      "2023-06-29 15:45:43.117558: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ab8e7a40 of size 226048 next 306\n",
      "2023-06-29 15:45:43.117568: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ab91ed40 of size 263617536 next 292\n",
      "2023-06-29 15:45:43.117576: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78bb486940 of size 240128 next 320\n",
      "2023-06-29 15:45:43.117585: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78bb4c1340 of size 120422400 next 446\n",
      "2023-06-29 15:45:43.117593: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78c2799340 of size 120422400 next 445\n",
      "2023-06-29 15:45:43.117601: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78c9a71340 of size 120422400 next 455\n",
      "2023-06-29 15:45:43.117610: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78d0d49340 of size 20070400 next 474\n",
      "2023-06-29 15:45:43.117618: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78d206d340 of size 120422400 next 473\n",
      "2023-06-29 15:45:43.117627: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78d9345340 of size 120422400 next 475\n",
      "2023-06-29 15:45:43.117635: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78e061d340 of size 226612736 next 284\n",
      "2023-06-29 15:45:43.117644: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ede3a940 of size 5779712 next 367\n",
      "2023-06-29 15:45:43.117653: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ee3bda40 of size 7526400 next 392\n",
      "2023-06-29 15:45:43.117661: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78eeaeb240 of size 7526400 next 394\n",
      "2023-06-29 15:45:43.117670: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ef218a40 of size 7526400 next 396\n",
      "2023-06-29 15:45:43.117678: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ef946240 of size 7526400 next 397\n",
      "2023-06-29 15:45:43.117686: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f0073a40 of size 7526400 next 399\n",
      "2023-06-29 15:45:43.117694: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f07a1240 of size 7526400 next 401\n",
      "2023-06-29 15:45:43.117703: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f0ecea40 of size 7526400 next 433\n",
      "2023-06-29 15:45:43.117711: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f15fc240 of size 7526400 next 437\n",
      "2023-06-29 15:45:43.117719: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f1d29a40 of size 11468288 next 131\n",
      "2023-06-29 15:45:43.117728: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f2819840 of size 196000000 next 179\n",
      "2023-06-29 15:45:43.117737: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78fe305140 of size 309080576 next 165\n",
      "2023-06-29 15:45:43.117747: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79109c8340 of size 325283072 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117755: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 1073741824\n",
      "2023-06-29 15:45:43.117763: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79a3ffe040 of size 439040000 next 251\n",
      "2023-06-29 15:45:43.117774: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79be2b1840 of size 439040000 next 254\n",
      "2023-06-29 15:45:43.117782: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79d8565040 of size 195661824 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117790: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 536870912\n",
      "2023-06-29 15:45:43.117799: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79e3fff040 of size 40560128 next 137\n",
      "2023-06-29 15:45:43.117808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79e66ad640 of size 40560128 next 150\n",
      "2023-06-29 15:45:43.117817: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79e8d5bc40 of size 40560128 next 43\n",
      "2023-06-29 15:45:43.117825: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79eb40a240 of size 3000064 next 145\n",
      "2023-06-29 15:45:43.117834: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79eb6e6940 of size 5600000 next 143\n",
      "2023-06-29 15:45:43.117842: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ebc3dc40 of size 3000064 next 250\n",
      "2023-06-29 15:45:43.117851: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ebf1a340 of size 8559872 next 133\n",
      "2023-06-29 15:45:43.117859: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ec744040 of size 7526400 next 364\n",
      "2023-06-29 15:45:43.117868: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ece71840 of size 7526400 next 330\n",
      "2023-06-29 15:45:43.117877: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ed59f040 of size 11698176 next 233\n",
      "2023-06-29 15:45:43.117885: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ee0c7040 of size 5600000 next 243\n",
      "2023-06-29 15:45:43.117894: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ee61e340 of size 174191360 next 248\n",
      "2023-06-29 15:45:43.117903: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79f8c3d640 of size 5600000 next 253\n",
      "2023-06-29 15:45:43.117911: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79f9194940 of size 7526400 next 341\n",
      "2023-06-29 15:45:43.117920: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79f98c2140 of size 7526400 next 384\n",
      "2023-06-29 15:45:43.117928: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79f9fef940 of size 7526400 next 385\n",
      "2023-06-29 15:45:43.117936: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fa71d140 of size 7526400 next 386\n",
      "2023-06-29 15:45:43.117944: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fae4a940 of size 7526400 next 387\n",
      "2023-06-29 15:45:43.117953: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fb578140 of size 13798400 next 355\n",
      "2023-06-29 15:45:43.117962: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fc2a0d40 of size 4014080 next 366\n",
      "2023-06-29 15:45:43.117971: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f79fc674d40 of size 7526400 next 375\n",
      "2023-06-29 15:45:43.117980: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fcda2540 of size 12293120 next 275\n",
      "2023-06-29 15:45:43.117989: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fd95b940 of size 6250240 next 180\n",
      "2023-06-29 15:45:43.117998: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fdf51840 of size 11200000 next 155\n",
      "2023-06-29 15:45:43.118007: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fe9ffe40 of size 524288 next 369\n",
      "2023-06-29 15:45:43.118016: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fea7fe40 of size 627200 next 335\n",
      "2023-06-29 15:45:43.118026: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feb19040 of size 401408 next 378\n",
      "2023-06-29 15:45:43.118035: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feb7b040 of size 1048576 next 30\n",
      "2023-06-29 15:45:43.118043: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fec7b040 of size 524288 next 260\n",
      "2023-06-29 15:45:43.118052: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fecfb040 of size 24576 next 340\n",
      "2023-06-29 15:45:43.118060: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed01040 of size 24576 next 265\n",
      "2023-06-29 15:45:43.118069: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed07040 of size 24576 next 351\n",
      "2023-06-29 15:45:43.118077: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed0d040 of size 24576 next 170\n",
      "2023-06-29 15:45:43.118085: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed13040 of size 24576 next 174\n",
      "2023-06-29 15:45:43.118093: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed19040 of size 24576 next 175\n",
      "2023-06-29 15:45:43.118102: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed1f040 of size 8192 next 268\n",
      "2023-06-29 15:45:43.118110: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed21040 of size 8192 next 317\n",
      "2023-06-29 15:45:43.118118: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed23040 of size 8192 next 329\n",
      "2023-06-29 15:45:43.118127: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed25040 of size 8192 next 266\n",
      "2023-06-29 15:45:43.118135: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed27040 of size 524288 next 353\n",
      "2023-06-29 15:45:43.118143: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feda7040 of size 524288 next 283\n",
      "2023-06-29 15:45:43.118152: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee27040 of size 143360 next 325\n",
      "2023-06-29 15:45:43.118160: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee4a040 of size 143360 next 350\n",
      "2023-06-29 15:45:43.118168: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee6d040 of size 12544 next 314\n",
      "2023-06-29 15:45:43.118178: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee70140 of size 8960 next 373\n",
      "2023-06-29 15:45:43.118187: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee72440 of size 8960 next 354\n",
      "2023-06-29 15:45:43.118196: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee74740 of size 8960 next 333\n",
      "2023-06-29 15:45:43.118205: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee76a40 of size 8960 next 435\n",
      "2023-06-29 15:45:43.118213: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee78d40 of size 8960 next 418\n",
      "2023-06-29 15:45:43.118221: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee7b040 of size 10240 next 148\n",
      "2023-06-29 15:45:43.118230: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee7d840 of size 28160 next 374\n",
      "2023-06-29 15:45:43.118240: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee84640 of size 28160 next 258\n",
      "2023-06-29 15:45:43.118248: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee8b440 of size 28160 next 270\n",
      "2023-06-29 15:45:43.118257: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee92240 of size 28160 next 380\n",
      "2023-06-29 15:45:43.118265: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee99040 of size 28160 next 264\n",
      "2023-06-29 15:45:43.118274: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee9fe40 of size 28160 next 382\n",
      "2023-06-29 15:45:43.118283: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feea6c40 of size 28160 next 339\n",
      "2023-06-29 15:45:43.118291: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feeada40 of size 28160 next 388\n",
      "2023-06-29 15:45:43.118300: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feeb4840 of size 28160 next 389\n",
      "2023-06-29 15:45:43.118308: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feebb640 of size 28160 next 390\n",
      "2023-06-29 15:45:43.118317: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feec2440 of size 28160 next 391\n",
      "2023-06-29 15:45:43.118325: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feec9240 of size 28160 next 393\n",
      "2023-06-29 15:45:43.118333: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feed0040 of size 28160 next 395\n",
      "2023-06-29 15:45:43.118342: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feed6e40 of size 28160 next 398\n",
      "2023-06-29 15:45:43.118350: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feeddc40 of size 28160 next 429\n",
      "2023-06-29 15:45:43.118359: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feee4a40 of size 28160 next 421\n",
      "2023-06-29 15:45:43.118367: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feeeb840 of size 28160 next 428\n",
      "2023-06-29 15:45:43.118376: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feef2640 of size 28160 next 426\n",
      "2023-06-29 15:45:43.118384: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feef9440 of size 28160 next 420\n",
      "2023-06-29 15:45:43.118393: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef00240 of size 28160 next 423\n",
      "2023-06-29 15:45:43.118401: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef07040 of size 28160 next 422\n",
      "2023-06-29 15:45:43.118410: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef0de40 of size 28160 next 408\n",
      "2023-06-29 15:45:43.118418: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef14c40 of size 28160 next 411\n",
      "2023-06-29 15:45:43.118426: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef1ba40 of size 28160 next 416\n",
      "2023-06-29 15:45:43.118435: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef22840 of size 28160 next 405\n",
      "2023-06-29 15:45:43.118443: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef29640 of size 28160 next 412\n",
      "2023-06-29 15:45:43.118451: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef30440 of size 28160 next 430\n",
      "2023-06-29 15:45:43.118460: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef37240 of size 28160 next 427\n",
      "2023-06-29 15:45:43.118468: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef3e040 of size 28160 next 440\n",
      "2023-06-29 15:45:43.118477: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef44e40 of size 28160 next 439\n",
      "2023-06-29 15:45:43.118485: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f79fef4bc40 of size 46336 next 128\n",
      "2023-06-29 15:45:43.118493: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef57140 of size 22108160 next 288\n",
      "2023-06-29 15:45:43.118502: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a0046c940 of size 30105600 next 285\n",
      "2023-06-29 15:45:43.118511: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02122940 of size 28160 next 400\n",
      "2023-06-29 15:45:43.118519: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02129740 of size 28160 next 402\n",
      "2023-06-29 15:45:43.118528: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02130540 of size 627200 next 436\n",
      "2023-06-29 15:45:43.118536: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f7a021c9740 of size 401408 next 419\n",
      "2023-06-29 15:45:43.118545: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a0222b740 of size 8960 next 434\n",
      "2023-06-29 15:45:43.118554: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a0222da40 of size 28160 next 417\n",
      "2023-06-29 15:45:43.118562: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02234840 of size 627200 next 442\n",
      "2023-06-29 15:45:43.118570: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a022cda40 of size 627200 next 448\n",
      "2023-06-29 15:45:43.118579: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02366c40 of size 627200 next 459\n",
      "2023-06-29 15:45:43.118587: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a023ffe40 of size 627200 next 458\n",
      "2023-06-29 15:45:43.118595: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02499040 of size 627200 next 471\n",
      "2023-06-29 15:45:43.118604: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02532240 of size 627200 next 470\n",
      "2023-06-29 15:45:43.118612: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f7a025cb440 of size 714752 next 241\n",
      "2023-06-29 15:45:43.118620: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02679c40 of size 5600000 next 142\n",
      "2023-06-29 15:45:43.118629: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f7a02bd0f40 of size 7500288 next 132\n",
      "2023-06-29 15:45:43.118637: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a032f8140 of size 3878912 next 301\n",
      "2023-06-29 15:45:43.118646: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a036ab140 of size 9780992 next 18446744073709551615\n",
      "2023-06-29 15:45:43.118655: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 268435456\n",
      "2023-06-29 15:45:43.118664: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7af3ffe040 of size 268435456 next 18446744073709551615\n",
      "2023-06-29 15:45:43.118672: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 134217728\n",
      "2023-06-29 15:45:43.118680: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b03fff040 of size 9437184 next 53\n",
      "2023-06-29 15:45:43.118690: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b048ff040 of size 9437184 next 58\n",
      "2023-06-29 15:45:43.118698: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b051ff040 of size 9437184 next 56\n",
      "2023-06-29 15:45:43.118707: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b05aff040 of size 9437184 next 52\n",
      "2023-06-29 15:45:43.118715: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b063ff040 of size 4718592 next 65\n",
      "2023-06-29 15:45:43.118724: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0687f040 of size 9437184 next 130\n",
      "2023-06-29 15:45:43.118732: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0717f040 of size 18432 next 204\n",
      "2023-06-29 15:45:43.118740: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07183840 of size 18432 next 205\n",
      "2023-06-29 15:45:43.118749: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07188040 of size 28672 next 189\n",
      "2023-06-29 15:45:43.118757: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0718f040 of size 65536 next 190\n",
      "2023-06-29 15:45:43.118766: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0719f040 of size 18432 next 207\n",
      "2023-06-29 15:45:43.118787: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071a3840 of size 18432 next 209\n",
      "2023-06-29 15:45:43.118796: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071a8040 of size 18432 next 210\n",
      "2023-06-29 15:45:43.118804: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071ac840 of size 18432 next 212\n",
      "2023-06-29 15:45:43.118812: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071b1040 of size 18432 next 213\n",
      "2023-06-29 15:45:43.118820: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071b5840 of size 18432 next 215\n",
      "2023-06-29 15:45:43.118829: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071ba040 of size 4096 next 225\n",
      "2023-06-29 15:45:43.118837: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071bb040 of size 4096 next 228\n",
      "2023-06-29 15:45:43.118846: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071bc040 of size 4096 next 229\n",
      "2023-06-29 15:45:43.118854: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071bd040 of size 4096 next 230\n",
      "2023-06-29 15:45:43.118862: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071be040 of size 4096 next 127\n",
      "2023-06-29 15:45:43.118871: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071bf040 of size 262144 next 134\n",
      "2023-06-29 15:45:43.118879: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071ff040 of size 36864 next 222\n",
      "2023-06-29 15:45:43.118888: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07208040 of size 24576 next 274\n",
      "2023-06-29 15:45:43.118897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0720e040 of size 24576 next 359\n",
      "2023-06-29 15:45:43.118905: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07214040 of size 24576 next 261\n",
      "2023-06-29 15:45:43.118913: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0721a040 of size 24576 next 319\n",
      "2023-06-29 15:45:43.118922: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07220040 of size 8960 next 332\n",
      "2023-06-29 15:45:43.118931: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07222340 of size 15616 next 159\n",
      "2023-06-29 15:45:43.118940: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07226040 of size 24576 next 338\n",
      "2023-06-29 15:45:43.118949: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0722c040 of size 24576 next 311\n",
      "2023-06-29 15:45:43.118957: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07232040 of size 8960 next 372\n",
      "2023-06-29 15:45:43.118966: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07234340 of size 16128 next 259\n",
      "2023-06-29 15:45:43.118975: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07238240 of size 7424 next 322\n",
      "2023-06-29 15:45:43.118984: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07239f40 of size 6144 next 326\n",
      "2023-06-29 15:45:43.118992: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0723b740 of size 6144 next 154\n",
      "2023-06-29 15:45:43.119000: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0723cf40 of size 8448 next 195\n",
      "2023-06-29 15:45:43.119010: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0723f040 of size 417792 next 136\n",
      "2023-06-29 15:45:43.119019: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b072a5040 of size 602112 next 20\n",
      "2023-06-29 15:45:43.119028: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07338040 of size 4096 next 231\n",
      "2023-06-29 15:45:43.119036: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07339040 of size 6144 next 135\n",
      "2023-06-29 15:45:43.119045: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0733a840 of size 11776 next 234\n",
      "2023-06-29 15:45:43.119054: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0733d640 of size 7424 next 21\n",
      "2023-06-29 15:45:43.119062: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0733f340 of size 7424 next 244\n",
      "2023-06-29 15:45:43.119071: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07341040 of size 12800 next 280\n",
      "2023-06-29 15:45:43.119079: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07344240 of size 8192 next 358\n",
      "2023-06-29 15:45:43.119088: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07346240 of size 15872 next 221\n",
      "2023-06-29 15:45:43.119097: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0734a040 of size 57344 next 193\n",
      "2023-06-29 15:45:43.119106: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07358040 of size 131072 next 194\n",
      "2023-06-29 15:45:43.119115: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07378040 of size 117760 next 235\n",
      "2023-06-29 15:45:43.119124: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07394c40 of size 401408 next 287\n",
      "2023-06-29 15:45:43.119133: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b073f6c40 of size 401408 next 346\n",
      "2023-06-29 15:45:43.119141: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07458c40 of size 24576 next 279\n",
      "2023-06-29 15:45:43.119150: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0745ec40 of size 24576 next 377\n",
      "2023-06-29 15:45:43.119158: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07464c40 of size 24576 next 371\n",
      "2023-06-29 15:45:43.119166: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0746ac40 of size 24576 next 357\n",
      "2023-06-29 15:45:43.119175: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07470c40 of size 24576 next 273\n",
      "2023-06-29 15:45:43.119183: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07476c40 of size 8192 next 267\n",
      "2023-06-29 15:45:43.119191: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07478c40 of size 8192 next 303\n",
      "2023-06-29 15:45:43.119200: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0747ac40 of size 8192 next 173\n",
      "2023-06-29 15:45:43.119208: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0747cc40 of size 8192 next 299\n",
      "2023-06-29 15:45:43.119217: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0747ec40 of size 24576 next 158\n",
      "2023-06-29 15:45:43.119225: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07484c40 of size 24576 next 349\n",
      "2023-06-29 15:45:43.119233: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0748ac40 of size 24576 next 321\n",
      "2023-06-29 15:45:43.119241: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07490c40 of size 24576 next 141\n",
      "2023-06-29 15:45:43.119250: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07496c40 of size 24576 next 310\n",
      "2023-06-29 15:45:43.119258: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0749cc40 of size 24576 next 300\n",
      "2023-06-29 15:45:43.119266: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074a2c40 of size 24576 next 342\n",
      "2023-06-29 15:45:43.119275: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074a8c40 of size 24576 next 177\n",
      "2023-06-29 15:45:43.119283: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074aec40 of size 8192 next 276\n",
      "2023-06-29 15:45:43.119291: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074b0c40 of size 8192 next 337\n",
      "2023-06-29 15:45:43.119300: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074b2c40 of size 8192 next 347\n",
      "2023-06-29 15:45:43.119308: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074b4c40 of size 8192 next 362\n",
      "2023-06-29 15:45:43.119316: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074b6c40 of size 24576 next 257\n",
      "2023-06-29 15:45:43.119325: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074bcc40 of size 26368 next 182\n",
      "2023-06-29 15:45:43.119334: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074c3340 of size 524288 next 201\n",
      "2023-06-29 15:45:43.119343: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07543340 of size 6245632 next 161\n",
      "2023-06-29 15:45:43.119351: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07b38040 of size 1048576 next 203\n",
      "2023-06-29 15:45:43.119360: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07c38040 of size 1048576 next 163\n",
      "2023-06-29 15:45:43.119368: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07d38040 of size 2097152 next 164\n",
      "2023-06-29 15:45:43.119377: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07f38040 of size 1048576 next 206\n",
      "2023-06-29 15:45:43.119385: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08038040 of size 1048576 next 208\n",
      "2023-06-29 15:45:43.119393: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08138040 of size 1048576 next 211\n",
      "2023-06-29 15:45:43.119402: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08238040 of size 1048576 next 214\n",
      "2023-06-29 15:45:43.119410: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08338040 of size 224000 next 237\n",
      "2023-06-29 15:45:43.119418: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0836eb40 of size 224000 next 239\n",
      "2023-06-29 15:45:43.119427: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083a5640 of size 224000 next 245\n",
      "2023-06-29 15:45:43.119435: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083dc140 of size 8192 next 263\n",
      "2023-06-29 15:45:43.119443: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083de140 of size 8192 next 11\n",
      "2023-06-29 15:45:43.119452: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083e0140 of size 8192 next 304\n",
      "2023-06-29 15:45:43.119460: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083e2140 of size 31744 next 282\n",
      "2023-06-29 15:45:43.119469: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083e9d40 of size 167680 next 246\n",
      "2023-06-29 15:45:43.119478: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08412c40 of size 204800 next 343\n",
      "2023-06-29 15:45:43.119487: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08444c40 of size 8192 next 323\n",
      "2023-06-29 15:45:43.119496: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08446c40 of size 8192 next 316\n",
      "2023-06-29 15:45:43.119504: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08448c40 of size 8192 next 327\n",
      "2023-06-29 15:45:43.119512: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0844ac40 of size 24576 next 383\n",
      "2023-06-29 15:45:43.119520: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08450c40 of size 24576 next 328\n",
      "2023-06-29 15:45:43.119529: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08456c40 of size 24576 next 278\n",
      "2023-06-29 15:45:43.119537: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0845cc40 of size 24576 next 290\n",
      "2023-06-29 15:45:43.119545: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08462c40 of size 8192 next 381\n",
      "2023-06-29 15:45:43.119554: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08464c40 of size 8192 next 286\n",
      "2023-06-29 15:45:43.119562: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08466c40 of size 8192 next 247\n",
      "2023-06-29 15:45:43.119570: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08468c40 of size 24576 next 331\n",
      "2023-06-29 15:45:43.119578: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0846ec40 of size 24576 next 166\n",
      "2023-06-29 15:45:43.119587: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08474c40 of size 24576 next 360\n",
      "2023-06-29 15:45:43.119595: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0847ac40 of size 24576 next 315\n",
      "2023-06-29 15:45:43.119603: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08480c40 of size 8192 next 256\n",
      "2023-06-29 15:45:43.119612: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08482c40 of size 8192 next 289\n",
      "2023-06-29 15:45:43.119620: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08484c40 of size 8192 next 262\n",
      "2023-06-29 15:45:43.119628: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08486c40 of size 24576 next 302\n",
      "2023-06-29 15:45:43.119636: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0848cc40 of size 24576 next 169\n",
      "2023-06-29 15:45:43.119644: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08492c40 of size 24576 next 313\n",
      "2023-06-29 15:45:43.119652: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08498c40 of size 24576 next 344\n",
      "2023-06-29 15:45:43.119660: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0849ec40 of size 24576 next 160\n",
      "2023-06-29 15:45:43.119669: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084a4c40 of size 29184 next 249\n",
      "2023-06-29 15:45:43.119677: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084abe40 of size 224000 next 255\n",
      "2023-06-29 15:45:43.119686: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084e2940 of size 24576 next 318\n",
      "2023-06-29 15:45:43.119694: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084e8940 of size 24576 next 345\n",
      "2023-06-29 15:45:43.119703: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084ee940 of size 24576 next 172\n",
      "2023-06-29 15:45:43.119711: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084f4940 of size 44032 next 167\n",
      "2023-06-29 15:45:43.119721: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084ff540 of size 220160 next 293\n",
      "2023-06-29 15:45:43.119729: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08535140 of size 7424 next 156\n",
      "2023-06-29 15:45:43.119738: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08536e40 of size 7424 next 129\n",
      "2023-06-29 15:45:43.119747: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08538b40 of size 738560 next 308\n",
      "2023-06-29 15:45:43.119756: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b085ed040 of size 5550080 next 178\n",
      "2023-06-29 15:45:43.119765: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08b38040 of size 2097152 next 217\n",
      "2023-06-29 15:45:43.119773: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08d38040 of size 4194304 next 226\n",
      "2023-06-29 15:45:43.119782: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09138040 of size 4194304 next 227\n",
      "2023-06-29 15:45:43.119791: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09538040 of size 3200000 next 236\n",
      "2023-06-29 15:45:43.119800: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09845440 of size 3200000 next 242\n",
      "2023-06-29 15:45:43.119808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09b52840 of size 3200000 next 252\n",
      "2023-06-29 15:45:43.119816: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09e5fc40 of size 4800000 next 238\n",
      "2023-06-29 15:45:43.119825: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0a2f3a40 of size 6200064 next 151\n",
      "2023-06-29 15:45:43.119835: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0a8dd540 of size 3283712 next 10\n",
      "2023-06-29 15:45:43.119844: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0abff040 of size 9437184 next 140\n",
      "2023-06-29 15:45:43.119853: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0b4ff040 of size 11534336 next 18446744073709551615\n",
      "2023-06-29 15:45:43.119860: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-06-29 15:45:43.119872: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 38 Chunks of size 4096 totalling 152.0KiB\n",
      "2023-06-29 15:45:43.119883: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 4608 totalling 22.5KiB\n",
      "2023-06-29 15:45:43.119892: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4864 totalling 4.8KiB\n",
      "2023-06-29 15:45:43.119901: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 6144 totalling 30.0KiB\n",
      "2023-06-29 15:45:43.119910: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6400 totalling 6.2KiB\n",
      "2023-06-29 15:45:43.119919: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6656 totalling 6.5KiB\n",
      "2023-06-29 15:45:43.119928: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6912 totalling 6.8KiB\n",
      "2023-06-29 15:45:43.119938: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 7424 totalling 36.2KiB\n",
      "2023-06-29 15:45:43.119947: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 50 Chunks of size 8192 totalling 400.0KiB\n",
      "2023-06-29 15:45:43.119956: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 8448 totalling 8.2KiB\n",
      "2023-06-29 15:45:43.119966: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 8 Chunks of size 8960 totalling 70.0KiB\n",
      "2023-06-29 15:45:43.119975: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 9216 totalling 36.0KiB\n",
      "2023-06-29 15:45:43.119984: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 10240 totalling 10.0KiB\n",
      "2023-06-29 15:45:43.119993: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11776 totalling 11.5KiB\n",
      "2023-06-29 15:45:43.120002: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12288 totalling 12.0KiB\n",
      "2023-06-29 15:45:43.120012: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12544 totalling 12.2KiB\n",
      "2023-06-29 15:45:43.120021: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12800 totalling 12.5KiB\n",
      "2023-06-29 15:45:43.120030: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 13824 totalling 13.5KiB\n",
      "2023-06-29 15:45:43.120040: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 15616 totalling 15.2KiB\n",
      "2023-06-29 15:45:43.120049: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 15872 totalling 15.5KiB\n",
      "2023-06-29 15:45:43.120058: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 16128 totalling 15.8KiB\n",
      "2023-06-29 15:45:43.120068: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 16384 totalling 16.0KiB\n",
      "2023-06-29 15:45:43.120077: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 9 Chunks of size 18432 totalling 162.0KiB\n",
      "2023-06-29 15:45:43.120086: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 21504 totalling 21.0KiB\n",
      "2023-06-29 15:45:43.120095: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 58 Chunks of size 24576 totalling 1.36MiB\n",
      "2023-06-29 15:45:43.120104: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 26368 totalling 25.8KiB\n",
      "2023-06-29 15:45:43.120114: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 33 Chunks of size 28160 totalling 907.5KiB\n",
      "2023-06-29 15:45:43.120123: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 28672 totalling 56.0KiB\n",
      "2023-06-29 15:45:43.120133: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 29184 totalling 28.5KiB\n",
      "2023-06-29 15:45:43.120142: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 31744 totalling 31.0KiB\n",
      "2023-06-29 15:45:43.120151: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 32768 totalling 32.0KiB\n",
      "2023-06-29 15:45:43.120160: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 35840 totalling 35.0KiB\n",
      "2023-06-29 15:45:43.120169: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 36864 totalling 36.0KiB\n",
      "2023-06-29 15:45:43.120178: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 37632 totalling 36.8KiB\n",
      "2023-06-29 15:45:43.120188: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 40960 totalling 80.0KiB\n",
      "2023-06-29 15:45:43.120197: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 44032 totalling 43.0KiB\n",
      "2023-06-29 15:45:43.120206: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 57344 totalling 56.0KiB\n",
      "2023-06-29 15:45:43.120215: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 65536 totalling 256.0KiB\n",
      "2023-06-29 15:45:43.120225: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 81920 totalling 160.0KiB\n",
      "2023-06-29 15:45:43.120234: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 100352 totalling 98.0KiB\n",
      "2023-06-29 15:45:43.120244: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 111872 totalling 109.2KiB\n",
      "2023-06-29 15:45:43.120253: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 117760 totalling 115.0KiB\n",
      "2023-06-29 15:45:43.120262: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 131072 totalling 128.0KiB\n",
      "2023-06-29 15:45:43.120272: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 143360 totalling 560.0KiB\n",
      "2023-06-29 15:45:43.120281: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 147456 totalling 432.0KiB\n",
      "2023-06-29 15:45:43.120290: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 167680 totalling 163.8KiB\n",
      "2023-06-29 15:45:43.120300: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 204800 totalling 200.0KiB\n",
      "2023-06-29 15:45:43.120309: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 220160 totalling 215.0KiB\n",
      "2023-06-29 15:45:43.120318: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 223232 totalling 218.0KiB\n",
      "2023-06-29 15:45:43.120327: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 6 Chunks of size 224000 totalling 1.28MiB\n",
      "2023-06-29 15:45:43.120337: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 226048 totalling 220.8KiB\n",
      "2023-06-29 15:45:43.120346: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 229376 totalling 224.0KiB\n",
      "2023-06-29 15:45:43.120355: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 240128 totalling 469.0KiB\n",
      "2023-06-29 15:45:43.120364: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 262144 totalling 1.25MiB\n",
      "2023-06-29 15:45:43.120373: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 294912 totalling 288.0KiB\n",
      "2023-06-29 15:45:43.120383: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 327680 totalling 320.0KiB\n",
      "2023-06-29 15:45:43.120392: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 401408 totalling 1.15MiB\n",
      "2023-06-29 15:45:43.120401: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 411392 totalling 401.8KiB\n",
      "2023-06-29 15:45:43.120411: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 417792 totalling 408.0KiB\n",
      "2023-06-29 15:45:43.120420: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 9 Chunks of size 524288 totalling 4.50MiB\n",
      "2023-06-29 15:45:43.120428: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 589824 totalling 1.12MiB\n",
      "2023-06-29 15:45:43.120438: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 602112 totalling 588.0KiB\n",
      "2023-06-29 15:45:43.120447: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 8 Chunks of size 627200 totalling 4.79MiB\n",
      "2023-06-29 15:45:43.120456: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 655360 totalling 640.0KiB\n",
      "2023-06-29 15:45:43.120466: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 738560 totalling 721.2KiB\n",
      "2023-06-29 15:45:43.120475: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 786432 totalling 768.0KiB\n",
      "2023-06-29 15:45:43.120484: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 884736 totalling 864.0KiB\n",
      "2023-06-29 15:45:43.120493: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 935936 totalling 914.0KiB\n",
      "2023-06-29 15:45:43.120502: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 10 Chunks of size 1048576 totalling 10.00MiB\n",
      "2023-06-29 15:45:43.120512: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 1179648 totalling 2.25MiB\n",
      "2023-06-29 15:45:43.120521: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 1310720 totalling 2.50MiB\n",
      "2023-06-29 15:45:43.120530: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1835008 totalling 1.75MiB\n",
      "2023-06-29 15:45:43.120539: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1966080 totalling 1.88MiB\n",
      "2023-06-29 15:45:43.120548: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 2097152 totalling 6.00MiB\n",
      "2023-06-29 15:45:43.120557: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2195456 totalling 2.09MiB\n",
      "2023-06-29 15:45:43.120566: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 2359296 totalling 6.75MiB\n",
      "2023-06-29 15:45:43.120575: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 2621440 totalling 5.00MiB\n",
      "2023-06-29 15:45:43.120584: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2883584 totalling 2.75MiB\n",
      "2023-06-29 15:45:43.120593: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2916352 totalling 2.78MiB\n",
      "2023-06-29 15:45:43.120602: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 3000064 totalling 5.72MiB\n",
      "2023-06-29 15:45:43.120611: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3145728 totalling 3.00MiB\n",
      "2023-06-29 15:45:43.120620: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 3200000 totalling 9.16MiB\n",
      "2023-06-29 15:45:43.120629: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3276800 totalling 3.12MiB\n",
      "2023-06-29 15:45:43.120638: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3283712 totalling 3.13MiB\n",
      "2023-06-29 15:45:43.120647: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3561728 totalling 3.40MiB\n",
      "2023-06-29 15:45:43.120656: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3670016 totalling 3.50MiB\n",
      "2023-06-29 15:45:43.120665: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3878912 totalling 3.70MiB\n",
      "2023-06-29 15:45:43.120674: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4014080 totalling 3.83MiB\n",
      "2023-06-29 15:45:43.120683: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 4194304 totalling 8.00MiB\n",
      "2023-06-29 15:45:43.120692: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4199936 totalling 4.00MiB\n",
      "2023-06-29 15:45:43.120701: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 4718592 totalling 9.00MiB\n",
      "2023-06-29 15:45:43.120710: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4800000 totalling 4.58MiB\n",
      "2023-06-29 15:45:43.120719: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4860416 totalling 4.63MiB\n",
      "2023-06-29 15:45:43.120728: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5242880 totalling 5.00MiB\n",
      "2023-06-29 15:45:43.120737: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5550080 totalling 5.29MiB\n",
      "2023-06-29 15:45:43.120747: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 5600000 totalling 26.70MiB\n",
      "2023-06-29 15:45:43.120756: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5767168 totalling 5.50MiB\n",
      "2023-06-29 15:45:43.120764: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5779712 totalling 5.51MiB\n",
      "2023-06-29 15:45:43.120773: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6200064 totalling 5.91MiB\n",
      "2023-06-29 15:45:43.120782: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6245632 totalling 5.96MiB\n",
      "2023-06-29 15:45:43.120791: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6250240 totalling 5.96MiB\n",
      "2023-06-29 15:45:43.120801: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 28 Chunks of size 7526400 totalling 200.98MiB\n",
      "2023-06-29 15:45:43.120810: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 8559872 totalling 8.16MiB\n",
      "2023-06-29 15:45:43.120819: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 11 Chunks of size 9437184 totalling 99.00MiB\n",
      "2023-06-29 15:45:43.120828: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 9780992 totalling 9.33MiB\n",
      "2023-06-29 15:45:43.120837: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 10035200 totalling 9.57MiB\n",
      "2023-06-29 15:45:43.120847: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11200000 totalling 10.68MiB\n",
      "2023-06-29 15:45:43.120856: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11468288 totalling 10.94MiB\n",
      "2023-06-29 15:45:43.120866: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11534336 totalling 11.00MiB\n",
      "2023-06-29 15:45:43.120875: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11698176 totalling 11.16MiB\n",
      "2023-06-29 15:45:43.120885: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12293120 totalling 11.72MiB\n",
      "2023-06-29 15:45:43.120894: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12582912 totalling 12.00MiB\n",
      "2023-06-29 15:45:43.120903: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 13798400 totalling 13.16MiB\n",
      "2023-06-29 15:45:43.120914: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 10 Chunks of size 20070400 totalling 191.41MiB\n",
      "2023-06-29 15:45:43.120923: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 20160000 totalling 19.23MiB\n",
      "2023-06-29 15:45:43.120932: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 22108160 totalling 21.08MiB\n",
      "2023-06-29 15:45:43.120942: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 26869760 totalling 25.62MiB\n",
      "2023-06-29 15:45:43.120951: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 30105600 totalling 57.42MiB\n",
      "2023-06-29 15:45:43.120961: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 35569408 totalling 33.92MiB\n",
      "2023-06-29 15:45:43.120970: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 40140800 totalling 153.12MiB\n",
      "2023-06-29 15:45:43.120980: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 40560128 totalling 116.04MiB\n",
      "2023-06-29 15:45:43.120990: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 7 Chunks of size 120422400 totalling 803.91MiB\n",
      "2023-06-29 15:45:43.120999: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 174191360 totalling 166.12MiB\n",
      "2023-06-29 15:45:43.121009: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 183606528 totalling 175.10MiB\n",
      "2023-06-29 15:45:43.121018: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 195661824 totalling 186.60MiB\n",
      "2023-06-29 15:45:43.121028: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 196000000 totalling 186.92MiB\n",
      "2023-06-29 15:45:43.121037: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 205520896 totalling 392.00MiB\n",
      "2023-06-29 15:45:43.121047: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 226612736 totalling 216.11MiB\n",
      "2023-06-29 15:45:43.121057: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 263617536 totalling 251.41MiB\n",
      "2023-06-29 15:45:43.121066: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 268435456 totalling 256.00MiB\n",
      "2023-06-29 15:45:43.121076: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 296750080 totalling 283.00MiB\n",
      "2023-06-29 15:45:43.121085: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 309080576 totalling 294.76MiB\n",
      "2023-06-29 15:45:43.121095: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 325283072 totalling 310.21MiB\n",
      "2023-06-29 15:45:43.121104: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 439040000 totalling 837.40MiB\n",
      "2023-06-29 15:45:43.121113: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2950348800 totalling 2.75GiB\n",
      "2023-06-29 15:45:43.121123: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 11801395200 totalling 32.97GiB\n",
      "2023-06-29 15:45:43.121133: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 41.18GiB\n",
      "2023-06-29 15:45:43.121141: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 67434475520 memory_limit_: 67434475520 available bytes: 0 curr_region_allocation_bytes_: 68719476736\n",
      "2023-06-29 15:45:43.121155: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                     67434475520\n",
      "InUse:                     44216678656\n",
      "MaxInUse:                  44216678656\n",
      "NumAllocs:                      246999\n",
      "MaxAllocSize:              11801395200\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-06-29 15:45:43.121210: W tensorflow/tsl/framework/bfc_allocator.cc:492] ************************************_____________******************_______**___________*************\n",
      "2023-06-29 15:45:43.121248: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at mkl_einsum_op.cc:259 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[50,6,3136,3136] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_9/multi_head_attention_5/einsum/Einsum' defined at (most recent call last):\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3182346/2668463021.py\", line 2, in <module>\n      hist = model.fit(x_train,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/layers/attention/multi_head_attention.py\", line 595, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/layers/attention/multi_head_attention.py\", line 524, in _compute_attention\n      attention_scores = tf.einsum(self._dot_product_equation, key, query)\nNode: 'model_9/multi_head_attention_5/einsum/Einsum'\nOOM when allocating tensor with shape[50,6,3136,3136] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node model_9/multi_head_attention_5/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_66762]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, \n\u001b[1;32m      3\u001b[0m                   y_train, \n\u001b[1;32m      4\u001b[0m                   epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m                 \n\u001b[1;32m      6\u001b[0m                   batch_size\u001b[39m=\u001b[39;49mbatch_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_9/multi_head_attention_5/einsum/Einsum' defined at (most recent call last):\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3182346/2668463021.py\", line 2, in <module>\n      hist = model.fit(x_train,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/layers/attention/multi_head_attention.py\", line 595, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/layers/attention/multi_head_attention.py\", line 524, in _compute_attention\n      attention_scores = tf.einsum(self._dot_product_equation, key, query)\nNode: 'model_9/multi_head_attention_5/einsum/Einsum'\nOOM when allocating tensor with shape[50,6,3136,3136] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node model_9/multi_head_attention_5/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_66762]"
     ]
    }
   ],
   "source": [
    "  #Train model\n",
    "hist = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    epochs=10, \n",
    "                  \n",
    "                    batch_size=batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 7, 7, 2048)  8192        ['conv5_block3_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4 (Gl  (None, 2048)        0           ['batch_normalization_25[0][0]'] \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 2048)         0           ['global_average_pooling2d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 64)           131136      ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 64)           0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 100)          6500        ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,733,540\n",
      "Trainable params: 141,732\n",
      "Non-trainable params: 23,591,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras import regularizers\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "img_shape = (224, 224, 3)\n",
    "inputs = Input(img_shape)\n",
    "num_classes = 100\n",
    "\n",
    "resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=img_shape)\n",
    "outputs = resnet50.output\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = GlobalAveragePooling2D()(outputs)\n",
    "outputs= Dropout(0.5)(outputs)\n",
    "outputs= Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(outputs)\n",
    "outputs= Dropout(0.5)(outputs)\n",
    "outputs = Dense(num_classes, activation = 'softmax')(outputs)\n",
    "model = Model(inputs = resnet50.input, outputs = outputs)\n",
    "\n",
    "# transfer learning freeze all convolutional  layers\n",
    "for layer in resnet50.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 334s 20s/step - loss: 4.8816 - accuracy: 0.0060\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 345s 22s/step - loss: 4.5974 - accuracy: 0.0220\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 329s 20s/step - loss: 4.4741 - accuracy: 0.0480\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 342s 21s/step - loss: 4.2808 - accuracy: 0.0920\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 340s 21s/step - loss: 4.0545 - accuracy: 0.1120\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 334s 21s/step - loss: 3.9533 - accuracy: 0.1360\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 337s 21s/step - loss: 3.8108 - accuracy: 0.1780\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 335s 21s/step - loss: 3.5748 - accuracy: 0.2100\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 337s 21s/step - loss: 3.3596 - accuracy: 0.2500\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 334s 21s/step - loss: 3.2463 - accuracy: 0.2520\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 332s 21s/step - loss: 3.0756 - accuracy: 0.3140\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 338s 21s/step - loss: 2.9044 - accuracy: 0.3280\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 333s 21s/step - loss: 2.7215 - accuracy: 0.3920\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 341s 21s/step - loss: 2.6430 - accuracy: 0.4080\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 337s 21s/step - loss: 2.5835 - accuracy: 0.4000\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 337s 21s/step - loss: 2.4431 - accuracy: 0.4360\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 330s 21s/step - loss: 2.3417 - accuracy: 0.4540\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 331s 21s/step - loss: 2.2357 - accuracy: 0.4820\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 339s 21s/step - loss: 2.1310 - accuracy: 0.5240\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 327s 20s/step - loss: 2.1172 - accuracy: 0.5240\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 338s 21s/step - loss: 2.0037 - accuracy: 0.5260\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 332s 21s/step - loss: 1.8772 - accuracy: 0.5900\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 335s 21s/step - loss: 1.9358 - accuracy: 0.5320\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 324s 20s/step - loss: 1.8278 - accuracy: 0.5860\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 336s 21s/step - loss: 1.7604 - accuracy: 0.5800\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 334s 21s/step - loss: 1.7051 - accuracy: 0.6120\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 327s 20s/step - loss: 1.7220 - accuracy: 0.6000\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 331s 21s/step - loss: 1.5666 - accuracy: 0.6480\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 331s 21s/step - loss: 1.5496 - accuracy: 0.6520\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 325s 20s/step - loss: 1.5053 - accuracy: 0.6680\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 331s 21s/step - loss: 1.5448 - accuracy: 0.6480\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 326s 20s/step - loss: 1.4428 - accuracy: 0.7180\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 329s 20s/step - loss: 1.3275 - accuracy: 0.7320\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 324s 20s/step - loss: 1.3763 - accuracy: 0.6800\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 337s 21s/step - loss: 1.3324 - accuracy: 0.7220\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 334s 21s/step - loss: 1.3388 - accuracy: 0.7260\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 338s 21s/step - loss: 1.3490 - accuracy: 0.7080\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 335s 21s/step - loss: 1.2632 - accuracy: 0.6960\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 332s 21s/step - loss: 1.3299 - accuracy: 0.7060\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 334s 21s/step - loss: 1.2460 - accuracy: 0.7440\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 332s 21s/step - loss: 1.2329 - accuracy: 0.7400\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 331s 21s/step - loss: 1.2429 - accuracy: 0.7320\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 326s 20s/step - loss: 1.1892 - accuracy: 0.7540\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 327s 20s/step - loss: 1.0923 - accuracy: 0.7920\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 328s 20s/step - loss: 1.2039 - accuracy: 0.7300\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 336s 21s/step - loss: 1.2455 - accuracy: 0.7020\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 334s 21s/step - loss: 1.1990 - accuracy: 0.7340\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 334s 21s/step - loss: 1.1205 - accuracy: 0.7680\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 325s 20s/step - loss: 1.0756 - accuracy: 0.8000\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 319s 20s/step - loss: 1.0720 - accuracy: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74585601c0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(preprocessed_images_train, one_hot_labels_train, epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 194s 19s/step - loss: 2.7572 - accuracy: 0.3667\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " BatchNormalization (BatchNo  (None, 7, 7, 512)        2048      \n",
      " rmalization)                                                    \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 140)               35980     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,193,740\n",
      "Trainable params: 168,332\n",
      "Non-trainable params: 20,025,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "IMG_SIZE = 224\n",
    "img_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "inputs = Input(img_shape)\n",
    "num_classes = 140\n",
    "\n",
    "vgg19_model = VGG19(weights='imagenet', include_top=False, input_shape= img_shape)\n",
    "vgg19_model.trainable = False\n",
    "outputs = vgg19_model(inputs)\n",
    "outputs = BatchNormalization(name = 'BatchNormalization')(outputs)\n",
    "outputs = GlobalAveragePooling2D()(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Dense(256)(outputs)\n",
    "outputs = LeakyReLU(alpha=0.1)(outputs)\n",
    "outputs = Dropout(0.25)(outputs)\n",
    "outputs = Dense(num_classes, activation = 'softmax')(outputs)\n",
    "\n",
    "model = Model(inputs = [inputs], outputs = [outputs])\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "374/374 [==============================] - 129s 341ms/step - loss: 4.6813 - accuracy: 0.0473\n",
      "Epoch 2/10\n",
      "374/374 [==============================] - 128s 342ms/step - loss: 3.2637 - accuracy: 0.2786\n",
      "Epoch 3/10\n",
      "374/374 [==============================] - 130s 346ms/step - loss: 2.1703 - accuracy: 0.4598\n",
      "Epoch 4/10\n",
      "374/374 [==============================] - 128s 343ms/step - loss: 1.5545 - accuracy: 0.5964\n",
      "Epoch 5/10\n",
      "374/374 [==============================] - 129s 346ms/step - loss: 1.2912 - accuracy: 0.6571\n",
      "Epoch 6/10\n",
      "374/374 [==============================] - 126s 338ms/step - loss: 0.9954 - accuracy: 0.7170\n",
      "Epoch 7/10\n",
      "374/374 [==============================] - 129s 344ms/step - loss: 0.8689 - accuracy: 0.7652\n",
      "Epoch 8/10\n",
      "374/374 [==============================] - 136s 364ms/step - loss: 0.7393 - accuracy: 0.7875\n",
      "Epoch 9/10\n",
      "374/374 [==============================] - 126s 338ms/step - loss: 0.6971 - accuracy: 0.8054\n",
      "Epoch 10/10\n",
      "374/374 [==============================] - 128s 342ms/step - loss: 0.6630 - accuracy: 0.8143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b5062d660>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(preprocessed_images_train, one_hot_labels_train, epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 43s 2s/step - loss: 0.4137 - accuracy: 0.8857\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model((224,224,3), num_classes=100)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling_2 (Rescaling)        (None, 224, 224, 3)  0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 112, 112, 12  3584        ['rescaling_2[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 112, 112, 12  512        ['conv2d_75[0][0]']              \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 112, 112, 12  0           ['batch_normalization_16[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 112, 112, 12  0           ['activation_16[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_14 (Separable  (None, 112, 112, 25  34176      ['activation_17[0][0]']          \n",
      " Conv2D)                        6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 112, 112, 25  1024       ['separable_conv2d_14[0][0]']    \n",
      " ormalization)                  6)                                                                \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 112, 112, 25  0           ['batch_normalization_17[0][0]'] \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_15 (Separable  (None, 112, 112, 25  68096      ['activation_18[0][0]']          \n",
      " Conv2D)                        6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 112, 112, 25  1024       ['separable_conv2d_15[0][0]']    \n",
      " ormalization)                  6)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_60 (MaxPooling2D  (None, 56, 56, 256)  0          ['batch_normalization_18[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 56, 56, 256)  33024       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 56, 56, 256)  0           ['max_pooling2d_60[0][0]',       \n",
      "                                                                  'conv2d_76[0][0]']              \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 56, 56, 256)  0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_16 (Separable  (None, 56, 56, 512)  133888     ['activation_19[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 56, 56, 512)  2048       ['separable_conv2d_16[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 56, 56, 512)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_17 (Separable  (None, 56, 56, 512)  267264     ['activation_20[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 56, 56, 512)  2048       ['separable_conv2d_17[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_61 (MaxPooling2D  (None, 28, 28, 512)  0          ['batch_normalization_20[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 28, 28, 512)  131584      ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 28, 28, 512)  0           ['max_pooling2d_61[0][0]',       \n",
      "                                                                  'conv2d_77[0][0]']              \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 28, 28, 512)  0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_18 (Separable  (None, 28, 28, 728)  378072     ['activation_21[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 28, 28, 728)  2912       ['separable_conv2d_18[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 28, 28, 728)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_19 (Separable  (None, 28, 28, 728)  537264     ['activation_22[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 28, 28, 728)  2912       ['separable_conv2d_19[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_62 (MaxPooling2D  (None, 14, 14, 728)  0          ['batch_normalization_22[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 14, 14, 728)  373464      ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 14, 14, 728)  0           ['max_pooling2d_62[0][0]',       \n",
      "                                                                  'conv2d_78[0][0]']              \n",
      "                                                                                                  \n",
      " separable_conv2d_20 (Separable  (None, 14, 14, 1024  753048     ['add_8[0][0]']                  \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 14, 14, 1024  4096       ['separable_conv2d_20[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 14, 14, 1024  0           ['batch_normalization_23[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 1024)        0           ['activation_23[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 1024)         0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 100)          102500      ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,832,540\n",
      "Trainable params: 2,824,252\n",
      "Non-trainable params: 8,288\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 89s 5s/step - loss: 5.0606 - accuracy: 0.0060\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 4.6566 - accuracy: 0.0220\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 84s 5s/step - loss: 4.3952 - accuracy: 0.0340\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 84s 5s/step - loss: 4.1305 - accuracy: 0.0500\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 3.7951 - accuracy: 0.0920\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 3.6054 - accuracy: 0.1100\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 3.3787 - accuracy: 0.1520\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 3.1388 - accuracy: 0.2020\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 2.9464 - accuracy: 0.2420\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 86s 5s/step - loss: 2.7146 - accuracy: 0.3180\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 2.5438 - accuracy: 0.3320\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 2.3024 - accuracy: 0.3760\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 2.0921 - accuracy: 0.4560\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 81s 5s/step - loss: 2.0120 - accuracy: 0.4920\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 1.7275 - accuracy: 0.5800\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 1.5908 - accuracy: 0.6260\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 1.3400 - accuracy: 0.7060\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 1.1757 - accuracy: 0.7500\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 0.9894 - accuracy: 0.8040\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 81s 5s/step - loss: 0.8067 - accuracy: 0.8720\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 87s 5s/step - loss: 0.6888 - accuracy: 0.8960\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 81s 5s/step - loss: 0.5428 - accuracy: 0.9280\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 0.4545 - accuracy: 0.9340\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 83s 5s/step - loss: 0.3346 - accuracy: 0.9740\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 0.2116 - accuracy: 0.9880\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 80s 5s/step - loss: 0.2100 - accuracy: 0.9860\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 0.1626 - accuracy: 0.9980\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 0.1300 - accuracy: 0.9900\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 86s 5s/step - loss: 0.1039 - accuracy: 0.9960\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 81s 5s/step - loss: 0.0745 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 82s 5s/step - loss: 0.0578 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 84s 5s/step - loss: 0.0443 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 96s 6s/step - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 100s 6s/step - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 101s 6s/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 100s 6s/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 113s 7s/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 102s 6s/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 98s 6s/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 111s 7s/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 106s 7s/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 106s 7s/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 107s 7s/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 106s 7s/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 111s 7s/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 106s 7s/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 112s 7s/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 99s 6s/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 107s 7s/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 104s 7s/step - loss: 0.0077 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74583891e0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(preprocessed_images_train, one_hot_labels_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 14:16:34.313201: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'model_2/add_8/add' exist for missing node 'model_2/add_7/add'.\n",
      "2023-07-13 14:16:34.385548: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'model_2/add_8/add' exist for missing node 'model_2/add_7/add'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 80s 8s/step - loss: 2.8060 - accuracy: 0.2900\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "import numpy as np # linear algebra\n",
    "import keras.backend as K \n",
    "import time as ti \n",
    "import cv2\n",
    "import os\n",
    "import glob # for including images\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, DepthwiseConv2D, MaxPooling2D, AveragePooling2D  \n",
    "# from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.python.keras.optimizers import RMSprop, SGD, Adadelta, Adam \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_224\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 224, 224, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  288         ['input_18[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_12 (G  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " predictions (Dense)            (None, 140)          179340      ['global_average_pooling2d_12[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,436,748\n",
      "Trainable params: 2,402,636\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2(input_shape=(224, 224, 1), alpha=1, weights=None,classes=140)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35/35 [==============================] - 94s 2s/step - loss: 5.1423 - accuracy: 0.0098\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 81s 2s/step - loss: 4.2785 - accuracy: 0.0491\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 81s 2s/step - loss: 3.7234 - accuracy: 0.1009\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 80s 2s/step - loss: 2.9112 - accuracy: 0.2339\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 80s 2s/step - loss: 2.2615 - accuracy: 0.3580\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 80s 2s/step - loss: 1.7196 - accuracy: 0.5080\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 81s 2s/step - loss: 1.2219 - accuracy: 0.6429\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.8206 - accuracy: 0.7830\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.6759 - accuracy: 0.8134\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.4186 - accuracy: 0.9036\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.2773 - accuracy: 0.9411\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.2249 - accuracy: 0.9554\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.1919 - accuracy: 0.9625\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.1323 - accuracy: 0.9768\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.1204 - accuracy: 0.9759\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.1266 - accuracy: 0.9768\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.1023 - accuracy: 0.9857\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.1078 - accuracy: 0.9786\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.1242 - accuracy: 0.9705\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.1409 - accuracy: 0.9670\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.1874 - accuracy: 0.9563\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.2425 - accuracy: 0.9312\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.2444 - accuracy: 0.9330\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.2510 - accuracy: 0.9268\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.2515 - accuracy: 0.9304\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.1324 - accuracy: 0.9643\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.0770 - accuracy: 0.9839\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.0402 - accuracy: 0.9946\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.0306 - accuracy: 0.9937\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.0395 - accuracy: 0.9929\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.0401 - accuracy: 0.9937\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.0371 - accuracy: 0.9911\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.0548 - accuracy: 0.9902\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.0472 - accuracy: 0.9893\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.0505 - accuracy: 0.9929\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.0398 - accuracy: 0.9911\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.0360 - accuracy: 0.9929\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.0355 - accuracy: 0.9929\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.0291 - accuracy: 0.9937\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.0590 - accuracy: 0.9812\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.0497 - accuracy: 0.9875\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.0302 - accuracy: 0.9955\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.0236 - accuracy: 0.9955\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.0543 - accuracy: 0.9866\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 70s 2s/step - loss: 0.0957 - accuracy: 0.9741\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.1475 - accuracy: 0.9545\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.2951 - accuracy: 0.9161\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.3936 - accuracy: 0.8696\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.3046 - accuracy: 0.9125\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.1900 - accuracy: 0.9339\n"
     ]
    }
   ],
   "source": [
    "input_shape=(224,224,1)\n",
    "epochs=50\n",
    "\n",
    "result =model.fit(preprocessed_images_train, one_hot_labels_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 69s 4s/step - loss: 11.6394 - accuracy: 0.0071\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow\n",
    "from keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 224, 224, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_168 (Conv2D)         (None, 224, 224, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 112, 112, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_169 (Conv2D)         (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 56, 56, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_170 (Conv2D)         (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_171 (Conv2D)         (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 112, 112, 128)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_172 (Conv2D)         (None, 112, 112, 64)      73792     \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 224, 224, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_173 (Conv2D)         (None, 224, 224, 1)       577       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 314,625\n",
      "Trainable params: 314,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape = (224,224, 1))\n",
    "def autoencoder(input_img):\n",
    "    #encoder\n",
    "    #input = 28 x 28 x 1 (wide and thin)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)#decoder\n",
    "    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128\n",
    "    up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n",
    "    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n",
    "    up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
    "    return decoded\n",
    "autoencoder = Model(input_img, autoencoder(input_img))\n",
    "\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop(), metrics=['accuracy'])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "9/9 [==============================] - 85s 9s/step - loss: 0.0737 - accuracy: 0.0112\n",
      "Epoch 2/30\n",
      "9/9 [==============================] - 80s 9s/step - loss: 2.2123e-05 - accuracy: 0.0112\n",
      "Epoch 3/30\n",
      "9/9 [==============================] - 86s 10s/step - loss: 5.5401e-06 - accuracy: 0.0112\n",
      "Epoch 4/30\n",
      "9/9 [==============================] - 90s 10s/step - loss: 2.7483e-06 - accuracy: 0.0112\n",
      "Epoch 5/30\n",
      "9/9 [==============================] - 81s 9s/step - loss: 1.5675e-06 - accuracy: 0.0112\n",
      "Epoch 6/30\n",
      "9/9 [==============================] - 75s 8s/step - loss: 9.3442e-07 - accuracy: 0.0112\n",
      "Epoch 7/30\n",
      "9/9 [==============================] - 75s 8s/step - loss: 5.6836e-07 - accuracy: 0.0112\n",
      "Epoch 8/30\n",
      "9/9 [==============================] - 77s 8s/step - loss: 3.5233e-07 - accuracy: 0.0112\n",
      "Epoch 9/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 2.2326e-07 - accuracy: 0.0112\n",
      "Epoch 10/30\n",
      "9/9 [==============================] - 78s 9s/step - loss: 1.4511e-07 - accuracy: 0.0112\n",
      "Epoch 11/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 9.6665e-08 - accuracy: 0.0112\n",
      "Epoch 12/30\n",
      "9/9 [==============================] - 78s 9s/step - loss: 6.5778e-08 - accuracy: 0.0112\n",
      "Epoch 13/30\n",
      "9/9 [==============================] - 82s 9s/step - loss: 4.5523e-08 - accuracy: 0.0112\n",
      "Epoch 14/30\n",
      "9/9 [==============================] - 79s 9s/step - loss: 3.1893e-08 - accuracy: 0.0112\n",
      "Epoch 15/30\n",
      "9/9 [==============================] - 81s 9s/step - loss: 2.2525e-08 - accuracy: 0.0112\n",
      "Epoch 16/30\n",
      "9/9 [==============================] - 90s 10s/step - loss: 1.5987e-08 - accuracy: 0.0112\n",
      "Epoch 17/30\n",
      "9/9 [==============================] - 78s 9s/step - loss: 1.1382e-08 - accuracy: 0.0112\n",
      "Epoch 18/30\n",
      "9/9 [==============================] - 74s 8s/step - loss: 8.1278e-09 - accuracy: 0.0112\n",
      "Epoch 19/30\n",
      "9/9 [==============================] - 75s 8s/step - loss: 5.8251e-09 - accuracy: 0.0112\n",
      "Epoch 20/30\n",
      "9/9 [==============================] - 75s 8s/step - loss: 4.1938e-09 - accuracy: 0.0112\n",
      "Epoch 21/30\n",
      "9/9 [==============================] - 77s 9s/step - loss: 3.0360e-09 - accuracy: 0.0112\n",
      "Epoch 22/30\n",
      "9/9 [==============================] - 77s 9s/step - loss: 2.2113e-09 - accuracy: 0.0112\n",
      "Epoch 23/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 1.6209e-09 - accuracy: 0.0112\n",
      "Epoch 24/30\n",
      "9/9 [==============================] - 78s 9s/step - loss: 1.1960e-09 - accuracy: 0.0112\n",
      "Epoch 25/30\n",
      "9/9 [==============================] - 80s 9s/step - loss: 8.8891e-10 - accuracy: 0.0112\n",
      "Epoch 26/30\n",
      "9/9 [==============================] - 80s 9s/step - loss: 6.6674e-10 - accuracy: 0.0112\n",
      "Epoch 27/30\n",
      "9/9 [==============================] - 86s 9s/step - loss: 5.0623e-10 - accuracy: 0.0112\n",
      "Epoch 28/30\n",
      "9/9 [==============================] - 87s 10s/step - loss: 3.9068e-10 - accuracy: 0.0112\n",
      "Epoch 29/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 3.0779e-10 - accuracy: 0.0112\n",
      "Epoch 30/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 2.4843e-10 - accuracy: 0.0112\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "autoencoder_train = autoencoder.fit(x_train, x_train, batch_size=batch_size,epochs=epochs,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 11s 570ms/step - loss: 2.2276e-10 - accuracy: 0.0108\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = autoencoder.evaluate(x_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "\n",
    "#Convolutional block to be used in autoencoder and U-Net\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block for autoencoder (no skip connections)\n",
    "def decoder_block(input, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Encoder will be the same for Autoencoder and U-net\n",
    "#We are getting both conv output and maxpool output for convenience.\n",
    "#we will ignore conv output for Autoencoder. It acts as skip connections for U-Net\n",
    "def build_encoder(input_image):\n",
    "    #inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(input_image, 32)\n",
    "    s2, p2 = encoder_block(p1, 64)\n",
    "    s3, p3 = encoder_block(p2, 128)\n",
    "    \n",
    "    encoded = conv_block(p3, 256) #Bridge\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "#Decoder for Autoencoder ONLY. \n",
    "def build_decoder(encoded):\n",
    "    d1 = decoder_block(encoded, 128)\n",
    "    d2 = decoder_block(d1, 64)\n",
    "    d3 = decoder_block(d2, 32)\n",
    "     \n",
    "    decoded = Conv2D(3, 3, padding=\"same\", activation=\"sigmoid\")(d3)\n",
    "    return decoded\n",
    "\n",
    "#Use encoder and decoder blocks to build the autoencoder. \n",
    "def build_autoencoder(input_shape):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    autoencoder = Model(input_img, build_decoder(build_encoder(input_img)))\n",
    "    return(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 224, 224, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_141 (Conv2D)         (None, 224, 224, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization_203 (Ba  (None, 224, 224, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv2d_142 (Conv2D)         (None, 224, 224, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_204 (Ba  (None, 224, 224, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 112, 112, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_143 (Conv2D)         (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_205 (Ba  (None, 112, 112, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2d_144 (Conv2D)         (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_206 (Ba  (None, 112, 112, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 56, 56, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_145 (Conv2D)         (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_207 (Ba  (None, 56, 56, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv2d_146 (Conv2D)         (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_208 (Ba  (None, 56, 56, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 28, 28, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_147 (Conv2D)         (None, 28, 28, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_209 (Ba  (None, 28, 28, 256)      1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv2d_148 (Conv2D)         (None, 28, 28, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_210 (Ba  (None, 28, 28, 256)      1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_49 (Activation)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 56, 56, 128)      131200    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_149 (Conv2D)         (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_211 (Ba  (None, 56, 56, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_50 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv2d_150 (Conv2D)         (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_212 (Ba  (None, 56, 56, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_51 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 112, 112, 64)     32832     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_151 (Conv2D)         (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_213 (Ba  (None, 112, 112, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_52 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2d_152 (Conv2D)         (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_214 (Ba  (None, 112, 112, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_53 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_8 (Conv2DT  (None, 224, 224, 32)     8224      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_153 (Conv2D)         (None, 224, 224, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_215 (Ba  (None, 224, 224, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_54 (Activation)  (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv2d_154 (Conv2D)         (None, 224, 224, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_216 (Ba  (None, 224, 224, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_55 (Activation)  (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv2d_155 (Conv2D)         (None, 224, 224, 3)       867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,737,955\n",
      "Trainable params: 1,735,139\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (224,224,3)\n",
    "model=build_autoencoder(input_shape)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 1500, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 224 and 32 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_6/conv2d_155/Sigmoid, IteratorGetNext:1)' with input shapes: [32,224,224,3], [32,224,224].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, x_train,\n\u001b[1;32m      2\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m         shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebo___s41.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 1500, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 224 and 32 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_6/conv2d_155/Sigmoid, IteratorGetNext:1)' with input shapes: [32,224,224,3], [32,224,224].\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, x_train,\n",
    "        epochs=10,\n",
    "        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_75 (Bat  (None, 224, 224, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_54 (ReLU)             (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " depthwise_conv2d_52 (Depthw  (None, 224, 224, 32)     320       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_53 (Depthw  (None, 224, 224, 32)     320       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_76 (Bat  (None, 224, 224, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_55 (ReLU)             (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 224, 224, 64)      2112      \n",
      "                                                                 \n",
      " batch_normalization_77 (Bat  (None, 224, 224, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_56 (ReLU)             (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " depthwise_conv2d_54 (Depthw  (None, 224, 224, 64)     640       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_55 (Depthw  (None, 224, 224, 64)     640       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 224, 224, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_57 (ReLU)             (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 224, 224, 128)     8320      \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 224, 224, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_58 (ReLU)             (None, 224, 224, 128)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_56 (Depthw  (None, 224, 224, 128)    1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_57 (Depthw  (None, 224, 224, 128)    1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_80 (Bat  (None, 224, 224, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_59 (ReLU)             (None, 224, 224, 128)     0         \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 224, 224, 128)     16512     \n",
      "                                                                 \n",
      " batch_normalization_81 (Bat  (None, 224, 224, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_60 (ReLU)             (None, 224, 224, 128)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_58 (Depthw  (None, 224, 224, 128)    1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_59 (Depthw  (None, 224, 224, 128)    1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_82 (Bat  (None, 224, 224, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_61 (ReLU)             (None, 224, 224, 128)     0         \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          (None, 224, 224, 256)     33024     \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 224, 224, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_62 (ReLU)             (None, 224, 224, 256)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_60 (Depthw  (None, 224, 224, 256)    2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_61 (Depthw  (None, 224, 224, 256)    2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 224, 224, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_63 (ReLU)             (None, 224, 224, 256)     0         \n",
      "                                                                 \n",
      " conv2d_64 (Conv2D)          (None, 224, 224, 256)     65792     \n",
      "                                                                 \n",
      " batch_normalization_85 (Bat  (None, 224, 224, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_64 (ReLU)             (None, 224, 224, 256)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_62 (Depthw  (None, 224, 224, 256)    2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_63 (Depthw  (None, 224, 224, 256)    2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_86 (Bat  (None, 224, 224, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_65 (ReLU)             (None, 224, 224, 256)     0         \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          (None, 224, 224, 512)     131584    \n",
      "                                                                 \n",
      " batch_normalization_87 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_66 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_64 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_65 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_67 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_66 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_68 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_66 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_67 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_90 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_69 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_91 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_70 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_68 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_69 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_92 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_71 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_68 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_72 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_70 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_71 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_73 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_69 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_95 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_74 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_72 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_73 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_96 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_75 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_70 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_97 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_76 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_74 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_75 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_98 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_77 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_71 (Conv2D)          (None, 224, 224, 1024)    525312    \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 224, 224, 1024)   4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_78 (ReLU)             (None, 224, 224, 1024)    0         \n",
      "                                                                 \n",
      " depthwise_conv2d_76 (Depthw  (None, 224, 224, 1024)   10240     \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_77 (Depthw  (None, 224, 224, 1024)   10240     \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_100 (Ba  (None, 224, 224, 1024)   4096      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_79 (ReLU)             (None, 224, 224, 1024)    0         \n",
      "                                                                 \n",
      " conv2d_72 (Conv2D)          (None, 224, 224, 1024)    1049600   \n",
      "                                                                 \n",
      " batch_normalization_101 (Ba  (None, 224, 224, 1024)   4096      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_80 (ReLU)             (None, 224, 224, 1024)    0         \n",
      "                                                                 \n",
      " average_pooling2d_3 (Averag  (None, 224, 218, 1018)   0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 224, 218, 140)     142660    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,432,068\n",
      "Trainable params: 3,410,180\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#https://journalofcloudcomputing.springeropen.com/counter/pdf/10.1186/s13677-020-00203-9.pdf\n",
    "#https://towardsdatascience.com/building-mobilenet-from-scratch-using-tensorflow-ad009c5dd42c\n",
    "#import all necessary layers\n",
    "from tensorflow.keras.layers import Input, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import ReLU, AvgPool2D, Flatten, Dense\n",
    "from tensorflow.keras import Model\n",
    "# MobileNet block\n",
    "def mobilnet_block (x, filters, strides):\n",
    "    \n",
    "    x = DepthwiseConv2D(kernel_size = 3, strides = strides,dilation_rate=3, padding = 'same')(x)\n",
    "    x = DepthwiseConv2D(kernel_size = 3, strides = strides, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters = filters, kernel_size = 1, strides = 1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "#stem of the model\n",
    "input = Input(shape = (224,224,3))\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "# main part of the model\n",
    "x = mobilnet_block(x, filters = 64, strides = 1)\n",
    "x = mobilnet_block(x, filters = 128, strides = 1)\n",
    "x = mobilnet_block(x, filters = 128, strides = 1)\n",
    "x = mobilnet_block(x, filters = 256, strides = 1)\n",
    "x = mobilnet_block(x, filters = 256, strides = 1)\n",
    "x = mobilnet_block(x, filters = 512, strides = 1)\n",
    "for _ in range (5):\n",
    "     x = mobilnet_block(x, filters = 512, strides = 1)\n",
    "x = mobilnet_block(x, filters = 1024, strides = 1)\n",
    "x = mobilnet_block(x, filters = 1024, strides = 1)\n",
    "x = AvgPool2D (pool_size = 7, strides = 1, data_format='channels_first')(x)\n",
    "output = Dense (units = 140, activation = 'softmax')(x)\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.summary()\n",
    "#plot the model\n",
    "# tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_dtype=False,show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 140) and (32, 224, 218, 140) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train,\n\u001b[1;32m      3\u001b[0m             \n\u001b[1;32m      4\u001b[0m               epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m               validation_data\u001b[39m=\u001b[39;49m(x_test, y_test),\n\u001b[1;32m      6\u001b[0m               shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filef4szjz_s.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 140) and (32, 224, 218, 140) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train,\n",
    "            \n",
    "              epochs=50,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few_shot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
