{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to read images from FVC2006 DB2_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "# Get the path to the images\n",
    "images_path = '/home/rs/21CS91R01/research/2023_ICVGIP-Code/datasets/DB2_A'\n",
    "\n",
    "# List the images in the folder\n",
    "images = os.listdir(images_path)\n",
    "\n",
    "# Iterate over the images\n",
    "for image in images:\n",
    "  # Read the image\n",
    "  image = cv2.imread(os.path.join(images_path, image))\n",
    "\n",
    "  # Display the image\n",
    "  # cv2_imshow(image)\n",
    "  cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 13:50:58.490991: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images_path = '/home/rs/21CS91R01/research/2023_ICVGIP-Code/datasets/processed_images_path'\n",
    "if not os.path.exists(processed_images_path):\n",
    "    os.mkdir(processed_images_path)\n",
    "\n",
    "# List to store preprocessed images\n",
    "preprocessed_images_train = []\n",
    "labels_train = []\n",
    "\n",
    "preprocessed_images_test = []\n",
    "labels_test = []\n",
    "\n",
    "# Iterate over the images\n",
    "for image_name in os.listdir(images_path):\n",
    "    if image_name.endswith('.bmp'):\n",
    "        # Extract the label from the image name without considering variations\n",
    "        image_name_ex = image_name.split('.')[0]\n",
    "        image_no = image_name_ex.split('_')[0]\n",
    "        index =image_name_ex.split('_')[1]\n",
    "\n",
    "\n",
    "        # Read the image\n",
    "        image_path = os.path.join(images_path, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        # Convert the image to grayscale\n",
    "        # gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Normalize the image\n",
    "        # normalized_image = gray_image\n",
    "        normalized_image = resized_image\n",
    "\n",
    "        # Save the image\n",
    "        processed_image_path = os.path.join(processed_images_path, image_name)\n",
    "        cv2.imwrite(processed_image_path, normalized_image)\n",
    "\n",
    "        # Add the preprocessed image and label to the lists\n",
    "        if (int(index) >= 5 and int(index) <= 12):\n",
    "          preprocessed_images_train.append(normalized_image)\n",
    "          labels_train.append(image_no)\n",
    "        else:\n",
    "          preprocessed_images_test.append(normalized_image)\n",
    "          labels_test.append(image_no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of preprocessed images to a NumPy array\n",
    "preprocessed_images_train = np.array(preprocessed_images_train)\n",
    "preprocessed_images_test = np.array(preprocessed_images_test)\n",
    "print(preprocessed_images_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120,)\n"
     ]
    }
   ],
   "source": [
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "-========\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "numerical_labels_train = label_encoder.fit_transform(labels_train)\n",
    "numerical_labels_test = label_encoder.fit_transform(labels_test)\n",
    "\n",
    "\n",
    "# Convert numerical labels to one-hot encoded format\n",
    "one_hot_labels_train = to_categorical(numerical_labels_train)\n",
    "one_hot_labels_test = to_categorical(numerical_labels_test)\n",
    "\n",
    "print(one_hot_labels_train)\n",
    "print(\"-========\")\n",
    "print(one_hot_labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 16\n",
    "margin = 1  # Margin for constrastive loss.\n",
    "x_train = preprocessed_images_train\n",
    "y_train = one_hot_labels_train\n",
    "x_test = preprocessed_images_test\n",
    "y_test = one_hot_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[42, 42, 42],\n",
       "         [44, 44, 44],\n",
       "         [47, 47, 47],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        [[43, 43, 43],\n",
       "         [46, 46, 46],\n",
       "         [49, 49, 49],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        [[45, 45, 45],\n",
       "         [47, 47, 47],\n",
       "         [50, 50, 50],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[49, 49, 49],\n",
       "         [54, 54, 54],\n",
       "         [57, 57, 57],\n",
       "         ...,\n",
       "         [41, 41, 41],\n",
       "         [46, 46, 46],\n",
       "         [50, 50, 50]],\n",
       "\n",
       "        [[48, 48, 48],\n",
       "         [52, 52, 52],\n",
       "         [57, 57, 57],\n",
       "         ...,\n",
       "         [41, 41, 41],\n",
       "         [44, 44, 44],\n",
       "         [46, 46, 46]],\n",
       "\n",
       "        [[47, 47, 47],\n",
       "         [49, 49, 49],\n",
       "         [55, 55, 55],\n",
       "         ...,\n",
       "         [39, 39, 39],\n",
       "         [41, 41, 41],\n",
       "         [46, 46, 46]]],\n",
       "\n",
       "\n",
       "       [[[37, 37, 37],\n",
       "         [40, 40, 40],\n",
       "         [40, 40, 40],\n",
       "         ...,\n",
       "         [11, 11, 11],\n",
       "         [ 9,  9,  9],\n",
       "         [ 7,  7,  7]],\n",
       "\n",
       "        [[38, 38, 38],\n",
       "         [42, 42, 42],\n",
       "         [42, 42, 42],\n",
       "         ...,\n",
       "         [13, 13, 13],\n",
       "         [ 9,  9,  9],\n",
       "         [ 8,  8,  8]],\n",
       "\n",
       "        [[42, 42, 42],\n",
       "         [42, 42, 42],\n",
       "         [42, 42, 42],\n",
       "         ...,\n",
       "         [13, 13, 13],\n",
       "         [11, 11, 11],\n",
       "         [11, 11, 11]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[67, 67, 67],\n",
       "         [71, 71, 71],\n",
       "         [76, 76, 76],\n",
       "         ...,\n",
       "         [22, 22, 22],\n",
       "         [29, 29, 29],\n",
       "         [37, 37, 37]],\n",
       "\n",
       "        [[67, 67, 67],\n",
       "         [72, 72, 72],\n",
       "         [76, 76, 76],\n",
       "         ...,\n",
       "         [23, 23, 23],\n",
       "         [29, 29, 29],\n",
       "         [40, 40, 40]],\n",
       "\n",
       "        [[64, 64, 64],\n",
       "         [70, 70, 70],\n",
       "         [74, 74, 74],\n",
       "         ...,\n",
       "         [21, 21, 21],\n",
       "         [32, 32, 32],\n",
       "         [41, 41, 41]]],\n",
       "\n",
       "\n",
       "       [[[31, 31, 31],\n",
       "         [34, 34, 34],\n",
       "         [37, 37, 37],\n",
       "         ...,\n",
       "         [ 5,  5,  5],\n",
       "         [ 3,  3,  3],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[33, 33, 33],\n",
       "         [34, 34, 34],\n",
       "         [36, 36, 36],\n",
       "         ...,\n",
       "         [ 6,  6,  6],\n",
       "         [ 4,  4,  4],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[35, 35, 35],\n",
       "         [35, 35, 35],\n",
       "         [38, 38, 38],\n",
       "         ...,\n",
       "         [ 7,  7,  7],\n",
       "         [ 5,  5,  5],\n",
       "         [ 3,  3,  3]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[61, 61, 61],\n",
       "         [64, 64, 64],\n",
       "         [68, 68, 68],\n",
       "         ...,\n",
       "         [15, 15, 15],\n",
       "         [26, 26, 26],\n",
       "         [33, 33, 33]],\n",
       "\n",
       "        [[60, 60, 60],\n",
       "         [64, 64, 64],\n",
       "         [67, 67, 67],\n",
       "         ...,\n",
       "         [17, 17, 17],\n",
       "         [25, 25, 25],\n",
       "         [34, 34, 34]],\n",
       "\n",
       "        [[58, 58, 58],\n",
       "         [61, 61, 61],\n",
       "         [67, 67, 67],\n",
       "         ...,\n",
       "         [17, 17, 17],\n",
       "         [26, 26, 26],\n",
       "         [35, 35, 35]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[32, 32, 32],\n",
       "         [34, 34, 34],\n",
       "         [36, 36, 36],\n",
       "         ...,\n",
       "         [ 6,  6,  6],\n",
       "         [ 3,  3,  3],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[33, 33, 33],\n",
       "         [36, 36, 36],\n",
       "         [36, 36, 36],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 5,  5,  5],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[36, 36, 36],\n",
       "         [38, 38, 38],\n",
       "         [38, 38, 38],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 6,  6,  6],\n",
       "         [ 5,  5,  5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[66, 66, 66],\n",
       "         [68, 68, 68],\n",
       "         [74, 74, 74],\n",
       "         ...,\n",
       "         [18, 18, 18],\n",
       "         [25, 25, 25],\n",
       "         [34, 34, 34]],\n",
       "\n",
       "        [[65, 65, 65],\n",
       "         [68, 68, 68],\n",
       "         [72, 72, 72],\n",
       "         ...,\n",
       "         [18, 18, 18],\n",
       "         [27, 27, 27],\n",
       "         [36, 36, 36]],\n",
       "\n",
       "        [[63, 63, 63],\n",
       "         [68, 68, 68],\n",
       "         [71, 71, 71],\n",
       "         ...,\n",
       "         [18, 18, 18],\n",
       "         [30, 30, 30],\n",
       "         [38, 38, 38]]],\n",
       "\n",
       "\n",
       "       [[[33, 33, 33],\n",
       "         [35, 35, 35],\n",
       "         [38, 38, 38],\n",
       "         ...,\n",
       "         [ 4,  4,  4],\n",
       "         [ 3,  3,  3],\n",
       "         [ 1,  1,  1]],\n",
       "\n",
       "        [[35, 35, 35],\n",
       "         [36, 36, 36],\n",
       "         [37, 37, 37],\n",
       "         ...,\n",
       "         [ 7,  7,  7],\n",
       "         [ 5,  5,  5],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[36, 36, 36],\n",
       "         [36, 36, 36],\n",
       "         [38, 38, 38],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 5,  5,  5],\n",
       "         [ 4,  4,  4]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[62, 62, 62],\n",
       "         [64, 64, 64],\n",
       "         [69, 69, 69],\n",
       "         ...,\n",
       "         [20, 20, 20],\n",
       "         [28, 28, 28],\n",
       "         [34, 34, 34]],\n",
       "\n",
       "        [[61, 61, 61],\n",
       "         [65, 65, 65],\n",
       "         [70, 70, 70],\n",
       "         ...,\n",
       "         [20, 20, 20],\n",
       "         [29, 29, 29],\n",
       "         [35, 35, 35]],\n",
       "\n",
       "        [[59, 59, 59],\n",
       "         [64, 64, 64],\n",
       "         [67, 67, 67],\n",
       "         ...,\n",
       "         [21, 21, 21],\n",
       "         [29, 29, 29],\n",
       "         [37, 37, 37]]],\n",
       "\n",
       "\n",
       "       [[[43, 43, 43],\n",
       "         [43, 43, 43],\n",
       "         [47, 47, 47],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        [[45, 45, 45],\n",
       "         [45, 45, 45],\n",
       "         [49, 49, 49],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        [[47, 47, 47],\n",
       "         [47, 47, 47],\n",
       "         [50, 50, 50],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[50, 50, 50],\n",
       "         [53, 53, 53],\n",
       "         [57, 57, 57],\n",
       "         ...,\n",
       "         [28, 28, 28],\n",
       "         [35, 35, 35],\n",
       "         [38, 38, 38]],\n",
       "\n",
       "        [[51, 51, 51],\n",
       "         [55, 55, 55],\n",
       "         [57, 57, 57],\n",
       "         ...,\n",
       "         [30, 30, 30],\n",
       "         [34, 34, 34],\n",
       "         [36, 36, 36]],\n",
       "\n",
       "        [[49, 49, 49],\n",
       "         [52, 52, 52],\n",
       "         [59, 59, 59],\n",
       "         ...,\n",
       "         [29, 29, 29],\n",
       "         [31, 31, 31],\n",
       "         [34, 34, 34]]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape=(1120, 224, 224, 3)\n",
      "y_train.shape=(1120, 140)\n",
      "x_test.shape=(560, 224, 224, 3)\n",
      "y_test.shape=(560, 140)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{x_test.shape=}\")\n",
    "print(f\"{y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.paperspace.com/attention-mechanisms-in-computer-vision-cbam/\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\n",
    "from keras import backend as K\n",
    "from keras.activations import sigmoid\n",
    "\n",
    "def attach_attention_module(net, attention_module):\n",
    "  if attention_module == 'se_block': # SE_block\n",
    "    net = se_block(net)\n",
    "  elif attention_module == 'cbam_block': # CBAM_block\n",
    "    net = cbam_block(net)\n",
    "  else:\n",
    "    raise Exception(\"'{}' is not supported attention module!\".format(attention_module))\n",
    "\n",
    "  return net\n",
    "\n",
    "def se_block(input_feature, ratio=8):\n",
    "\t\"\"\"Contains the implementation of Squeeze-and-Excitation(SE) block.\n",
    "\tAs described in https://arxiv.org/abs/1709.01507.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\n",
    "\tse_feature = GlobalAveragePooling2D()(input_feature)\n",
    "\tse_feature = Reshape((1, 1, channel))(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel)\n",
    "\tse_feature = Dense(channel // ratio,\n",
    "\t\t\t\t\t   activation='relu',\n",
    "\t\t\t\t\t   kernel_initializer='he_normal',\n",
    "\t\t\t\t\t   use_bias=True,\n",
    "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel//ratio)\n",
    "\tse_feature = Dense(channel,\n",
    "\t\t\t\t\t   activation='sigmoid',\n",
    "\t\t\t\t\t   kernel_initializer='he_normal',\n",
    "\t\t\t\t\t   use_bias=True,\n",
    "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel)\n",
    "\tif K.image_data_format() == 'channels_first':\n",
    "\t\tse_feature = Permute((3, 1, 2))(se_feature)\n",
    "\n",
    "\tse_feature = multiply([input_feature, se_feature])\n",
    "\treturn se_feature\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "\t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "\tAs described in https://arxiv.org/abs/1807.06521.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
    "\tcbam_feature = spatial_attention(cbam_feature)\n",
    "\treturn cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\t\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\t\n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\t\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "\tkernel_size = 7\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tchannel = input_feature.shape[1]\n",
    "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
    "\telse:\n",
    "\t\tchannel = input_feature.shape[-1]\n",
    "\t\tcbam_feature = input_feature\n",
    "\t\n",
    "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert avg_pool.shape[-1] == 1\n",
    "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert max_pool.shape[-1] == 1\n",
    "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\tassert concat.shape[-1] == 2\n",
    "\tcbam_feature = Conv2D(filters = 1,\n",
    "\t\t\t\t\tkernel_size=kernel_size,\n",
    "\t\t\t\t\tstrides=1,\n",
    "\t\t\t\t\tpadding='same',\n",
    "\t\t\t\t\tactivation='sigmoid',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tuse_bias=False)(concat)\t\n",
    "\tassert cbam_feature.shape[-1] == 1\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\t\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\t\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet v1\n",
    "This is a revised implementation from Cifar10 ResNet example in Keras:\n",
    "(https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py)\n",
    "[a] Deep Residual Learning for Image Recognition\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=140, attention_module=None):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            # attention_module\n",
    "            if attention_module is not None:\n",
    "                y = attach_attention_module(y, attention_module)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "data_augmentation = False\n",
    "num_classes = 10\n",
    "subtract_pixel_mean = True  # Subtracting pixel mean improves accuracy\n",
    "base_model = 'resnet20'\n",
    "# Choose what attention_module to use: cbam_block / se_block / None\n",
    "attention_module = 'cbam_block'\n",
    "model_type = base_model if attention_module==None else base_model+'_'+attention_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1120, 224, 224, 3)\n",
      "1120 train samples\n",
      "560 test samples\n",
      "y_train shape: (1120, 140)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Input\n",
    "depth = 20 # For ResNet, specify the depth (e.g. ResNet50: depth=50)\n",
    "model = resnet_v1(input_shape=input_shape, depth=depth, attention_module=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 224, 224, 16  448         ['input_3[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 224, 224, 16  64         ['conv2d_21[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 224, 224, 16  0           ['batch_normalization_19[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 224, 224, 16  2320        ['activation_19[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 224, 224, 16  64         ['conv2d_22[0][0]']              \n",
      " ormalization)                  )                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 224, 224, 16  0           ['batch_normalization_20[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 224, 224, 16  2320        ['activation_20[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 224, 224, 16  64         ['conv2d_23[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 224, 224, 16  0           ['activation_19[0][0]',          \n",
      "                                )                                 'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 224, 224, 16  0           ['add_33[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 224, 224, 16  2320        ['activation_21[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 224, 224, 16  64         ['conv2d_24[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 224, 224, 16  0           ['batch_normalization_22[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 224, 224, 16  2320        ['activation_22[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 224, 224, 16  64         ['conv2d_25[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " add_34 (Add)                   (None, 224, 224, 16  0           ['activation_21[0][0]',          \n",
      "                                )                                 'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 224, 224, 16  0           ['add_34[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 224, 224, 16  2320        ['activation_23[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 224, 224, 16  64         ['conv2d_26[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 224, 224, 16  0           ['batch_normalization_24[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 224, 224, 16  2320        ['activation_24[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 224, 224, 16  64         ['conv2d_27[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " add_35 (Add)                   (None, 224, 224, 16  0           ['activation_23[0][0]',          \n",
      "                                )                                 'batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 224, 224, 16  0           ['add_35[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 112, 112, 32  4640        ['activation_25[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 112, 112, 32  128        ['conv2d_28[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 112, 112, 32  0           ['batch_normalization_26[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 112, 112, 32  9248        ['activation_26[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 112, 112, 32  544         ['activation_25[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 112, 112, 32  128        ['conv2d_29[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " add_36 (Add)                   (None, 112, 112, 32  0           ['conv2d_30[0][0]',              \n",
      "                                )                                 'batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 112, 112, 32  0           ['add_36[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 112, 112, 32  9248        ['activation_27[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 112, 112, 32  128        ['conv2d_31[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 112, 112, 32  0           ['batch_normalization_28[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 112, 112, 32  9248        ['activation_28[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 112, 112, 32  128        ['conv2d_32[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " add_37 (Add)                   (None, 112, 112, 32  0           ['activation_27[0][0]',          \n",
      "                                )                                 'batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 112, 112, 32  0           ['add_37[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 112, 112, 32  9248        ['activation_29[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 112, 112, 32  128        ['conv2d_33[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 112, 112, 32  0           ['batch_normalization_30[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 112, 112, 32  9248        ['activation_30[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 112, 112, 32  128        ['conv2d_34[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " add_38 (Add)                   (None, 112, 112, 32  0           ['activation_29[0][0]',          \n",
      "                                )                                 'batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 112, 112, 32  0           ['add_38[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 56, 56, 64)   18496       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 56, 56, 64)  256         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 56, 56, 64)   0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 56, 56, 64)   2112        ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 56, 56, 64)  256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 56, 56, 64)   0           ['conv2d_37[0][0]',              \n",
      "                                                                  'batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 56, 56, 64)   0           ['add_39[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 56, 56, 64)  256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 56, 56, 64)   0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 56, 56, 64)  256         ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_40 (Add)                   (None, 56, 56, 64)   0           ['activation_33[0][0]',          \n",
      "                                                                  'batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 56, 56, 64)   0           ['add_40[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 56, 56, 64)  256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 56, 56, 64)   0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 56, 56, 64)   36928       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 56, 56, 64)  256         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_41 (Add)                   (None, 56, 56, 64)   0           ['activation_35[0][0]',          \n",
      "                                                                  'batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 56, 56, 64)   0           ['add_41[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 7, 7, 64)    0           ['activation_37[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 3136)         0           ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 140)          439180      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 712,972\n",
      "Trainable params: 711,596\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "resnet20_cbam_block\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35/35 [==============================] - 466s 13s/step - loss: 8.1557 - accuracy: 0.0161 - val_loss: 747.0964 - val_accuracy: 0.0054\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 523s 15s/step - loss: 4.3040 - accuracy: 0.1277 - val_loss: 8.8994 - val_accuracy: 0.0143\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 483s 14s/step - loss: 3.0175 - accuracy: 0.3232 - val_loss: 6.0258 - val_accuracy: 0.0304\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 475s 14s/step - loss: 1.9003 - accuracy: 0.5518 - val_loss: 7.6319 - val_accuracy: 0.0429\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 475s 14s/step - loss: 1.1954 - accuracy: 0.7054 - val_loss: 6.5585 - val_accuracy: 0.0946\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 479s 14s/step - loss: 0.5827 - accuracy: 0.8929 - val_loss: 7.4542 - val_accuracy: 0.0893\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 516s 15s/step - loss: 0.4396 - accuracy: 0.9295 - val_loss: 6.6095 - val_accuracy: 0.1393\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 486s 14s/step - loss: 0.3137 - accuracy: 0.9589 - val_loss: 5.7296 - val_accuracy: 0.1893\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 537s 16s/step - loss: 0.3200 - accuracy: 0.9643 - val_loss: 4.7151 - val_accuracy: 0.2411\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 465s 14s/step - loss: 0.2490 - accuracy: 0.9732 - val_loss: 3.9861 - val_accuracy: 0.3161\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 473s 14s/step - loss: 0.2079 - accuracy: 0.9902 - val_loss: 2.2514 - val_accuracy: 0.5911\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 529s 15s/step - loss: 0.1736 - accuracy: 0.9964 - val_loss: 2.2276 - val_accuracy: 0.5946\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 472s 14s/step - loss: 0.1834 - accuracy: 0.9920 - val_loss: 2.5151 - val_accuracy: 0.5893\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 470s 14s/step - loss: 0.1857 - accuracy: 0.9929 - val_loss: 1.9878 - val_accuracy: 0.6696\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 462s 13s/step - loss: 0.1711 - accuracy: 0.9964 - val_loss: 1.9426 - val_accuracy: 0.6661\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 487s 14s/step - loss: 0.1822 - accuracy: 0.9920 - val_loss: 1.9158 - val_accuracy: 0.7054\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 457s 13s/step - loss: 0.2681 - accuracy: 0.9741 - val_loss: 2.5918 - val_accuracy: 0.5893\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 469s 14s/step - loss: 0.2457 - accuracy: 0.9723 - val_loss: 2.0517 - val_accuracy: 0.6768\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 455s 13s/step - loss: 0.1988 - accuracy: 0.9875 - val_loss: 1.8989 - val_accuracy: 0.7107\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 461s 13s/step - loss: 0.2068 - accuracy: 0.9804 - val_loss: 2.1359 - val_accuracy: 0.6482\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 455s 13s/step - loss: 0.2006 - accuracy: 0.9866 - val_loss: 2.3622 - val_accuracy: 0.6554\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 467s 14s/step - loss: 0.1897 - accuracy: 0.9893 - val_loss: 2.1014 - val_accuracy: 0.6839\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 468s 14s/step - loss: 0.1861 - accuracy: 0.9857 - val_loss: 2.3500 - val_accuracy: 0.6643\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 498s 14s/step - loss: 0.2051 - accuracy: 0.9821 - val_loss: 2.4251 - val_accuracy: 0.6500\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 457s 13s/step - loss: 0.2175 - accuracy: 0.9795 - val_loss: 2.9488 - val_accuracy: 0.6054\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 464s 13s/step - loss: 0.1879 - accuracy: 0.9929 - val_loss: 1.9468 - val_accuracy: 0.7107\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 455s 13s/step - loss: 0.1561 - accuracy: 0.9964 - val_loss: 1.7123 - val_accuracy: 0.7357\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 511s 15s/step - loss: 0.1481 - accuracy: 0.9973 - val_loss: 2.1197 - val_accuracy: 0.7036\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 454s 13s/step - loss: 0.1437 - accuracy: 0.9991 - val_loss: 1.5741 - val_accuracy: 0.7679\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 460s 13s/step - loss: 0.1454 - accuracy: 0.9964 - val_loss: 1.7047 - val_accuracy: 0.7536\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 459s 13s/step - loss: 0.1456 - accuracy: 0.9982 - val_loss: 1.6938 - val_accuracy: 0.7375\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 458s 13s/step - loss: 0.1344 - accuracy: 1.0000 - val_loss: 1.6566 - val_accuracy: 0.7589\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 442s 13s/step - loss: 0.1323 - accuracy: 1.0000 - val_loss: 1.6287 - val_accuracy: 0.7696\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 452s 13s/step - loss: 0.1307 - accuracy: 1.0000 - val_loss: 1.6127 - val_accuracy: 0.7607\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 453s 13s/step - loss: 0.1290 - accuracy: 1.0000 - val_loss: 1.5956 - val_accuracy: 0.7643\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 455s 13s/step - loss: 0.1279 - accuracy: 1.0000 - val_loss: 1.5564 - val_accuracy: 0.7661\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 460s 13s/step - loss: 0.1268 - accuracy: 1.0000 - val_loss: 1.5874 - val_accuracy: 0.7679\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 456s 13s/step - loss: 0.1253 - accuracy: 1.0000 - val_loss: 1.6076 - val_accuracy: 0.7571\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 447s 13s/step - loss: 0.1238 - accuracy: 1.0000 - val_loss: 1.5691 - val_accuracy: 0.7696\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 450s 13s/step - loss: 0.1226 - accuracy: 1.0000 - val_loss: 1.5467 - val_accuracy: 0.7714\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 456s 13s/step - loss: 0.1213 - accuracy: 1.0000 - val_loss: 1.5431 - val_accuracy: 0.7679\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 512s 15s/step - loss: 0.1199 - accuracy: 1.0000 - val_loss: 1.5340 - val_accuracy: 0.7714\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 455s 13s/step - loss: 0.1186 - accuracy: 1.0000 - val_loss: 1.5178 - val_accuracy: 0.7804\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 461s 13s/step - loss: 0.1173 - accuracy: 1.0000 - val_loss: 1.5094 - val_accuracy: 0.7804\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 461s 13s/step - loss: 0.1160 - accuracy: 1.0000 - val_loss: 1.5016 - val_accuracy: 0.7839\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 462s 13s/step - loss: 0.1148 - accuracy: 1.0000 - val_loss: 1.4969 - val_accuracy: 0.7821\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 457s 13s/step - loss: 0.1135 - accuracy: 1.0000 - val_loss: 1.4924 - val_accuracy: 0.7839\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 461s 13s/step - loss: 0.1122 - accuracy: 1.0000 - val_loss: 1.4963 - val_accuracy: 0.7804\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 456s 13s/step - loss: 0.1109 - accuracy: 1.0000 - val_loss: 1.4937 - val_accuracy: 0.7857\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 462s 13s/step - loss: 0.1096 - accuracy: 1.0000 - val_loss: 1.4902 - val_accuracy: 0.7857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6e8202470>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=50,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 255s 14s/step - loss: 1.5554 - accuracy: 0.7411\n",
      "Test loss: 1.5553959608078003\n",
      "Test accuracy: 0.7410714030265808\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Hyper-Parameters\n",
    "\n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "optimizer = Adam(learning_rate=lr) #lr = 1e-4,\n",
    "# optimizer = RMSprop(learning_rate=lr) #lr = 1e-5\n",
    "# optimizer = SGD(learning_rate=lr) #lr = 1e-2\n",
    "# optimizer = tfa.optimizers.SGDW(learning_rate=lr, momentum=0.9, nesterov=True, weight_decay=1e-4) #lr = 1e-3\n",
    "batch_size = 50\n",
    "input_shape = x_test.shape\n",
    "input_shape = (224,224,3)\n",
    "output_num_units = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Layer, Conv2D, MaxPooling2D, BatchNormalization, ZeroPadding2D\n",
    "from tensorflow.keras.metrics import Precision, Recall, TruePositives\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, \\\n",
    "EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.nn import local_response_normalization #, conv2d\n",
    "\n",
    "# import tensorflow_probability as tfp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from typing import Any, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Localization(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 filters_1: int, \n",
    "                 filters_2: int, \n",
    "                 fc_units: int, \n",
    "                 kernel_size=(5,5),\n",
    "                 pool_size=(2,2),\n",
    "                 name='localization', \n",
    "                 **kwargs):\n",
    "        super(Localization, self).__init__(**kwargs)\n",
    "        self.filters_1 = filters_1\n",
    "        self.filters_2 = filters_2\n",
    "        self.fc_units = fc_units\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.network = keras.Sequential(\n",
    "            [\n",
    "                MaxPooling2D(pool_size=pool_size, name=name+'_mp_1'),\n",
    "                Conv2D(filters=filters_1, \n",
    "                       kernel_size=kernel_size, \n",
    "                       padding='same', \n",
    "                       strides=1, \n",
    "                       activation='relu',\n",
    "                       name=name+'_c_1'),\n",
    "                MaxPooling2D(pool_size=pool_size, name=name+'_mp_2'),\n",
    "                Conv2D(filters=filters_2, \n",
    "                       kernel_size=kernel_size, \n",
    "                       padding='same', \n",
    "                       strides=1, \n",
    "                       activation='relu',\n",
    "                       name=name+'_c_2'),\n",
    "                MaxPooling2D(pool_size=pool_size, name=name+'_mp_3'),\n",
    "                Flatten(name=name+'_fl'),\n",
    "                Dense(fc_units, activation='relu', name=name+'_d_1'),\n",
    "                Dense(6, activation=None, \n",
    "                      bias_initializer=tf.keras.initializers.constant\\\n",
    "                      ([1.0, 0.0, 0.0, 0.0, 1.0, 0.0]), \n",
    "                      kernel_initializer='zeros',\n",
    "                      name=name+'_d_2'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(\"Building Localization Network with input shape:\", input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [None, 6]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        theta = self.network(inputs)\n",
    "        theta = tf.keras.layers.Reshape((2, 3))(theta)\n",
    "        return theta\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Localization, self).get_config()\n",
    "        config.update({\n",
    "            'filters_1': self.filters_1,\n",
    "            'filters_2': self.filters_2,\n",
    "            'fc_units': self.fc_units,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'pool_size': self.pool_size,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearInterpolation(tf.keras.layers.Layer):\n",
    "    def __init__(self, height=48, width=48):\n",
    "        super(BilinearInterpolation, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [None, self.height, self.width, 1]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'height': self.height,\n",
    "            'width': self.width,\n",
    "        }\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        print(\"Building Bilinear Interpolation Layer with input shape:\", input_shape)\n",
    "\n",
    "    def advance_indexing(self, inputs, x, y):\n",
    "        '''\n",
    "        Utility function to get pixel value for coordinate\n",
    "        vectors x and y from a  4D tensor image.\n",
    "        '''        \n",
    "        shape = tf.shape(inputs)\n",
    "        batch_size, _, _ = shape[0], shape[1], shape[2]\n",
    "        \n",
    "        batch_idx = tf.range(0, batch_size)\n",
    "        batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
    "        b = tf.tile(batch_idx, (1, self.height, self.width))\n",
    "        indices = tf.stack([b, y, x], 3)\n",
    "        return tf.gather_nd(inputs, indices)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        images, theta = inputs\n",
    "        sampling_grid = self.grid_generator(batch=tf.shape(images)[0])\n",
    "        return self.interpolate(images, sampling_grid, theta)\n",
    "\n",
    "    def grid_generator(self, batch):\n",
    "        '''\n",
    "        This function returns a sampling grid, which when\n",
    "        used with the bilinear sampler on the input feature\n",
    "        map, will create an output feature map that is an\n",
    "        affine transformation of the input feature map.\n",
    "        '''\n",
    "        # create normalized 2D grid\n",
    "        x = tf.linspace(-1, 1, self.width)\n",
    "        y = tf.linspace(-1, 1, self.height)\n",
    "        # x and y are selected in the range of -1 to 1 so the the transformation happens considering the center\n",
    "        # of the image as the origin. The images will be later scaled up.\n",
    "        xx, yy = tf.meshgrid(x, y)\n",
    "            \n",
    "        # flatten\n",
    "        xx = tf.reshape(xx, (-1,))\n",
    "        yy = tf.reshape(yy, (-1,))\n",
    "\n",
    "        # reshape to [x_t, y_t , 1] - (homogeneous form)\n",
    "        homogenous_coordinates = tf.stack([xx, yy, tf.ones_like(xx)])\n",
    "        # # repeat grid num_batch times\n",
    "        # sampling_grid = np.resize(sampling_grid, (num_batch, 3, H*W))\n",
    "        # repeat grid num_batch times\n",
    "        homogenous_coordinates = tf.expand_dims(homogenous_coordinates, axis=0)\n",
    "        homogenous_coordinates = tf.tile(homogenous_coordinates, tf.stack([batch, 1, 1]))\n",
    "        # homogenous_coordinates = tf.tile(homogenous_coordinates, [batch, 1, 1])\n",
    "\n",
    "        # cast to float32 (required for matmul)\n",
    "        homogenous_coordinates = tf.cast(homogenous_coordinates, dtype=tf.float32)\n",
    "\n",
    "        return homogenous_coordinates\n",
    "    \n",
    "    def interpolate(self, images, grid, theta):\n",
    "        '''\n",
    "        Performs bilinear sampling of the input images according to the\n",
    "        normalized coordinates provided by the sampling grid. Note that\n",
    "        the sampling is done identically for each channel of the input.\n",
    "        '''\n",
    "\n",
    "        with tf.name_scope(\"Transformation\"):\n",
    "            # transform the sampling grid - batch multiply\n",
    "            transformed = tf.matmul(theta, grid)\n",
    "            # batch grid has shape (num_batch, 2, H*W)\n",
    "\n",
    "            # reshape to (num_batch, H, W, 2)\n",
    "            transformed = tf.transpose(transformed, perm=[0, 2, 1])\n",
    "            transformed = tf.reshape(transformed, [-1, self.height, self.width, 2])\n",
    "                \n",
    "            x_transformed = transformed[:, :, :, 0]\n",
    "            y_transformed = transformed[:, :, :, 1]\n",
    "                \n",
    "            # rescale x and y to [0, W-1/H-1]\n",
    "            x = ((x_transformed + 1.) * tf.cast(self.width, dtype=tf.float32)) * 0.5\n",
    "            y = ((y_transformed + 1.) * tf.cast(self.height, dtype=tf.float32)) * 0.5\n",
    "\n",
    "        with tf.name_scope(\"VariableCasting\"):\n",
    "            # grab 4 nearest corner points for each (x_i, y_i)\n",
    "            x0 = tf.cast(tf.math.floor(x), dtype=tf.int32)\n",
    "            x1 = x0 + 1\n",
    "            y0 = tf.cast(tf.math.floor(y), dtype=tf.int32)\n",
    "            y1 = y0 + 1\n",
    "\n",
    "            # clip to range [0, H-1/W-1] to not violate img boundaries\n",
    "            x0 = tf.clip_by_value(x0, 0, self.width-1)\n",
    "            x1 = tf.clip_by_value(x1, 0, self.width-1)\n",
    "            y0 = tf.clip_by_value(y0, 0, self.height-1)\n",
    "            y1 = tf.clip_by_value(y1, 0, self.height-1)\n",
    "            x = tf.clip_by_value(x, 0, tf.cast(self.width, dtype=tf.float32)-1.0)\n",
    "            y = tf.clip_by_value(y, 0, tf.cast(self.height, dtype=tf.float32)-1)\n",
    "\n",
    "        with tf.name_scope(\"AdvanceIndexing\"):\n",
    "            # get pixel value at corner coords\n",
    "            Ia = self.advance_indexing(images, x0, y0)\n",
    "            Ib = self.advance_indexing(images, x0, y1)\n",
    "            Ic = self.advance_indexing(images, x1, y0)\n",
    "            Id = self.advance_indexing(images, x1, y1)\n",
    "\n",
    "        with tf.name_scope(\"Interpolation\"):\n",
    "            # recast as float for delta calculation\n",
    "            x0 = tf.cast(x0, dtype=tf.float32)\n",
    "            x1 = tf.cast(x1, dtype=tf.float32)\n",
    "            y0 = tf.cast(y0, dtype=tf.float32)\n",
    "            y1 = tf.cast(y1, dtype=tf.float32)\n",
    "                            \n",
    "            # calculate deltas\n",
    "            wa = (x1-x) * (y1-y)\n",
    "            wb = (x1-x) * (y-y0)\n",
    "            wc = (x-x0) * (y1-y)\n",
    "            wd = (x-x0) * (y-y0)\n",
    "\n",
    "            # add dimension for addition\n",
    "            wa = tf.expand_dims(wa, axis=3)\n",
    "            wb = tf.expand_dims(wb, axis=3)\n",
    "            wc = tf.expand_dims(wc, axis=3)\n",
    "            wd = tf.expand_dims(wd, axis=3)\n",
    "                        \n",
    "        return tf.math.add_n([wa*Ia + wb*Ib + wc*Ic + wd*Id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_block(filters, kernel_size, pool_size=(2, 2), name='zcmn'):\n",
    "    def layer_fn(x):\n",
    "        zeropad = ZeroPadding2D(padding=2, \n",
    "                                name=name+'_zp')(x)\n",
    "        conv = Conv2D(filters=filters, \n",
    "                    kernel_size=kernel_size, \n",
    "                    padding='valid', \n",
    "                    strides=1, \n",
    "                    activation='relu', \n",
    "                    name=name+'_c')(zeropad)   \n",
    "        maxpool = MaxPooling2D(pool_size=pool_size, \n",
    "                                strides=2, \n",
    "                                padding='valid',\n",
    "                                name=name+'_mp')(conv)       \n",
    "        norm = local_response_normalization(maxpool, name=name+'_lrn')\n",
    "\n",
    "        return norm\n",
    "    return layer_fn\n",
    "\n",
    "def STN(input_shape=input_shape, num_classes=output_num_units):\n",
    "    image = Input(shape=input_shape)\n",
    "    theta = Localization(250, 250, 250, name='localization_0')(image)\n",
    "    spatial_transformer_network_1 = BilinearInterpolation(height=input_shape[0], \n",
    "                                                          width=input_shape[1])([image, theta])    \n",
    "    conv_1 = conv_block(200, (7,7), name='zcmn_0')(spatial_transformer_network_1)\n",
    "    theta = Localization(150, 200, 300, name='localization_1')(conv_1)\n",
    "    spatial_transformer_network_2 = BilinearInterpolation(height=23, \n",
    "                                                          width=23)([conv_1, theta])        \n",
    "    conv_2 = conv_block(250, (4,4), name='zcmn_1')(spatial_transformer_network_2)\n",
    "    theta = Localization(150, 200, 300, name='localization_2')(conv_2)\n",
    "    spatial_transformer_network_3 = BilinearInterpolation(height=12, \n",
    "                                                          width=12)([conv_2, theta])         \n",
    "    conv_3 = conv_block(350, (4,4), name='zcmn_2')(spatial_transformer_network_3)\n",
    "    flatten = Flatten()(conv_3)\n",
    "    dense = Dense(units= 400, activation='relu')(flatten)\n",
    "    predictions = Dense(units=num_classes, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(inputs=image, outputs=predictions)\n",
    "    return model\n",
    " \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Localization Network with input shape: (None, 224, 224, 3)\n",
      "Building Bilinear Interpolation Layer with input shape: [TensorShape([None, 224, 224, 3]), TensorShape([None, 2, 3])]\n",
      "Building Localization Network with input shape: (None, 111, 111, 200)\n",
      "Building Bilinear Interpolation Layer with input shape: [TensorShape([None, 111, 111, 200]), TensorShape([None, 2, 3])]\n",
      "Building Localization Network with input shape: (None, 12, 12, 250)\n",
      "Building Bilinear Interpolation Layer with input shape: [TensorShape([None, 12, 12, 250]), TensorShape([None, 2, 3])]\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " localization (Localization)    (None, 2, 3)         50583506    ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " bilinear_interpolation (Biline  (None, 224, 224, 3)  0          ['input_12[0][0]',               \n",
      " arInterpolation)                                                 'localization[0][0]']           \n",
      "                                                                                                  \n",
      " zcmn_0_zp (ZeroPadding2D)      (None, 228, 228, 3)  0           ['bilinear_interpolation[0][0]'] \n",
      "                                                                                                  \n",
      " zcmn_0_c (Conv2D)              (None, 222, 222, 20  29600       ['zcmn_0_zp[0][0]']              \n",
      "                                0)                                                                \n",
      "                                                                                                  \n",
      " zcmn_0_mp (MaxPooling2D)       (None, 111, 111, 20  0           ['zcmn_0_c[0][0]']               \n",
      "                                0)                                                                \n",
      "                                                                                                  \n",
      " tf.nn.local_response_normaliza  (None, 111, 111, 20  0          ['zcmn_0_mp[0][0]']              \n",
      " tion_9 (TFOpLambda)            0)                                                                \n",
      "                                                                                                  \n",
      " localization_1 (Localization)  (None, 2, 3)         11642456    ['tf.nn.local_response_normalizat\n",
      "                                                                 ion_9[0][0]']                    \n",
      "                                                                                                  \n",
      " bilinear_interpolation_1 (Bili  (None, 23, 23, 200)  0          ['tf.nn.local_response_normalizat\n",
      " nearInterpolation)                                              ion_9[0][0]',                    \n",
      "                                                                  'localization_1[0][0]']         \n",
      "                                                                                                  \n",
      " zcmn_1_zp (ZeroPadding2D)      (None, 27, 27, 200)  0           ['bilinear_interpolation_1[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " zcmn_1_c (Conv2D)              (None, 24, 24, 250)  800250      ['zcmn_1_zp[0][0]']              \n",
      "                                                                                                  \n",
      " zcmn_1_mp (MaxPooling2D)       (None, 12, 12, 250)  0           ['zcmn_1_c[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.local_response_normaliza  (None, 12, 12, 250)  0          ['zcmn_1_mp[0][0]']              \n",
      " tion_10 (TFOpLambda)                                                                             \n",
      "                                                                                                  \n",
      " localization_2 (Localization)  (None, 2, 3)         1749956     ['tf.nn.local_response_normalizat\n",
      "                                                                 ion_10[0][0]']                   \n",
      "                                                                                                  \n",
      " bilinear_interpolation_2 (Bili  (None, 12, 12, 250)  0          ['tf.nn.local_response_normalizat\n",
      " nearInterpolation)                                              ion_10[0][0]',                   \n",
      "                                                                  'localization_2[0][0]']         \n",
      "                                                                                                  \n",
      " zcmn_2_zp (ZeroPadding2D)      (None, 16, 16, 250)  0           ['bilinear_interpolation_2[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " zcmn_2_c (Conv2D)              (None, 13, 13, 350)  1400350     ['zcmn_2_zp[0][0]']              \n",
      "                                                                                                  \n",
      " zcmn_2_mp (MaxPooling2D)       (None, 6, 6, 350)    0           ['zcmn_2_c[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.local_response_normaliza  (None, 6, 6, 350)   0           ['zcmn_2_mp[0][0]']              \n",
      " tion_11 (TFOpLambda)                                                                             \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 12600)        0           ['tf.nn.local_response_normalizat\n",
      "                                                                 ion_11[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 400)          5040400     ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 140)          56140       ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 71,302,658\n",
      "Trainable params: 71,302,658\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = STN()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 244s 10s/step - loss: 4.9449 - accuracy: 8.9286e-04\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 236s 10s/step - loss: 4.9417 - accuracy: 0.0063\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 236s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 237s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 235s 10s/step - loss: 4.9417 - accuracy: 0.0054\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 238s 10s/step - loss: 4.9417 - accuracy: 0.0054\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 237s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 236s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 235s 10s/step - loss: 4.9417 - accuracy: 0.0071\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 238s 10s/step - loss: 4.9417 - accuracy: 0.0036\n"
     ]
    }
   ],
   "source": [
    "  #Train model\n",
    "hist = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    epochs=10, \n",
    "                  \n",
    "                    batch_size=batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper-Parameters\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "optimizer = Adam(learning_rate=lr) #lr = 1e-3\n",
    "# optimizer = RMSprop(learning_rate=lr) #lr = 1e-3\n",
    "# optimizer = SGD(learning_rate=lr) #1e-1 \n",
    "# optimizer = tfa.optimizers.SGDW(learning_rate=lr, momentum=0.9, nesterov=True, weight_decay=1e-4) #lr = 1e-2\n",
    "\n",
    "batch_size = 50\n",
    "num_classes = 140\n",
    "input_shape = x_test.shape\n",
    "input_shape = (224,224,3)\n",
    "# input_shape = (48, 48, 3)\n",
    "image_size = input_shape[0]  # We'll resize input images to this size\n",
    "patch_size = 4 # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 32\n",
    "num_heads = 6\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 4\n",
    "mlp_head_units = [512, 256]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Patches(Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'patch_size': self.patch_size,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projection = Dense(units=projection_dim)\n",
    "        self.position_embedding = Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'num_patches': self.num_patches,\n",
    "            'projection_dim': self.projection_dim,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fanisspr/traffic-sign-recognition/blob/master/ipynb/CNN_with_STNs-DFG.ipynb\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomRotation(factor=0.02),\n",
    "        tf.keras.layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = Flatten()(representation)\n",
    "    representation = Dropout(0.5)(representation) \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = Dense(num_classes, activation='softmax')(features)\n",
    "    # Create the Keras model.\n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 224, 224, 3)  0          ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " patches_1 (Patches)            (None, None, 48)     0           ['data_augmentation[0][0]']      \n",
      "                                                                                                  \n",
      " patch_encoder_1 (PatchEncoder)  (None, 3136, 32)    101920      ['patches_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 3136, 32)    64          ['patch_encoder_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 3136, 32)    25184       ['layer_normalization_9[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 3136, 32)     0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'patch_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 3136, 32)    64          ['add_8[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 3136, 64)     2112        ['layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 3136, 64)     0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 3136, 32)     2080        ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 3136, 32)     0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 3136, 32)     0           ['dropout_16[0][0]',             \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 3136, 32)    64          ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 3136, 32)    25184       ['layer_normalization_11[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 3136, 32)     0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 3136, 32)    64          ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 3136, 64)     2112        ['layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 3136, 64)     0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 3136, 32)     2080        ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 3136, 32)     0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 3136, 32)     0           ['dropout_18[0][0]',             \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 3136, 32)    64          ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 3136, 32)    25184       ['layer_normalization_13[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 3136, 32)     0           ['multi_head_attention_6[0][0]', \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 3136, 32)    64          ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 3136, 64)     2112        ['layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 3136, 64)     0           ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 3136, 32)     2080        ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 3136, 32)     0           ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 3136, 32)     0           ['dropout_20[0][0]',             \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 3136, 32)    64          ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 3136, 32)    25184       ['layer_normalization_15[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 3136, 32)     0           ['multi_head_attention_7[0][0]', \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 3136, 32)    64          ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 3136, 64)     2112        ['layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 3136, 64)     0           ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 3136, 32)     2080        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 3136, 32)     0           ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 3136, 32)     0           ['dropout_22[0][0]',             \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 3136, 32)    64          ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 100352)       0           ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 100352)       0           ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 512)          51380736    ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 512)          0           ['dense_35[0][0]']               \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 256)          131328      ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 256)          0           ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 140)          35980       ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 51,768,044\n",
      "Trainable params: 51,768,044\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_vit_classifier()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 15:45:43.115093: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (mklcpu) ran out of memory trying to allocate 10.99GiB (rounded to 11801395200)requested by op model_9/multi_head_attention_5/einsum/Einsum\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-06-29 15:45:43.115187: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for mklcpu\n",
      "2023-06-29 15:45:43.115214: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115229: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115243: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115256: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115276: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 57, Chunks in use: 57. 265.0KiB allocated for chunks. 265.0KiB in use in bin. 257.5KiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115294: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 72, Chunks in use: 72. 632.5KiB allocated for chunks. 632.5KiB in use in bin. 590.1KiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115310: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 107, Chunks in use: 107. 2.58MiB allocated for chunks. 2.58MiB in use in bin. 2.54MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115327: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 9, Chunks in use: 8. 364.0KiB allocated for chunks. 318.8KiB in use in bin. 247.8KiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115344: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 9, Chunks in use: 9. 738.2KiB allocated for chunks. 738.2KiB in use in bin. 654.6KiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115359: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 22, Chunks in use: 22. 4.05MiB allocated for chunks. 4.05MiB in use in bin. 3.52MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115373: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 13, Chunks in use: 12. 4.17MiB allocated for chunks. 3.78MiB in use in bin. 3.30MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115390: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 26, Chunks in use: 25. 15.48MiB allocated for chunks. 14.80MiB in use in bin. 13.49MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115407: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 16, Chunks in use: 16. 18.38MiB allocated for chunks. 18.38MiB in use in bin. 15.27MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115423: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 23, Chunks in use: 23. 63.93MiB allocated for chunks. 63.93MiB in use in bin. 54.65MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115440: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 49, Chunks in use: 47. 311.36MiB allocated for chunks. 297.03MiB in use in bin. 279.69MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115457: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 21, Chunks in use: 21. 206.72MiB allocated for chunks. 206.72MiB in use in bin. 172.54MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115475: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 15, Chunks in use: 15. 314.76MiB allocated for chunks. 314.76MiB in use in bin. 296.94MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115492: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 8, Chunks in use: 8. 303.09MiB allocated for chunks. 303.09MiB in use in bin. 288.31MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115522: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 7, Chunks in use: 7. 803.91MiB allocated for chunks. 803.91MiB in use in bin. 803.91MiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115537: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 8, Chunks in use: 8. 1.54GiB allocated for chunks. 1.54GiB in use in bin. 1.26GiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115549: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 13, Chunks in use: 10. 59.26GiB allocated for chunks. 37.66GiB in use in bin. 37.29GiB client-requested in use in bin.\n",
      "2023-06-29 15:45:43.115560: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 10.99GiB was 256.00MiB, Chunk State: \n",
      "2023-06-29 15:45:43.115578: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 5.01GiB | Requested Size: 588.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 10.99GiB | Requested Size: 10.99GiB | in_use: 1 | bin_num: -1\n",
      "2023-06-29 15:45:43.115591: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 7.78GiB | Requested Size: 588.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 114.84MiB | Requested Size: 114.84MiB | in_use: 1 | bin_num: -1\n",
      "2023-06-29 15:45:43.115603: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 8.82GiB | Requested Size: 588.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 10.99GiB | Requested Size: 10.99GiB | in_use: 1 | bin_num: -1\n",
      "2023-06-29 15:45:43.115611: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 2097152\n",
      "2023-06-29 15:45:43.115623: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e41c0 of size 4096 next 82\n",
      "2023-06-29 15:45:43.115632: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e51c0 of size 4096 next 83\n",
      "2023-06-29 15:45:43.115641: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e61c0 of size 4096 next 87\n",
      "2023-06-29 15:45:43.115649: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e71c0 of size 4096 next 88\n",
      "2023-06-29 15:45:43.115658: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59e81c0 of size 12288 next 91\n",
      "2023-06-29 15:45:43.115667: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59eb1c0 of size 8192 next 92\n",
      "2023-06-29 15:45:43.115675: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59ed1c0 of size 8192 next 95\n",
      "2023-06-29 15:45:43.115684: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59ef1c0 of size 8192 next 96\n",
      "2023-06-29 15:45:43.115692: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59f11c0 of size 8192 next 97\n",
      "2023-06-29 15:45:43.115701: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59f31c0 of size 13824 next 2\n",
      "2023-06-29 15:45:43.115710: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59f67c0 of size 16384 next 125\n",
      "2023-06-29 15:45:43.115719: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fa7c0 of size 4096 next 74\n",
      "2023-06-29 15:45:43.115727: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fb7c0 of size 4096 next 69\n",
      "2023-06-29 15:45:43.115736: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fc7c0 of size 4096 next 76\n",
      "2023-06-29 15:45:43.115744: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fd7c0 of size 4096 next 80\n",
      "2023-06-29 15:45:43.115753: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59fe7c0 of size 4096 next 81\n",
      "2023-06-29 15:45:43.115762: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 59ff7c0 of size 4864 next 41\n",
      "2023-06-29 15:45:43.115771: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a00ac0 of size 4096 next 44\n",
      "2023-06-29 15:45:43.115779: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a01ac0 of size 4096 next 48\n",
      "2023-06-29 15:45:43.115788: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a02ac0 of size 4096 next 49\n",
      "2023-06-29 15:45:43.115797: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a03ac0 of size 4096 next 45\n",
      "2023-06-29 15:45:43.115813: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a04ac0 of size 4096 next 51\n",
      "2023-06-29 15:45:43.115822: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a05ac0 of size 4096 next 59\n",
      "2023-06-29 15:45:43.115831: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a06ac0 of size 4096 next 6\n",
      "2023-06-29 15:45:43.115840: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a07ac0 of size 4096 next 7\n",
      "2023-06-29 15:45:43.115848: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a08ac0 of size 4096 next 36\n",
      "2023-06-29 15:45:43.115856: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a09ac0 of size 4096 next 39\n",
      "2023-06-29 15:45:43.115864: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0aac0 of size 4096 next 3\n",
      "2023-06-29 15:45:43.115873: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0bac0 of size 4096 next 64\n",
      "2023-06-29 15:45:43.115881: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0cac0 of size 4096 next 66\n",
      "2023-06-29 15:45:43.115889: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0dac0 of size 4096 next 67\n",
      "2023-06-29 15:45:43.115897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0eac0 of size 4096 next 1\n",
      "2023-06-29 15:45:43.115906: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a0fac0 of size 65536 next 4\n",
      "2023-06-29 15:45:43.115914: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a1fac0 of size 4096 next 57\n",
      "2023-06-29 15:45:43.115923: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a20ac0 of size 143360 next 99\n",
      "2023-06-29 15:45:43.115931: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a43ac0 of size 147456 next 8\n",
      "2023-06-29 15:45:43.115940: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a67ac0 of size 65536 next 126\n",
      "2023-06-29 15:45:43.115948: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a77ac0 of size 81920 next 9\n",
      "2023-06-29 15:45:43.115957: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a8bac0 of size 65536 next 12\n",
      "2023-06-29 15:45:43.115965: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5a9bac0 of size 229376 next 13\n",
      "2023-06-29 15:45:43.115974: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5ad3ac0 of size 147456 next 14\n",
      "2023-06-29 15:45:43.115982: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5af7ac0 of size 8192 next 17\n",
      "2023-06-29 15:45:43.115991: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5af9ac0 of size 8192 next 104\n",
      "2023-06-29 15:45:43.115999: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5afbac0 of size 8192 next 111\n",
      "2023-06-29 15:45:43.116007: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5afdac0 of size 8192 next 107\n",
      "2023-06-29 15:45:43.116015: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5affac0 of size 8192 next 112\n",
      "2023-06-29 15:45:43.116023: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b01ac0 of size 8192 next 113\n",
      "2023-06-29 15:45:43.116032: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b03ac0 of size 8192 next 109\n",
      "2023-06-29 15:45:43.116040: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b05ac0 of size 8192 next 119\n",
      "2023-06-29 15:45:43.116048: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b07ac0 of size 8192 next 101\n",
      "2023-06-29 15:45:43.116057: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b09ac0 of size 8192 next 115\n",
      "2023-06-29 15:45:43.116065: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b0bac0 of size 24576 next 365\n",
      "2023-06-29 15:45:43.116074: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b11ac0 of size 40960 next 15\n",
      "2023-06-29 15:45:43.116083: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b1bac0 of size 147456 next 16\n",
      "2023-06-29 15:45:43.116091: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b3fac0 of size 262144 next 26\n",
      "2023-06-29 15:45:43.116100: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5b7fac0 of size 411392 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116108: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 4194304\n",
      "2023-06-29 15:45:43.116116: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5be4200 of size 8192 next 117\n",
      "2023-06-29 15:45:43.116125: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5be6200 of size 8192 next 120\n",
      "2023-06-29 15:45:43.116133: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5be8200 of size 8192 next 121\n",
      "2023-06-29 15:45:43.116141: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bea200 of size 8192 next 122\n",
      "2023-06-29 15:45:43.116149: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bec200 of size 8192 next 123\n",
      "2023-06-29 15:45:43.116158: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bee200 of size 37632 next 124\n",
      "2023-06-29 15:45:43.116166: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bf7500 of size 6912 next 40\n",
      "2023-06-29 15:45:43.116174: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bf9000 of size 8192 next 348\n",
      "2023-06-29 15:45:43.116183: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bfb000 of size 8192 next 147\n",
      "2023-06-29 15:45:43.116191: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bfd000 of size 6144 next 38\n",
      "2023-06-29 15:45:43.116200: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5bfe800 of size 6400 next 114\n",
      "2023-06-29 15:45:43.116209: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5c00100 of size 35840 next 106\n",
      "2023-06-29 15:45:43.116217: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5c08d00 of size 111872 next 22\n",
      "2023-06-29 15:45:43.116225: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5c24200 of size 262144 next 19\n",
      "2023-06-29 15:45:43.116234: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5c64200 of size 1966080 next 27\n",
      "2023-06-29 15:45:43.116243: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5e44200 of size 262144 next 25\n",
      "2023-06-29 15:45:43.116252: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5e84200 of size 655360 next 28\n",
      "2023-06-29 15:45:43.116261: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 5f24200 of size 786432 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116270: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 8388608\n",
      "2023-06-29 15:45:43.116278: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6005780 of size 262144 next 32\n",
      "2023-06-29 15:45:43.116287: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6045780 of size 1179648 next 35\n",
      "2023-06-29 15:45:43.116297: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6165780 of size 327680 next 33\n",
      "2023-06-29 15:45:43.116306: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 61b5780 of size 589824 next 34\n",
      "2023-06-29 15:45:43.116314: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6245780 of size 524288 next 42\n",
      "2023-06-29 15:45:43.116322: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 62c5780 of size 2621440 next 46\n",
      "2023-06-29 15:45:43.116332: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6545780 of size 1048576 next 47\n",
      "2023-06-29 15:45:43.116341: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6645780 of size 1835008 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116349: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 16777216\n",
      "2023-06-29 15:45:43.116358: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 686ba40 of size 224000 next 232\n",
      "2023-06-29 15:45:43.116366: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68a2540 of size 240128 next 152\n",
      "2023-06-29 15:45:43.116375: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68dcf40 of size 24576 next 352\n",
      "2023-06-29 15:45:43.116383: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68e2f40 of size 24576 next 324\n",
      "2023-06-29 15:45:43.116391: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68e8f40 of size 24576 next 294\n",
      "2023-06-29 15:45:43.116400: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68eef40 of size 24576 next 297\n",
      "2023-06-29 15:45:43.116408: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68f4f40 of size 24576 next 277\n",
      "2023-06-29 15:45:43.116416: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 68faf40 of size 24576 next 296\n",
      "2023-06-29 15:45:43.116424: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6900f40 of size 24576 next 176\n",
      "2023-06-29 15:45:43.116432: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6906f40 of size 24576 next 376\n",
      "2023-06-29 15:45:43.116440: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 690cf40 of size 24576 next 23\n",
      "2023-06-29 15:45:43.116449: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6912f40 of size 24576 next 298\n",
      "2023-06-29 15:45:43.116457: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6918f40 of size 24576 next 281\n",
      "2023-06-29 15:45:43.116465: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 691ef40 of size 24576 next 379\n",
      "2023-06-29 15:45:43.116473: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6924f40 of size 24576 next 312\n",
      "2023-06-29 15:45:43.116481: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 692af40 of size 40960 next 361\n",
      "2023-06-29 15:45:43.116490: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6934f40 of size 224000 next 100\n",
      "2023-06-29 15:45:43.116498: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 696ba40 of size 524288 next 103\n",
      "2023-06-29 15:45:43.116507: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 69eba40 of size 884736 next 89\n",
      "2023-06-29 15:45:43.116515: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ac3a40 of size 81920 next 181\n",
      "2023-06-29 15:45:43.116524: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ad7a40 of size 4608 next 183\n",
      "2023-06-29 15:45:43.116532: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ad8c40 of size 4608 next 187\n",
      "2023-06-29 15:45:43.116540: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ad9e40 of size 4608 next 188\n",
      "2023-06-29 15:45:43.116549: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6adb040 of size 6656 next 168\n",
      "2023-06-29 15:45:43.116558: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6adca40 of size 24576 next 334\n",
      "2023-06-29 15:45:43.116566: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ae2a40 of size 24576 next 291\n",
      "2023-06-29 15:45:43.116574: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6ae8a40 of size 8192 next 370\n",
      "2023-06-29 15:45:43.116583: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6aeaa40 of size 8192 next 309\n",
      "2023-06-29 15:45:43.116591: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6aeca40 of size 8192 next 171\n",
      "2023-06-29 15:45:43.116599: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6aeea40 of size 4096 next 220\n",
      "2023-06-29 15:45:43.116608: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6aefa40 of size 4096 next 223\n",
      "2023-06-29 15:45:43.116616: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6af0a40 of size 4096 next 224\n",
      "2023-06-29 15:45:43.116624: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6af1a40 of size 6144 next 198\n",
      "2023-06-29 15:45:43.116632: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6af3240 of size 18432 next 202\n",
      "2023-06-29 15:45:43.116642: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6af7a40 of size 28672 next 185\n",
      "2023-06-29 15:45:43.116651: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6afea40 of size 32768 next 186\n",
      "2023-06-29 15:45:43.116661: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b06a40 of size 9216 next 196\n",
      "2023-06-29 15:45:43.116670: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b08e40 of size 9216 next 197\n",
      "2023-06-29 15:45:43.116679: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b0b240 of size 9216 next 199\n",
      "2023-06-29 15:45:43.116687: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b0d640 of size 9216 next 200\n",
      "2023-06-29 15:45:43.116696: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b0fa40 of size 4096 next 216\n",
      "2023-06-29 15:45:43.116705: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b10a40 of size 4096 next 218\n",
      "2023-06-29 15:45:43.116714: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b11a40 of size 4096 next 219\n",
      "2023-06-29 15:45:43.116722: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b12a40 of size 4096 next 144\n",
      "2023-06-29 15:45:43.116732: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b13a40 of size 223232 next 162\n",
      "2023-06-29 15:45:43.116741: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b4a240 of size 8192 next 184\n",
      "2023-06-29 15:45:43.116750: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b4c240 of size 4608 next 191\n",
      "2023-06-29 15:45:43.116759: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b4d440 of size 4608 next 192\n",
      "2023-06-29 15:45:43.116768: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b4e640 of size 21504 next 146\n",
      "2023-06-29 15:45:43.116778: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6b53a40 of size 589824 next 105\n",
      "2023-06-29 15:45:43.116787: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6be3a40 of size 2916352 next 60\n",
      "2023-06-29 15:45:43.116798: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 6eaba40 of size 2621440 next 54\n",
      "2023-06-29 15:45:43.116807: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 712ba40 of size 2359296 next 55\n",
      "2023-06-29 15:45:43.116817: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 736ba40 of size 1048576 next 62\n",
      "2023-06-29 15:45:43.116827: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 746ba40 of size 1310720 next 61\n",
      "2023-06-29 15:45:43.116837: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 75aba40 of size 2883584 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116846: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 33554432\n",
      "2023-06-29 15:45:43.116856: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 79e0640 of size 1048576 next 71\n",
      "2023-06-29 15:45:43.116865: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7ae0640 of size 3670016 next 70\n",
      "2023-06-29 15:45:43.116875: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7e60640 of size 5767168 next 86\n",
      "2023-06-29 15:45:43.116885: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 83e0640 of size 1310720 next 84\n",
      "2023-06-29 15:45:43.116895: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 8520640 of size 2359296 next 85\n",
      "2023-06-29 15:45:43.116905: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 8760640 of size 2097152 next 98\n",
      "2023-06-29 15:45:43.116914: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 8960640 of size 4718592 next 79\n",
      "2023-06-29 15:45:43.116924: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 8de0640 of size 12582912 next 18446744073709551615\n",
      "2023-06-29 15:45:43.116933: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 67108864\n",
      "2023-06-29 15:45:43.116942: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 9b3e380 of size 9437184 next 72\n",
      "2023-06-29 15:45:43.116952: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at a43e380 of size 9437184 next 63\n",
      "2023-06-29 15:45:43.116962: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at ad3e380 of size 9437184 next 139\n",
      "2023-06-29 15:45:43.116972: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at b63e380 of size 3145728 next 116\n",
      "2023-06-29 15:45:43.116982: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at b93e380 of size 5242880 next 108\n",
      "2023-06-29 15:45:43.116992: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at be3e380 of size 9437184 next 110\n",
      "2023-06-29 15:45:43.117001: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at c73e380 of size 9437184 next 90\n",
      "2023-06-29 15:45:43.117011: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d03e380 of size 100352 next 31\n",
      "2023-06-29 15:45:43.117020: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d056b80 of size 143360 next 77\n",
      "2023-06-29 15:45:43.117029: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d079b80 of size 935936 next 5\n",
      "2023-06-29 15:45:43.117038: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d15e380 of size 1179648 next 78\n",
      "2023-06-29 15:45:43.117048: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d27e380 of size 2359296 next 75\n",
      "2023-06-29 15:45:43.117057: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d4be380 of size 524288 next 94\n",
      "2023-06-29 15:45:43.117065: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d53e380 of size 524288 next 102\n",
      "2023-06-29 15:45:43.117074: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d5be380 of size 294912 next 153\n",
      "2023-06-29 15:45:43.117083: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d606380 of size 2195456 next 73\n",
      "2023-06-29 15:45:43.117092: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at d81e380 of size 3276800 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117100: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 33076834304\n",
      "2023-06-29 15:45:43.117109: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6964772040 of size 11801395200 next 452\n",
      "2023-06-29 15:45:43.117118: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6c23e22040 of size 11801395200 next 449\n",
      "2023-06-29 15:45:43.117127: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f6ee34d2040 of size 9474043904 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117135: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 17179869184\n",
      "2023-06-29 15:45:43.117143: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7137ffb040 of size 11801395200 next 468\n",
      "2023-06-29 15:45:43.117152: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f73f76ab040 of size 5378473984 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117160: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 8589934592\n",
      "2023-06-29 15:45:43.117169: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7547ffd040 of size 120422400 next 472\n",
      "2023-06-29 15:45:43.117178: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f754f2d5040 of size 120422400 next 469\n",
      "2023-06-29 15:45:43.117186: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f75565ad040 of size 8349089792 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117194: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 4294967296\n",
      "2023-06-29 15:45:43.117203: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f774ffff040 of size 2950348800 next 453\n",
      "2023-06-29 15:45:43.117212: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f77ffdab040 of size 20070400 next 441\n",
      "2023-06-29 15:45:43.117221: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78010cf040 of size 20070400 next 454\n",
      "2023-06-29 15:45:43.117230: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78023f3040 of size 20070400 next 462\n",
      "2023-06-29 15:45:43.117238: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7803717040 of size 40140800 next 463\n",
      "2023-06-29 15:45:43.117247: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7805d5f040 of size 40140800 next 461\n",
      "2023-06-29 15:45:43.117256: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78083a7040 of size 40140800 next 460\n",
      "2023-06-29 15:45:43.117264: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f780a9ef040 of size 40140800 next 457\n",
      "2023-06-29 15:45:43.117272: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f780d037040 of size 20070400 next 464\n",
      "2023-06-29 15:45:43.117281: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f780e35b040 of size 20070400 next 456\n",
      "2023-06-29 15:45:43.117289: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f780f67f040 of size 10035200 next 451\n",
      "2023-06-29 15:45:43.117299: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7810011040 of size 20070400 next 467\n",
      "2023-06-29 15:45:43.117307: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7811335040 of size 20070400 next 466\n",
      "2023-06-29 15:45:43.117315: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7812659040 of size 26869760 next 363\n",
      "2023-06-29 15:45:43.117324: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7813ff9040 of size 4199936 next 336\n",
      "2023-06-29 15:45:43.117334: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78143fa640 of size 20160000 next 24\n",
      "2023-06-29 15:45:43.117343: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7815734440 of size 296750080 next 305\n",
      "2023-06-29 15:45:43.117352: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7827235040 of size 5600000 next 356\n",
      "2023-06-29 15:45:43.117361: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f782778c340 of size 205520896 next 368\n",
      "2023-06-29 15:45:43.117370: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7833b8c340 of size 205520896 next 272\n",
      "2023-06-29 15:45:43.117379: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f783ff8c340 of size 7526400 next 438\n",
      "2023-06-29 15:45:43.117388: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78406b9b40 of size 7526400 next 443\n",
      "2023-06-29 15:45:43.117396: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7840de7340 of size 30105600 next 450\n",
      "2023-06-29 15:45:43.117405: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7842a9d340 of size 20070400 next 444\n",
      "2023-06-29 15:45:43.117413: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7843dc1340 of size 20070400 next 447\n",
      "2023-06-29 15:45:43.117422: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78450e5340 of size 183606528 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117430: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 2147483648\n",
      "2023-06-29 15:45:43.117439: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a3fff040 of size 7526400 next 432\n",
      "2023-06-29 15:45:43.117447: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a472c840 of size 7526400 next 414\n",
      "2023-06-29 15:45:43.117456: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a4e5a040 of size 7526400 next 424\n",
      "2023-06-29 15:45:43.117464: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a5587840 of size 7526400 next 425\n",
      "2023-06-29 15:45:43.117473: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a5cb5040 of size 7526400 next 407\n",
      "2023-06-29 15:45:43.117481: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a63e2840 of size 7526400 next 413\n",
      "2023-06-29 15:45:43.117490: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a6b10040 of size 7526400 next 415\n",
      "2023-06-29 15:45:43.117498: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a723d840 of size 7526400 next 409\n",
      "2023-06-29 15:45:43.117506: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a796b040 of size 7526400 next 403\n",
      "2023-06-29 15:45:43.117515: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a8098840 of size 7526400 next 406\n",
      "2023-06-29 15:45:43.117523: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a87c6040 of size 7526400 next 431\n",
      "2023-06-29 15:45:43.117531: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78a8ef3840 of size 35569408 next 271\n",
      "2023-06-29 15:45:43.117540: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ab0df740 of size 4860416 next 269\n",
      "2023-06-29 15:45:43.117549: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ab582140 of size 3561728 next 157\n",
      "2023-06-29 15:45:43.117558: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ab8e7a40 of size 226048 next 306\n",
      "2023-06-29 15:45:43.117568: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ab91ed40 of size 263617536 next 292\n",
      "2023-06-29 15:45:43.117576: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78bb486940 of size 240128 next 320\n",
      "2023-06-29 15:45:43.117585: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78bb4c1340 of size 120422400 next 446\n",
      "2023-06-29 15:45:43.117593: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78c2799340 of size 120422400 next 445\n",
      "2023-06-29 15:45:43.117601: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78c9a71340 of size 120422400 next 455\n",
      "2023-06-29 15:45:43.117610: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78d0d49340 of size 20070400 next 474\n",
      "2023-06-29 15:45:43.117618: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78d206d340 of size 120422400 next 473\n",
      "2023-06-29 15:45:43.117627: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78d9345340 of size 120422400 next 475\n",
      "2023-06-29 15:45:43.117635: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78e061d340 of size 226612736 next 284\n",
      "2023-06-29 15:45:43.117644: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ede3a940 of size 5779712 next 367\n",
      "2023-06-29 15:45:43.117653: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ee3bda40 of size 7526400 next 392\n",
      "2023-06-29 15:45:43.117661: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78eeaeb240 of size 7526400 next 394\n",
      "2023-06-29 15:45:43.117670: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ef218a40 of size 7526400 next 396\n",
      "2023-06-29 15:45:43.117678: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78ef946240 of size 7526400 next 397\n",
      "2023-06-29 15:45:43.117686: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f0073a40 of size 7526400 next 399\n",
      "2023-06-29 15:45:43.117694: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f07a1240 of size 7526400 next 401\n",
      "2023-06-29 15:45:43.117703: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f0ecea40 of size 7526400 next 433\n",
      "2023-06-29 15:45:43.117711: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f15fc240 of size 7526400 next 437\n",
      "2023-06-29 15:45:43.117719: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f1d29a40 of size 11468288 next 131\n",
      "2023-06-29 15:45:43.117728: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78f2819840 of size 196000000 next 179\n",
      "2023-06-29 15:45:43.117737: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f78fe305140 of size 309080576 next 165\n",
      "2023-06-29 15:45:43.117747: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79109c8340 of size 325283072 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117755: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 1073741824\n",
      "2023-06-29 15:45:43.117763: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79a3ffe040 of size 439040000 next 251\n",
      "2023-06-29 15:45:43.117774: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79be2b1840 of size 439040000 next 254\n",
      "2023-06-29 15:45:43.117782: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79d8565040 of size 195661824 next 18446744073709551615\n",
      "2023-06-29 15:45:43.117790: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 536870912\n",
      "2023-06-29 15:45:43.117799: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79e3fff040 of size 40560128 next 137\n",
      "2023-06-29 15:45:43.117808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79e66ad640 of size 40560128 next 150\n",
      "2023-06-29 15:45:43.117817: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79e8d5bc40 of size 40560128 next 43\n",
      "2023-06-29 15:45:43.117825: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79eb40a240 of size 3000064 next 145\n",
      "2023-06-29 15:45:43.117834: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79eb6e6940 of size 5600000 next 143\n",
      "2023-06-29 15:45:43.117842: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ebc3dc40 of size 3000064 next 250\n",
      "2023-06-29 15:45:43.117851: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ebf1a340 of size 8559872 next 133\n",
      "2023-06-29 15:45:43.117859: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ec744040 of size 7526400 next 364\n",
      "2023-06-29 15:45:43.117868: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ece71840 of size 7526400 next 330\n",
      "2023-06-29 15:45:43.117877: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ed59f040 of size 11698176 next 233\n",
      "2023-06-29 15:45:43.117885: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ee0c7040 of size 5600000 next 243\n",
      "2023-06-29 15:45:43.117894: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79ee61e340 of size 174191360 next 248\n",
      "2023-06-29 15:45:43.117903: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79f8c3d640 of size 5600000 next 253\n",
      "2023-06-29 15:45:43.117911: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79f9194940 of size 7526400 next 341\n",
      "2023-06-29 15:45:43.117920: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79f98c2140 of size 7526400 next 384\n",
      "2023-06-29 15:45:43.117928: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79f9fef940 of size 7526400 next 385\n",
      "2023-06-29 15:45:43.117936: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fa71d140 of size 7526400 next 386\n",
      "2023-06-29 15:45:43.117944: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fae4a940 of size 7526400 next 387\n",
      "2023-06-29 15:45:43.117953: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fb578140 of size 13798400 next 355\n",
      "2023-06-29 15:45:43.117962: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fc2a0d40 of size 4014080 next 366\n",
      "2023-06-29 15:45:43.117971: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f79fc674d40 of size 7526400 next 375\n",
      "2023-06-29 15:45:43.117980: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fcda2540 of size 12293120 next 275\n",
      "2023-06-29 15:45:43.117989: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fd95b940 of size 6250240 next 180\n",
      "2023-06-29 15:45:43.117998: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fdf51840 of size 11200000 next 155\n",
      "2023-06-29 15:45:43.118007: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fe9ffe40 of size 524288 next 369\n",
      "2023-06-29 15:45:43.118016: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fea7fe40 of size 627200 next 335\n",
      "2023-06-29 15:45:43.118026: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feb19040 of size 401408 next 378\n",
      "2023-06-29 15:45:43.118035: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feb7b040 of size 1048576 next 30\n",
      "2023-06-29 15:45:43.118043: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fec7b040 of size 524288 next 260\n",
      "2023-06-29 15:45:43.118052: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fecfb040 of size 24576 next 340\n",
      "2023-06-29 15:45:43.118060: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed01040 of size 24576 next 265\n",
      "2023-06-29 15:45:43.118069: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed07040 of size 24576 next 351\n",
      "2023-06-29 15:45:43.118077: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed0d040 of size 24576 next 170\n",
      "2023-06-29 15:45:43.118085: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed13040 of size 24576 next 174\n",
      "2023-06-29 15:45:43.118093: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed19040 of size 24576 next 175\n",
      "2023-06-29 15:45:43.118102: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed1f040 of size 8192 next 268\n",
      "2023-06-29 15:45:43.118110: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed21040 of size 8192 next 317\n",
      "2023-06-29 15:45:43.118118: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed23040 of size 8192 next 329\n",
      "2023-06-29 15:45:43.118127: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed25040 of size 8192 next 266\n",
      "2023-06-29 15:45:43.118135: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fed27040 of size 524288 next 353\n",
      "2023-06-29 15:45:43.118143: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feda7040 of size 524288 next 283\n",
      "2023-06-29 15:45:43.118152: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee27040 of size 143360 next 325\n",
      "2023-06-29 15:45:43.118160: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee4a040 of size 143360 next 350\n",
      "2023-06-29 15:45:43.118168: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee6d040 of size 12544 next 314\n",
      "2023-06-29 15:45:43.118178: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee70140 of size 8960 next 373\n",
      "2023-06-29 15:45:43.118187: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee72440 of size 8960 next 354\n",
      "2023-06-29 15:45:43.118196: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee74740 of size 8960 next 333\n",
      "2023-06-29 15:45:43.118205: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee76a40 of size 8960 next 435\n",
      "2023-06-29 15:45:43.118213: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee78d40 of size 8960 next 418\n",
      "2023-06-29 15:45:43.118221: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee7b040 of size 10240 next 148\n",
      "2023-06-29 15:45:43.118230: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee7d840 of size 28160 next 374\n",
      "2023-06-29 15:45:43.118240: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee84640 of size 28160 next 258\n",
      "2023-06-29 15:45:43.118248: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee8b440 of size 28160 next 270\n",
      "2023-06-29 15:45:43.118257: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee92240 of size 28160 next 380\n",
      "2023-06-29 15:45:43.118265: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee99040 of size 28160 next 264\n",
      "2023-06-29 15:45:43.118274: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fee9fe40 of size 28160 next 382\n",
      "2023-06-29 15:45:43.118283: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feea6c40 of size 28160 next 339\n",
      "2023-06-29 15:45:43.118291: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feeada40 of size 28160 next 388\n",
      "2023-06-29 15:45:43.118300: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feeb4840 of size 28160 next 389\n",
      "2023-06-29 15:45:43.118308: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feebb640 of size 28160 next 390\n",
      "2023-06-29 15:45:43.118317: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feec2440 of size 28160 next 391\n",
      "2023-06-29 15:45:43.118325: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feec9240 of size 28160 next 393\n",
      "2023-06-29 15:45:43.118333: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feed0040 of size 28160 next 395\n",
      "2023-06-29 15:45:43.118342: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feed6e40 of size 28160 next 398\n",
      "2023-06-29 15:45:43.118350: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feeddc40 of size 28160 next 429\n",
      "2023-06-29 15:45:43.118359: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feee4a40 of size 28160 next 421\n",
      "2023-06-29 15:45:43.118367: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feeeb840 of size 28160 next 428\n",
      "2023-06-29 15:45:43.118376: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feef2640 of size 28160 next 426\n",
      "2023-06-29 15:45:43.118384: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79feef9440 of size 28160 next 420\n",
      "2023-06-29 15:45:43.118393: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef00240 of size 28160 next 423\n",
      "2023-06-29 15:45:43.118401: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef07040 of size 28160 next 422\n",
      "2023-06-29 15:45:43.118410: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef0de40 of size 28160 next 408\n",
      "2023-06-29 15:45:43.118418: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef14c40 of size 28160 next 411\n",
      "2023-06-29 15:45:43.118426: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef1ba40 of size 28160 next 416\n",
      "2023-06-29 15:45:43.118435: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef22840 of size 28160 next 405\n",
      "2023-06-29 15:45:43.118443: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef29640 of size 28160 next 412\n",
      "2023-06-29 15:45:43.118451: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef30440 of size 28160 next 430\n",
      "2023-06-29 15:45:43.118460: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef37240 of size 28160 next 427\n",
      "2023-06-29 15:45:43.118468: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef3e040 of size 28160 next 440\n",
      "2023-06-29 15:45:43.118477: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef44e40 of size 28160 next 439\n",
      "2023-06-29 15:45:43.118485: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f79fef4bc40 of size 46336 next 128\n",
      "2023-06-29 15:45:43.118493: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f79fef57140 of size 22108160 next 288\n",
      "2023-06-29 15:45:43.118502: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a0046c940 of size 30105600 next 285\n",
      "2023-06-29 15:45:43.118511: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02122940 of size 28160 next 400\n",
      "2023-06-29 15:45:43.118519: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02129740 of size 28160 next 402\n",
      "2023-06-29 15:45:43.118528: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02130540 of size 627200 next 436\n",
      "2023-06-29 15:45:43.118536: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f7a021c9740 of size 401408 next 419\n",
      "2023-06-29 15:45:43.118545: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a0222b740 of size 8960 next 434\n",
      "2023-06-29 15:45:43.118554: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a0222da40 of size 28160 next 417\n",
      "2023-06-29 15:45:43.118562: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02234840 of size 627200 next 442\n",
      "2023-06-29 15:45:43.118570: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a022cda40 of size 627200 next 448\n",
      "2023-06-29 15:45:43.118579: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02366c40 of size 627200 next 459\n",
      "2023-06-29 15:45:43.118587: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a023ffe40 of size 627200 next 458\n",
      "2023-06-29 15:45:43.118595: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02499040 of size 627200 next 471\n",
      "2023-06-29 15:45:43.118604: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02532240 of size 627200 next 470\n",
      "2023-06-29 15:45:43.118612: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f7a025cb440 of size 714752 next 241\n",
      "2023-06-29 15:45:43.118620: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a02679c40 of size 5600000 next 142\n",
      "2023-06-29 15:45:43.118629: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f7a02bd0f40 of size 7500288 next 132\n",
      "2023-06-29 15:45:43.118637: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a032f8140 of size 3878912 next 301\n",
      "2023-06-29 15:45:43.118646: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7a036ab140 of size 9780992 next 18446744073709551615\n",
      "2023-06-29 15:45:43.118655: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 268435456\n",
      "2023-06-29 15:45:43.118664: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7af3ffe040 of size 268435456 next 18446744073709551615\n",
      "2023-06-29 15:45:43.118672: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 134217728\n",
      "2023-06-29 15:45:43.118680: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b03fff040 of size 9437184 next 53\n",
      "2023-06-29 15:45:43.118690: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b048ff040 of size 9437184 next 58\n",
      "2023-06-29 15:45:43.118698: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b051ff040 of size 9437184 next 56\n",
      "2023-06-29 15:45:43.118707: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b05aff040 of size 9437184 next 52\n",
      "2023-06-29 15:45:43.118715: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b063ff040 of size 4718592 next 65\n",
      "2023-06-29 15:45:43.118724: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0687f040 of size 9437184 next 130\n",
      "2023-06-29 15:45:43.118732: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0717f040 of size 18432 next 204\n",
      "2023-06-29 15:45:43.118740: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07183840 of size 18432 next 205\n",
      "2023-06-29 15:45:43.118749: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07188040 of size 28672 next 189\n",
      "2023-06-29 15:45:43.118757: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0718f040 of size 65536 next 190\n",
      "2023-06-29 15:45:43.118766: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0719f040 of size 18432 next 207\n",
      "2023-06-29 15:45:43.118787: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071a3840 of size 18432 next 209\n",
      "2023-06-29 15:45:43.118796: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071a8040 of size 18432 next 210\n",
      "2023-06-29 15:45:43.118804: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071ac840 of size 18432 next 212\n",
      "2023-06-29 15:45:43.118812: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071b1040 of size 18432 next 213\n",
      "2023-06-29 15:45:43.118820: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071b5840 of size 18432 next 215\n",
      "2023-06-29 15:45:43.118829: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071ba040 of size 4096 next 225\n",
      "2023-06-29 15:45:43.118837: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071bb040 of size 4096 next 228\n",
      "2023-06-29 15:45:43.118846: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071bc040 of size 4096 next 229\n",
      "2023-06-29 15:45:43.118854: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071bd040 of size 4096 next 230\n",
      "2023-06-29 15:45:43.118862: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071be040 of size 4096 next 127\n",
      "2023-06-29 15:45:43.118871: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071bf040 of size 262144 next 134\n",
      "2023-06-29 15:45:43.118879: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b071ff040 of size 36864 next 222\n",
      "2023-06-29 15:45:43.118888: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07208040 of size 24576 next 274\n",
      "2023-06-29 15:45:43.118897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0720e040 of size 24576 next 359\n",
      "2023-06-29 15:45:43.118905: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07214040 of size 24576 next 261\n",
      "2023-06-29 15:45:43.118913: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0721a040 of size 24576 next 319\n",
      "2023-06-29 15:45:43.118922: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07220040 of size 8960 next 332\n",
      "2023-06-29 15:45:43.118931: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07222340 of size 15616 next 159\n",
      "2023-06-29 15:45:43.118940: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07226040 of size 24576 next 338\n",
      "2023-06-29 15:45:43.118949: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0722c040 of size 24576 next 311\n",
      "2023-06-29 15:45:43.118957: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07232040 of size 8960 next 372\n",
      "2023-06-29 15:45:43.118966: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07234340 of size 16128 next 259\n",
      "2023-06-29 15:45:43.118975: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07238240 of size 7424 next 322\n",
      "2023-06-29 15:45:43.118984: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07239f40 of size 6144 next 326\n",
      "2023-06-29 15:45:43.118992: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0723b740 of size 6144 next 154\n",
      "2023-06-29 15:45:43.119000: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0723cf40 of size 8448 next 195\n",
      "2023-06-29 15:45:43.119010: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0723f040 of size 417792 next 136\n",
      "2023-06-29 15:45:43.119019: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b072a5040 of size 602112 next 20\n",
      "2023-06-29 15:45:43.119028: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07338040 of size 4096 next 231\n",
      "2023-06-29 15:45:43.119036: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07339040 of size 6144 next 135\n",
      "2023-06-29 15:45:43.119045: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0733a840 of size 11776 next 234\n",
      "2023-06-29 15:45:43.119054: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0733d640 of size 7424 next 21\n",
      "2023-06-29 15:45:43.119062: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0733f340 of size 7424 next 244\n",
      "2023-06-29 15:45:43.119071: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07341040 of size 12800 next 280\n",
      "2023-06-29 15:45:43.119079: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07344240 of size 8192 next 358\n",
      "2023-06-29 15:45:43.119088: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07346240 of size 15872 next 221\n",
      "2023-06-29 15:45:43.119097: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0734a040 of size 57344 next 193\n",
      "2023-06-29 15:45:43.119106: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07358040 of size 131072 next 194\n",
      "2023-06-29 15:45:43.119115: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07378040 of size 117760 next 235\n",
      "2023-06-29 15:45:43.119124: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07394c40 of size 401408 next 287\n",
      "2023-06-29 15:45:43.119133: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b073f6c40 of size 401408 next 346\n",
      "2023-06-29 15:45:43.119141: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07458c40 of size 24576 next 279\n",
      "2023-06-29 15:45:43.119150: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0745ec40 of size 24576 next 377\n",
      "2023-06-29 15:45:43.119158: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07464c40 of size 24576 next 371\n",
      "2023-06-29 15:45:43.119166: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0746ac40 of size 24576 next 357\n",
      "2023-06-29 15:45:43.119175: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07470c40 of size 24576 next 273\n",
      "2023-06-29 15:45:43.119183: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07476c40 of size 8192 next 267\n",
      "2023-06-29 15:45:43.119191: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07478c40 of size 8192 next 303\n",
      "2023-06-29 15:45:43.119200: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0747ac40 of size 8192 next 173\n",
      "2023-06-29 15:45:43.119208: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0747cc40 of size 8192 next 299\n",
      "2023-06-29 15:45:43.119217: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0747ec40 of size 24576 next 158\n",
      "2023-06-29 15:45:43.119225: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07484c40 of size 24576 next 349\n",
      "2023-06-29 15:45:43.119233: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0748ac40 of size 24576 next 321\n",
      "2023-06-29 15:45:43.119241: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07490c40 of size 24576 next 141\n",
      "2023-06-29 15:45:43.119250: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07496c40 of size 24576 next 310\n",
      "2023-06-29 15:45:43.119258: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0749cc40 of size 24576 next 300\n",
      "2023-06-29 15:45:43.119266: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074a2c40 of size 24576 next 342\n",
      "2023-06-29 15:45:43.119275: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074a8c40 of size 24576 next 177\n",
      "2023-06-29 15:45:43.119283: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074aec40 of size 8192 next 276\n",
      "2023-06-29 15:45:43.119291: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074b0c40 of size 8192 next 337\n",
      "2023-06-29 15:45:43.119300: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074b2c40 of size 8192 next 347\n",
      "2023-06-29 15:45:43.119308: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074b4c40 of size 8192 next 362\n",
      "2023-06-29 15:45:43.119316: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074b6c40 of size 24576 next 257\n",
      "2023-06-29 15:45:43.119325: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074bcc40 of size 26368 next 182\n",
      "2023-06-29 15:45:43.119334: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b074c3340 of size 524288 next 201\n",
      "2023-06-29 15:45:43.119343: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07543340 of size 6245632 next 161\n",
      "2023-06-29 15:45:43.119351: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07b38040 of size 1048576 next 203\n",
      "2023-06-29 15:45:43.119360: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07c38040 of size 1048576 next 163\n",
      "2023-06-29 15:45:43.119368: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07d38040 of size 2097152 next 164\n",
      "2023-06-29 15:45:43.119377: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b07f38040 of size 1048576 next 206\n",
      "2023-06-29 15:45:43.119385: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08038040 of size 1048576 next 208\n",
      "2023-06-29 15:45:43.119393: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08138040 of size 1048576 next 211\n",
      "2023-06-29 15:45:43.119402: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08238040 of size 1048576 next 214\n",
      "2023-06-29 15:45:43.119410: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08338040 of size 224000 next 237\n",
      "2023-06-29 15:45:43.119418: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0836eb40 of size 224000 next 239\n",
      "2023-06-29 15:45:43.119427: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083a5640 of size 224000 next 245\n",
      "2023-06-29 15:45:43.119435: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083dc140 of size 8192 next 263\n",
      "2023-06-29 15:45:43.119443: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083de140 of size 8192 next 11\n",
      "2023-06-29 15:45:43.119452: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083e0140 of size 8192 next 304\n",
      "2023-06-29 15:45:43.119460: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083e2140 of size 31744 next 282\n",
      "2023-06-29 15:45:43.119469: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b083e9d40 of size 167680 next 246\n",
      "2023-06-29 15:45:43.119478: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08412c40 of size 204800 next 343\n",
      "2023-06-29 15:45:43.119487: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08444c40 of size 8192 next 323\n",
      "2023-06-29 15:45:43.119496: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08446c40 of size 8192 next 316\n",
      "2023-06-29 15:45:43.119504: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08448c40 of size 8192 next 327\n",
      "2023-06-29 15:45:43.119512: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0844ac40 of size 24576 next 383\n",
      "2023-06-29 15:45:43.119520: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08450c40 of size 24576 next 328\n",
      "2023-06-29 15:45:43.119529: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08456c40 of size 24576 next 278\n",
      "2023-06-29 15:45:43.119537: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0845cc40 of size 24576 next 290\n",
      "2023-06-29 15:45:43.119545: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08462c40 of size 8192 next 381\n",
      "2023-06-29 15:45:43.119554: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08464c40 of size 8192 next 286\n",
      "2023-06-29 15:45:43.119562: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08466c40 of size 8192 next 247\n",
      "2023-06-29 15:45:43.119570: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08468c40 of size 24576 next 331\n",
      "2023-06-29 15:45:43.119578: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0846ec40 of size 24576 next 166\n",
      "2023-06-29 15:45:43.119587: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08474c40 of size 24576 next 360\n",
      "2023-06-29 15:45:43.119595: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0847ac40 of size 24576 next 315\n",
      "2023-06-29 15:45:43.119603: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08480c40 of size 8192 next 256\n",
      "2023-06-29 15:45:43.119612: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08482c40 of size 8192 next 289\n",
      "2023-06-29 15:45:43.119620: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08484c40 of size 8192 next 262\n",
      "2023-06-29 15:45:43.119628: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08486c40 of size 24576 next 302\n",
      "2023-06-29 15:45:43.119636: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0848cc40 of size 24576 next 169\n",
      "2023-06-29 15:45:43.119644: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08492c40 of size 24576 next 313\n",
      "2023-06-29 15:45:43.119652: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08498c40 of size 24576 next 344\n",
      "2023-06-29 15:45:43.119660: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0849ec40 of size 24576 next 160\n",
      "2023-06-29 15:45:43.119669: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084a4c40 of size 29184 next 249\n",
      "2023-06-29 15:45:43.119677: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084abe40 of size 224000 next 255\n",
      "2023-06-29 15:45:43.119686: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084e2940 of size 24576 next 318\n",
      "2023-06-29 15:45:43.119694: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084e8940 of size 24576 next 345\n",
      "2023-06-29 15:45:43.119703: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084ee940 of size 24576 next 172\n",
      "2023-06-29 15:45:43.119711: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084f4940 of size 44032 next 167\n",
      "2023-06-29 15:45:43.119721: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b084ff540 of size 220160 next 293\n",
      "2023-06-29 15:45:43.119729: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08535140 of size 7424 next 156\n",
      "2023-06-29 15:45:43.119738: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08536e40 of size 7424 next 129\n",
      "2023-06-29 15:45:43.119747: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08538b40 of size 738560 next 308\n",
      "2023-06-29 15:45:43.119756: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b085ed040 of size 5550080 next 178\n",
      "2023-06-29 15:45:43.119765: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08b38040 of size 2097152 next 217\n",
      "2023-06-29 15:45:43.119773: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b08d38040 of size 4194304 next 226\n",
      "2023-06-29 15:45:43.119782: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09138040 of size 4194304 next 227\n",
      "2023-06-29 15:45:43.119791: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09538040 of size 3200000 next 236\n",
      "2023-06-29 15:45:43.119800: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09845440 of size 3200000 next 242\n",
      "2023-06-29 15:45:43.119808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09b52840 of size 3200000 next 252\n",
      "2023-06-29 15:45:43.119816: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b09e5fc40 of size 4800000 next 238\n",
      "2023-06-29 15:45:43.119825: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0a2f3a40 of size 6200064 next 151\n",
      "2023-06-29 15:45:43.119835: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0a8dd540 of size 3283712 next 10\n",
      "2023-06-29 15:45:43.119844: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0abff040 of size 9437184 next 140\n",
      "2023-06-29 15:45:43.119853: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f7b0b4ff040 of size 11534336 next 18446744073709551615\n",
      "2023-06-29 15:45:43.119860: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-06-29 15:45:43.119872: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 38 Chunks of size 4096 totalling 152.0KiB\n",
      "2023-06-29 15:45:43.119883: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 4608 totalling 22.5KiB\n",
      "2023-06-29 15:45:43.119892: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4864 totalling 4.8KiB\n",
      "2023-06-29 15:45:43.119901: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 6144 totalling 30.0KiB\n",
      "2023-06-29 15:45:43.119910: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6400 totalling 6.2KiB\n",
      "2023-06-29 15:45:43.119919: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6656 totalling 6.5KiB\n",
      "2023-06-29 15:45:43.119928: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6912 totalling 6.8KiB\n",
      "2023-06-29 15:45:43.119938: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 7424 totalling 36.2KiB\n",
      "2023-06-29 15:45:43.119947: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 50 Chunks of size 8192 totalling 400.0KiB\n",
      "2023-06-29 15:45:43.119956: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 8448 totalling 8.2KiB\n",
      "2023-06-29 15:45:43.119966: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 8 Chunks of size 8960 totalling 70.0KiB\n",
      "2023-06-29 15:45:43.119975: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 9216 totalling 36.0KiB\n",
      "2023-06-29 15:45:43.119984: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 10240 totalling 10.0KiB\n",
      "2023-06-29 15:45:43.119993: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11776 totalling 11.5KiB\n",
      "2023-06-29 15:45:43.120002: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12288 totalling 12.0KiB\n",
      "2023-06-29 15:45:43.120012: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12544 totalling 12.2KiB\n",
      "2023-06-29 15:45:43.120021: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12800 totalling 12.5KiB\n",
      "2023-06-29 15:45:43.120030: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 13824 totalling 13.5KiB\n",
      "2023-06-29 15:45:43.120040: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 15616 totalling 15.2KiB\n",
      "2023-06-29 15:45:43.120049: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 15872 totalling 15.5KiB\n",
      "2023-06-29 15:45:43.120058: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 16128 totalling 15.8KiB\n",
      "2023-06-29 15:45:43.120068: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 16384 totalling 16.0KiB\n",
      "2023-06-29 15:45:43.120077: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 9 Chunks of size 18432 totalling 162.0KiB\n",
      "2023-06-29 15:45:43.120086: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 21504 totalling 21.0KiB\n",
      "2023-06-29 15:45:43.120095: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 58 Chunks of size 24576 totalling 1.36MiB\n",
      "2023-06-29 15:45:43.120104: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 26368 totalling 25.8KiB\n",
      "2023-06-29 15:45:43.120114: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 33 Chunks of size 28160 totalling 907.5KiB\n",
      "2023-06-29 15:45:43.120123: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 28672 totalling 56.0KiB\n",
      "2023-06-29 15:45:43.120133: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 29184 totalling 28.5KiB\n",
      "2023-06-29 15:45:43.120142: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 31744 totalling 31.0KiB\n",
      "2023-06-29 15:45:43.120151: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 32768 totalling 32.0KiB\n",
      "2023-06-29 15:45:43.120160: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 35840 totalling 35.0KiB\n",
      "2023-06-29 15:45:43.120169: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 36864 totalling 36.0KiB\n",
      "2023-06-29 15:45:43.120178: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 37632 totalling 36.8KiB\n",
      "2023-06-29 15:45:43.120188: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 40960 totalling 80.0KiB\n",
      "2023-06-29 15:45:43.120197: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 44032 totalling 43.0KiB\n",
      "2023-06-29 15:45:43.120206: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 57344 totalling 56.0KiB\n",
      "2023-06-29 15:45:43.120215: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 65536 totalling 256.0KiB\n",
      "2023-06-29 15:45:43.120225: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 81920 totalling 160.0KiB\n",
      "2023-06-29 15:45:43.120234: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 100352 totalling 98.0KiB\n",
      "2023-06-29 15:45:43.120244: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 111872 totalling 109.2KiB\n",
      "2023-06-29 15:45:43.120253: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 117760 totalling 115.0KiB\n",
      "2023-06-29 15:45:43.120262: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 131072 totalling 128.0KiB\n",
      "2023-06-29 15:45:43.120272: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 143360 totalling 560.0KiB\n",
      "2023-06-29 15:45:43.120281: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 147456 totalling 432.0KiB\n",
      "2023-06-29 15:45:43.120290: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 167680 totalling 163.8KiB\n",
      "2023-06-29 15:45:43.120300: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 204800 totalling 200.0KiB\n",
      "2023-06-29 15:45:43.120309: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 220160 totalling 215.0KiB\n",
      "2023-06-29 15:45:43.120318: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 223232 totalling 218.0KiB\n",
      "2023-06-29 15:45:43.120327: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 6 Chunks of size 224000 totalling 1.28MiB\n",
      "2023-06-29 15:45:43.120337: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 226048 totalling 220.8KiB\n",
      "2023-06-29 15:45:43.120346: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 229376 totalling 224.0KiB\n",
      "2023-06-29 15:45:43.120355: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 240128 totalling 469.0KiB\n",
      "2023-06-29 15:45:43.120364: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 262144 totalling 1.25MiB\n",
      "2023-06-29 15:45:43.120373: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 294912 totalling 288.0KiB\n",
      "2023-06-29 15:45:43.120383: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 327680 totalling 320.0KiB\n",
      "2023-06-29 15:45:43.120392: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 401408 totalling 1.15MiB\n",
      "2023-06-29 15:45:43.120401: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 411392 totalling 401.8KiB\n",
      "2023-06-29 15:45:43.120411: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 417792 totalling 408.0KiB\n",
      "2023-06-29 15:45:43.120420: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 9 Chunks of size 524288 totalling 4.50MiB\n",
      "2023-06-29 15:45:43.120428: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 589824 totalling 1.12MiB\n",
      "2023-06-29 15:45:43.120438: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 602112 totalling 588.0KiB\n",
      "2023-06-29 15:45:43.120447: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 8 Chunks of size 627200 totalling 4.79MiB\n",
      "2023-06-29 15:45:43.120456: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 655360 totalling 640.0KiB\n",
      "2023-06-29 15:45:43.120466: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 738560 totalling 721.2KiB\n",
      "2023-06-29 15:45:43.120475: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 786432 totalling 768.0KiB\n",
      "2023-06-29 15:45:43.120484: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 884736 totalling 864.0KiB\n",
      "2023-06-29 15:45:43.120493: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 935936 totalling 914.0KiB\n",
      "2023-06-29 15:45:43.120502: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 10 Chunks of size 1048576 totalling 10.00MiB\n",
      "2023-06-29 15:45:43.120512: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 1179648 totalling 2.25MiB\n",
      "2023-06-29 15:45:43.120521: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 1310720 totalling 2.50MiB\n",
      "2023-06-29 15:45:43.120530: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1835008 totalling 1.75MiB\n",
      "2023-06-29 15:45:43.120539: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1966080 totalling 1.88MiB\n",
      "2023-06-29 15:45:43.120548: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 2097152 totalling 6.00MiB\n",
      "2023-06-29 15:45:43.120557: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2195456 totalling 2.09MiB\n",
      "2023-06-29 15:45:43.120566: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 2359296 totalling 6.75MiB\n",
      "2023-06-29 15:45:43.120575: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 2621440 totalling 5.00MiB\n",
      "2023-06-29 15:45:43.120584: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2883584 totalling 2.75MiB\n",
      "2023-06-29 15:45:43.120593: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2916352 totalling 2.78MiB\n",
      "2023-06-29 15:45:43.120602: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 3000064 totalling 5.72MiB\n",
      "2023-06-29 15:45:43.120611: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3145728 totalling 3.00MiB\n",
      "2023-06-29 15:45:43.120620: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 3200000 totalling 9.16MiB\n",
      "2023-06-29 15:45:43.120629: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3276800 totalling 3.12MiB\n",
      "2023-06-29 15:45:43.120638: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3283712 totalling 3.13MiB\n",
      "2023-06-29 15:45:43.120647: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3561728 totalling 3.40MiB\n",
      "2023-06-29 15:45:43.120656: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3670016 totalling 3.50MiB\n",
      "2023-06-29 15:45:43.120665: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3878912 totalling 3.70MiB\n",
      "2023-06-29 15:45:43.120674: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4014080 totalling 3.83MiB\n",
      "2023-06-29 15:45:43.120683: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 4194304 totalling 8.00MiB\n",
      "2023-06-29 15:45:43.120692: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4199936 totalling 4.00MiB\n",
      "2023-06-29 15:45:43.120701: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 4718592 totalling 9.00MiB\n",
      "2023-06-29 15:45:43.120710: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4800000 totalling 4.58MiB\n",
      "2023-06-29 15:45:43.120719: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4860416 totalling 4.63MiB\n",
      "2023-06-29 15:45:43.120728: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5242880 totalling 5.00MiB\n",
      "2023-06-29 15:45:43.120737: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5550080 totalling 5.29MiB\n",
      "2023-06-29 15:45:43.120747: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 5600000 totalling 26.70MiB\n",
      "2023-06-29 15:45:43.120756: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5767168 totalling 5.50MiB\n",
      "2023-06-29 15:45:43.120764: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5779712 totalling 5.51MiB\n",
      "2023-06-29 15:45:43.120773: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6200064 totalling 5.91MiB\n",
      "2023-06-29 15:45:43.120782: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6245632 totalling 5.96MiB\n",
      "2023-06-29 15:45:43.120791: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6250240 totalling 5.96MiB\n",
      "2023-06-29 15:45:43.120801: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 28 Chunks of size 7526400 totalling 200.98MiB\n",
      "2023-06-29 15:45:43.120810: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 8559872 totalling 8.16MiB\n",
      "2023-06-29 15:45:43.120819: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 11 Chunks of size 9437184 totalling 99.00MiB\n",
      "2023-06-29 15:45:43.120828: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 9780992 totalling 9.33MiB\n",
      "2023-06-29 15:45:43.120837: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 10035200 totalling 9.57MiB\n",
      "2023-06-29 15:45:43.120847: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11200000 totalling 10.68MiB\n",
      "2023-06-29 15:45:43.120856: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11468288 totalling 10.94MiB\n",
      "2023-06-29 15:45:43.120866: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11534336 totalling 11.00MiB\n",
      "2023-06-29 15:45:43.120875: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11698176 totalling 11.16MiB\n",
      "2023-06-29 15:45:43.120885: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12293120 totalling 11.72MiB\n",
      "2023-06-29 15:45:43.120894: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12582912 totalling 12.00MiB\n",
      "2023-06-29 15:45:43.120903: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 13798400 totalling 13.16MiB\n",
      "2023-06-29 15:45:43.120914: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 10 Chunks of size 20070400 totalling 191.41MiB\n",
      "2023-06-29 15:45:43.120923: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 20160000 totalling 19.23MiB\n",
      "2023-06-29 15:45:43.120932: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 22108160 totalling 21.08MiB\n",
      "2023-06-29 15:45:43.120942: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 26869760 totalling 25.62MiB\n",
      "2023-06-29 15:45:43.120951: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 30105600 totalling 57.42MiB\n",
      "2023-06-29 15:45:43.120961: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 35569408 totalling 33.92MiB\n",
      "2023-06-29 15:45:43.120970: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 40140800 totalling 153.12MiB\n",
      "2023-06-29 15:45:43.120980: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 40560128 totalling 116.04MiB\n",
      "2023-06-29 15:45:43.120990: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 7 Chunks of size 120422400 totalling 803.91MiB\n",
      "2023-06-29 15:45:43.120999: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 174191360 totalling 166.12MiB\n",
      "2023-06-29 15:45:43.121009: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 183606528 totalling 175.10MiB\n",
      "2023-06-29 15:45:43.121018: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 195661824 totalling 186.60MiB\n",
      "2023-06-29 15:45:43.121028: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 196000000 totalling 186.92MiB\n",
      "2023-06-29 15:45:43.121037: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 205520896 totalling 392.00MiB\n",
      "2023-06-29 15:45:43.121047: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 226612736 totalling 216.11MiB\n",
      "2023-06-29 15:45:43.121057: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 263617536 totalling 251.41MiB\n",
      "2023-06-29 15:45:43.121066: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 268435456 totalling 256.00MiB\n",
      "2023-06-29 15:45:43.121076: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 296750080 totalling 283.00MiB\n",
      "2023-06-29 15:45:43.121085: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 309080576 totalling 294.76MiB\n",
      "2023-06-29 15:45:43.121095: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 325283072 totalling 310.21MiB\n",
      "2023-06-29 15:45:43.121104: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 439040000 totalling 837.40MiB\n",
      "2023-06-29 15:45:43.121113: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2950348800 totalling 2.75GiB\n",
      "2023-06-29 15:45:43.121123: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 11801395200 totalling 32.97GiB\n",
      "2023-06-29 15:45:43.121133: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 41.18GiB\n",
      "2023-06-29 15:45:43.121141: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 67434475520 memory_limit_: 67434475520 available bytes: 0 curr_region_allocation_bytes_: 68719476736\n",
      "2023-06-29 15:45:43.121155: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                     67434475520\n",
      "InUse:                     44216678656\n",
      "MaxInUse:                  44216678656\n",
      "NumAllocs:                      246999\n",
      "MaxAllocSize:              11801395200\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-06-29 15:45:43.121210: W tensorflow/tsl/framework/bfc_allocator.cc:492] ************************************_____________******************_______**___________*************\n",
      "2023-06-29 15:45:43.121248: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at mkl_einsum_op.cc:259 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[50,6,3136,3136] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_9/multi_head_attention_5/einsum/Einsum' defined at (most recent call last):\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3182346/2668463021.py\", line 2, in <module>\n      hist = model.fit(x_train,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/layers/attention/multi_head_attention.py\", line 595, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/layers/attention/multi_head_attention.py\", line 524, in _compute_attention\n      attention_scores = tf.einsum(self._dot_product_equation, key, query)\nNode: 'model_9/multi_head_attention_5/einsum/Einsum'\nOOM when allocating tensor with shape[50,6,3136,3136] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node model_9/multi_head_attention_5/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_66762]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, \n\u001b[1;32m      3\u001b[0m                   y_train, \n\u001b[1;32m      4\u001b[0m                   epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m                 \n\u001b[1;32m      6\u001b[0m                   batch_size\u001b[39m=\u001b[39;49mbatch_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_9/multi_head_attention_5/einsum/Einsum' defined at (most recent call last):\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3182346/2668463021.py\", line 2, in <module>\n      hist = model.fit(x_train,\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/layers/attention/multi_head_attention.py\", line 595, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/layers/attention/multi_head_attention.py\", line 524, in _compute_attention\n      attention_scores = tf.einsum(self._dot_product_equation, key, query)\nNode: 'model_9/multi_head_attention_5/einsum/Einsum'\nOOM when allocating tensor with shape[50,6,3136,3136] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node model_9/multi_head_attention_5/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_66762]"
     ]
    }
   ],
   "source": [
    "  #Train model\n",
    "hist = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    epochs=10, \n",
    "                  \n",
    "                    batch_size=batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 10:33:57.717332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-11 10:33:57.725780: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 7, 7, 2048)  8192        ['conv5_block3_out[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['batch_normalization[0][0]']    \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           131136      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 140)          9100        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,736,140\n",
      "Trainable params: 144,332\n",
      "Non-trainable params: 23,591,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras import regularizers\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "img_shape = (224, 224, 3)\n",
    "inputs = Input(img_shape)\n",
    "num_classes = 140\n",
    "\n",
    "resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=img_shape)\n",
    "outputs = resnet50.output\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = GlobalAveragePooling2D()(outputs)\n",
    "outputs= Dropout(0.5)(outputs)\n",
    "outputs= Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(outputs)\n",
    "outputs= Dropout(0.5)(outputs)\n",
    "outputs = Dense(num_classes, activation = 'softmax')(outputs)\n",
    "model = Model(inputs = resnet50.input, outputs = outputs)\n",
    "\n",
    "# transfer learning freeze all convolutional  layers\n",
    "for layer in resnet50.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "35/35 [==============================] - 700s 20s/step - loss: 4.9750 - accuracy: 0.0330\n",
      "Epoch 2/30\n",
      "35/35 [==============================] - 704s 20s/step - loss: 4.4731 - accuracy: 0.0946\n",
      "Epoch 3/30\n",
      "35/35 [==============================] - 686s 20s/step - loss: 3.9624 - accuracy: 0.1679\n",
      "Epoch 4/30\n",
      "35/35 [==============================] - 701s 20s/step - loss: 3.4162 - accuracy: 0.2509\n",
      "Epoch 5/30\n",
      "35/35 [==============================] - 704s 20s/step - loss: 2.9955 - accuracy: 0.3080\n",
      "Epoch 6/30\n",
      "35/35 [==============================] - 708s 20s/step - loss: 2.6682 - accuracy: 0.3795\n",
      "Epoch 7/30\n",
      "35/35 [==============================] - 699s 20s/step - loss: 2.3736 - accuracy: 0.4187\n",
      "Epoch 8/30\n",
      "35/35 [==============================] - 704s 20s/step - loss: 2.1488 - accuracy: 0.4795\n",
      "Epoch 9/30\n",
      "35/35 [==============================] - 700s 20s/step - loss: 1.9274 - accuracy: 0.5420\n",
      "Epoch 10/30\n",
      "35/35 [==============================] - 699s 20s/step - loss: 1.8571 - accuracy: 0.5527\n",
      "Epoch 11/30\n",
      "35/35 [==============================] - 708s 20s/step - loss: 1.7375 - accuracy: 0.5750\n",
      "Epoch 12/30\n",
      "35/35 [==============================] - 698s 20s/step - loss: 1.5948 - accuracy: 0.6161\n",
      "Epoch 13/30\n",
      "35/35 [==============================] - 704s 20s/step - loss: 1.4770 - accuracy: 0.6411\n",
      "Epoch 14/30\n",
      "35/35 [==============================] - 691s 20s/step - loss: 1.4386 - accuracy: 0.6482\n",
      "Epoch 15/30\n",
      "35/35 [==============================] - 705s 20s/step - loss: 1.3679 - accuracy: 0.6786\n",
      "Epoch 16/30\n",
      "35/35 [==============================] - 703s 20s/step - loss: 1.3484 - accuracy: 0.6768\n",
      "Epoch 17/30\n",
      "35/35 [==============================] - 684s 20s/step - loss: 1.2645 - accuracy: 0.6946\n",
      "Epoch 18/30\n",
      "35/35 [==============================] - 699s 20s/step - loss: 1.2436 - accuracy: 0.7152\n",
      "Epoch 19/30\n",
      "35/35 [==============================] - 698s 20s/step - loss: 1.1744 - accuracy: 0.7411\n",
      "Epoch 20/30\n",
      "35/35 [==============================] - 707s 20s/step - loss: 1.1298 - accuracy: 0.7402\n",
      "Epoch 21/30\n",
      "23/35 [==================>...........] - ETA: 3:59 - loss: 1.1136 - accuracy: 0.7663"
     ]
    }
   ],
   "source": [
    "model.fit(preprocessed_images_train, one_hot_labels_train, epochs=30,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " BatchNormalization (BatchNo  (None, 7, 7, 512)        2048      \n",
      " rmalization)                                                    \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 140)               35980     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,193,740\n",
      "Trainable params: 168,332\n",
      "Non-trainable params: 20,025,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "IMG_SIZE = 224\n",
    "img_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "inputs = Input(img_shape)\n",
    "num_classes = 140\n",
    "\n",
    "vgg19_model = VGG19(weights='imagenet', include_top=False, input_shape= img_shape)\n",
    "vgg19_model.trainable = False\n",
    "outputs = vgg19_model(inputs)\n",
    "outputs = BatchNormalization(name = 'BatchNormalization')(outputs)\n",
    "outputs = GlobalAveragePooling2D()(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Dense(256)(outputs)\n",
    "outputs = LeakyReLU(alpha=0.1)(outputs)\n",
    "outputs = Dropout(0.25)(outputs)\n",
    "outputs = Dense(num_classes, activation = 'softmax')(outputs)\n",
    "\n",
    "model = Model(inputs = [inputs], outputs = [outputs])\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "374/374 [==============================] - 129s 341ms/step - loss: 4.6813 - accuracy: 0.0473\n",
      "Epoch 2/10\n",
      "374/374 [==============================] - 128s 342ms/step - loss: 3.2637 - accuracy: 0.2786\n",
      "Epoch 3/10\n",
      "374/374 [==============================] - 130s 346ms/step - loss: 2.1703 - accuracy: 0.4598\n",
      "Epoch 4/10\n",
      "374/374 [==============================] - 128s 343ms/step - loss: 1.5545 - accuracy: 0.5964\n",
      "Epoch 5/10\n",
      "374/374 [==============================] - 129s 346ms/step - loss: 1.2912 - accuracy: 0.6571\n",
      "Epoch 6/10\n",
      "374/374 [==============================] - 126s 338ms/step - loss: 0.9954 - accuracy: 0.7170\n",
      "Epoch 7/10\n",
      "374/374 [==============================] - 129s 344ms/step - loss: 0.8689 - accuracy: 0.7652\n",
      "Epoch 8/10\n",
      "374/374 [==============================] - 136s 364ms/step - loss: 0.7393 - accuracy: 0.7875\n",
      "Epoch 9/10\n",
      "374/374 [==============================] - 126s 338ms/step - loss: 0.6971 - accuracy: 0.8054\n",
      "Epoch 10/10\n",
      "374/374 [==============================] - 128s 342ms/step - loss: 0.6630 - accuracy: 0.8143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b5062d660>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(preprocessed_images_train, one_hot_labels_train, epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 43s 2s/step - loss: 0.4137 - accuracy: 0.8857\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 10:48:07.745415: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-26 10:48:07.751052: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model((224,224,1), num_classes=140)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 224, 224, 1)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 112, 112, 12  1280        ['rescaling[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 112, 112, 12  512        ['conv2d[0][0]']                 \n",
      " alization)                     8)                                                                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 112, 112, 12  0           ['batch_normalization[0][0]']    \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 112, 112, 12  0           ['activation[0][0]']             \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 112, 112, 25  34176      ['activation_1[0][0]']           \n",
      " v2D)                           6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 112, 112, 25  1024       ['separable_conv2d[0][0]']       \n",
      " rmalization)                   6)                                                                \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 112, 112, 25  0           ['batch_normalization_1[0][0]']  \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 112, 112, 25  68096      ['activation_2[0][0]']           \n",
      " onv2D)                         6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 112, 112, 25  1024       ['separable_conv2d_1[0][0]']     \n",
      " rmalization)                   6)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 56, 56, 256)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 56, 56, 256)  33024       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 56, 56, 256)  0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 56, 56, 256)  0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 56, 56, 512)  133888     ['activation_3[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 56, 56, 512)  2048       ['separable_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 56, 56, 512)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 56, 56, 512)  267264     ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 56, 56, 512)  2048       ['separable_conv2d_3[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 512)  0          ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 28, 28, 512)  131584      ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 28, 28, 512)  0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 28, 28, 512)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 28, 28, 728)  378072     ['activation_5[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 28, 28, 728)  2912       ['separable_conv2d_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 28, 28, 728)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 28, 28, 728)  537264     ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 28, 28, 728)  2912       ['separable_conv2d_5[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 728)  0          ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 14, 14, 728)  373464      ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 14, 14, 728)  0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv2d_6 (SeparableC  (None, 14, 14, 1024  753048     ['add_2[0][0]']                  \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 14, 14, 1024  4096       ['separable_conv2d_6[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 14, 14, 1024  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['activation_7[0][0]']           \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 140)          143500      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,871,236\n",
      "Trainable params: 2,862,948\n",
      "Non-trainable params: 8,288\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 10:48:57.694599: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'gradient_tape/model/conv2d_3/Conv2D/ShapeN' exist for missing node 'model/add_1/add'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 158s 4s/step - loss: 5.0445 - accuracy: 0.0250\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 148s 4s/step - loss: 3.9097 - accuracy: 0.0991\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 146s 4s/step - loss: 3.2415 - accuracy: 0.1982\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 2.7167 - accuracy: 0.2973\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 2.2193 - accuracy: 0.4250\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 1.7559 - accuracy: 0.5607\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 146s 4s/step - loss: 1.3943 - accuracy: 0.6527\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 1.1060 - accuracy: 0.7509\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.9043 - accuracy: 0.8027\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 0.7497 - accuracy: 0.8402\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.5350 - accuracy: 0.8991\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.4245 - accuracy: 0.9330\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.3255 - accuracy: 0.9491\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 146s 4s/step - loss: 0.2946 - accuracy: 0.9598\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 148s 4s/step - loss: 0.2175 - accuracy: 0.9786\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 146s 4s/step - loss: 0.1588 - accuracy: 0.9848\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 0.1158 - accuracy: 0.9929\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 146s 4s/step - loss: 0.1025 - accuracy: 0.9902\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 0.0818 - accuracy: 0.9964\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 144s 4s/step - loss: 0.0497 - accuracy: 0.9982\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 0.0398 - accuracy: 0.9991\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0146 - accuracy: 0.9991\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 143s 4s/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 148s 4s/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 147s 4s/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 146s 4s/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 146s 4s/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 144s 4s/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 146s 4s/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 144s 4s/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 144s 4s/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 145s 4s/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 177s 5s/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 186s 5s/step - loss: 0.0019 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f16d26288b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(preprocessed_images_train, one_hot_labels_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 12:52:20.996582: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'model/add_2/add' exist for missing node 'model/add_1/add'.\n",
      "2023-06-26 12:52:21.060601: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'model/add_2/add' exist for missing node 'model/add_1/add'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 117s 7s/step - loss: 0.0896 - accuracy: 0.9821\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "import numpy as np # linear algebra\n",
    "import keras.backend as K \n",
    "import time as ti \n",
    "import cv2\n",
    "import os\n",
    "import glob # for including images\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, DepthwiseConv2D, MaxPooling2D, AveragePooling2D  \n",
    "# from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.python.keras.optimizers import RMSprop, SGD, Adadelta, Adam \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_224\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 224, 224, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  288         ['input_18[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_12 (G  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " predictions (Dense)            (None, 140)          179340      ['global_average_pooling2d_12[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,436,748\n",
      "Trainable params: 2,402,636\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2(input_shape=(224, 224, 1), alpha=1, weights=None,classes=140)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35/35 [==============================] - 94s 2s/step - loss: 5.1423 - accuracy: 0.0098\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 81s 2s/step - loss: 4.2785 - accuracy: 0.0491\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 81s 2s/step - loss: 3.7234 - accuracy: 0.1009\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 80s 2s/step - loss: 2.9112 - accuracy: 0.2339\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 80s 2s/step - loss: 2.2615 - accuracy: 0.3580\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 80s 2s/step - loss: 1.7196 - accuracy: 0.5080\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 81s 2s/step - loss: 1.2219 - accuracy: 0.6429\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.8206 - accuracy: 0.7830\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.6759 - accuracy: 0.8134\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.4186 - accuracy: 0.9036\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.2773 - accuracy: 0.9411\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.2249 - accuracy: 0.9554\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.1919 - accuracy: 0.9625\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.1323 - accuracy: 0.9768\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.1204 - accuracy: 0.9759\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.1266 - accuracy: 0.9768\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.1023 - accuracy: 0.9857\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.1078 - accuracy: 0.9786\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.1242 - accuracy: 0.9705\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.1409 - accuracy: 0.9670\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.1874 - accuracy: 0.9563\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.2425 - accuracy: 0.9312\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.2444 - accuracy: 0.9330\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.2510 - accuracy: 0.9268\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.2515 - accuracy: 0.9304\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.1324 - accuracy: 0.9643\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.0770 - accuracy: 0.9839\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.0402 - accuracy: 0.9946\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.0306 - accuracy: 0.9937\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.0395 - accuracy: 0.9929\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.0401 - accuracy: 0.9937\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.0371 - accuracy: 0.9911\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.0548 - accuracy: 0.9902\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.0472 - accuracy: 0.9893\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.0505 - accuracy: 0.9929\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.0398 - accuracy: 0.9911\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.0360 - accuracy: 0.9929\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.0355 - accuracy: 0.9929\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.0291 - accuracy: 0.9937\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.0590 - accuracy: 0.9812\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.0497 - accuracy: 0.9875\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.0302 - accuracy: 0.9955\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.0236 - accuracy: 0.9955\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.0543 - accuracy: 0.9866\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 70s 2s/step - loss: 0.0957 - accuracy: 0.9741\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.1475 - accuracy: 0.9545\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.2951 - accuracy: 0.9161\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.3936 - accuracy: 0.8696\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.3046 - accuracy: 0.9125\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.1900 - accuracy: 0.9339\n"
     ]
    }
   ],
   "source": [
    "input_shape=(224,224,1)\n",
    "epochs=50\n",
    "\n",
    "result =model.fit(preprocessed_images_train, one_hot_labels_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 69s 4s/step - loss: 11.6394 - accuracy: 0.0071\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(preprocessed_images_test, one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow\n",
    "from keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 224, 224, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_168 (Conv2D)         (None, 224, 224, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 112, 112, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_169 (Conv2D)         (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 56, 56, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_170 (Conv2D)         (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_171 (Conv2D)         (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 112, 112, 128)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_172 (Conv2D)         (None, 112, 112, 64)      73792     \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 224, 224, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_173 (Conv2D)         (None, 224, 224, 1)       577       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 314,625\n",
      "Trainable params: 314,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape = (224,224, 1))\n",
    "def autoencoder(input_img):\n",
    "    #encoder\n",
    "    #input = 28 x 28 x 1 (wide and thin)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)#decoder\n",
    "    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128\n",
    "    up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n",
    "    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n",
    "    up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
    "    return decoded\n",
    "autoencoder = Model(input_img, autoencoder(input_img))\n",
    "\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop(), metrics=['accuracy'])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "9/9 [==============================] - 85s 9s/step - loss: 0.0737 - accuracy: 0.0112\n",
      "Epoch 2/30\n",
      "9/9 [==============================] - 80s 9s/step - loss: 2.2123e-05 - accuracy: 0.0112\n",
      "Epoch 3/30\n",
      "9/9 [==============================] - 86s 10s/step - loss: 5.5401e-06 - accuracy: 0.0112\n",
      "Epoch 4/30\n",
      "9/9 [==============================] - 90s 10s/step - loss: 2.7483e-06 - accuracy: 0.0112\n",
      "Epoch 5/30\n",
      "9/9 [==============================] - 81s 9s/step - loss: 1.5675e-06 - accuracy: 0.0112\n",
      "Epoch 6/30\n",
      "9/9 [==============================] - 75s 8s/step - loss: 9.3442e-07 - accuracy: 0.0112\n",
      "Epoch 7/30\n",
      "9/9 [==============================] - 75s 8s/step - loss: 5.6836e-07 - accuracy: 0.0112\n",
      "Epoch 8/30\n",
      "9/9 [==============================] - 77s 8s/step - loss: 3.5233e-07 - accuracy: 0.0112\n",
      "Epoch 9/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 2.2326e-07 - accuracy: 0.0112\n",
      "Epoch 10/30\n",
      "9/9 [==============================] - 78s 9s/step - loss: 1.4511e-07 - accuracy: 0.0112\n",
      "Epoch 11/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 9.6665e-08 - accuracy: 0.0112\n",
      "Epoch 12/30\n",
      "9/9 [==============================] - 78s 9s/step - loss: 6.5778e-08 - accuracy: 0.0112\n",
      "Epoch 13/30\n",
      "9/9 [==============================] - 82s 9s/step - loss: 4.5523e-08 - accuracy: 0.0112\n",
      "Epoch 14/30\n",
      "9/9 [==============================] - 79s 9s/step - loss: 3.1893e-08 - accuracy: 0.0112\n",
      "Epoch 15/30\n",
      "9/9 [==============================] - 81s 9s/step - loss: 2.2525e-08 - accuracy: 0.0112\n",
      "Epoch 16/30\n",
      "9/9 [==============================] - 90s 10s/step - loss: 1.5987e-08 - accuracy: 0.0112\n",
      "Epoch 17/30\n",
      "9/9 [==============================] - 78s 9s/step - loss: 1.1382e-08 - accuracy: 0.0112\n",
      "Epoch 18/30\n",
      "9/9 [==============================] - 74s 8s/step - loss: 8.1278e-09 - accuracy: 0.0112\n",
      "Epoch 19/30\n",
      "9/9 [==============================] - 75s 8s/step - loss: 5.8251e-09 - accuracy: 0.0112\n",
      "Epoch 20/30\n",
      "9/9 [==============================] - 75s 8s/step - loss: 4.1938e-09 - accuracy: 0.0112\n",
      "Epoch 21/30\n",
      "9/9 [==============================] - 77s 9s/step - loss: 3.0360e-09 - accuracy: 0.0112\n",
      "Epoch 22/30\n",
      "9/9 [==============================] - 77s 9s/step - loss: 2.2113e-09 - accuracy: 0.0112\n",
      "Epoch 23/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 1.6209e-09 - accuracy: 0.0112\n",
      "Epoch 24/30\n",
      "9/9 [==============================] - 78s 9s/step - loss: 1.1960e-09 - accuracy: 0.0112\n",
      "Epoch 25/30\n",
      "9/9 [==============================] - 80s 9s/step - loss: 8.8891e-10 - accuracy: 0.0112\n",
      "Epoch 26/30\n",
      "9/9 [==============================] - 80s 9s/step - loss: 6.6674e-10 - accuracy: 0.0112\n",
      "Epoch 27/30\n",
      "9/9 [==============================] - 86s 9s/step - loss: 5.0623e-10 - accuracy: 0.0112\n",
      "Epoch 28/30\n",
      "9/9 [==============================] - 87s 10s/step - loss: 3.9068e-10 - accuracy: 0.0112\n",
      "Epoch 29/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 3.0779e-10 - accuracy: 0.0112\n",
      "Epoch 30/30\n",
      "9/9 [==============================] - 76s 8s/step - loss: 2.4843e-10 - accuracy: 0.0112\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "autoencoder_train = autoencoder.fit(x_train, x_train, batch_size=batch_size,epochs=epochs,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 11s 570ms/step - loss: 2.2276e-10 - accuracy: 0.0108\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = autoencoder.evaluate(x_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "\n",
    "#Convolutional block to be used in autoencoder and U-Net\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block for autoencoder (no skip connections)\n",
    "def decoder_block(input, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Encoder will be the same for Autoencoder and U-net\n",
    "#We are getting both conv output and maxpool output for convenience.\n",
    "#we will ignore conv output for Autoencoder. It acts as skip connections for U-Net\n",
    "def build_encoder(input_image):\n",
    "    #inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(input_image, 32)\n",
    "    s2, p2 = encoder_block(p1, 64)\n",
    "    s3, p3 = encoder_block(p2, 128)\n",
    "    \n",
    "    encoded = conv_block(p3, 256) #Bridge\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "#Decoder for Autoencoder ONLY. \n",
    "def build_decoder(encoded):\n",
    "    d1 = decoder_block(encoded, 128)\n",
    "    d2 = decoder_block(d1, 64)\n",
    "    d3 = decoder_block(d2, 32)\n",
    "     \n",
    "    decoded = Conv2D(3, 3, padding=\"same\", activation=\"sigmoid\")(d3)\n",
    "    return decoded\n",
    "\n",
    "#Use encoder and decoder blocks to build the autoencoder. \n",
    "def build_autoencoder(input_shape):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    autoencoder = Model(input_img, build_decoder(build_encoder(input_img)))\n",
    "    return(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 224, 224, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_141 (Conv2D)         (None, 224, 224, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization_203 (Ba  (None, 224, 224, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv2d_142 (Conv2D)         (None, 224, 224, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_204 (Ba  (None, 224, 224, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 112, 112, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_143 (Conv2D)         (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_205 (Ba  (None, 112, 112, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2d_144 (Conv2D)         (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_206 (Ba  (None, 112, 112, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 56, 56, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_145 (Conv2D)         (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_207 (Ba  (None, 56, 56, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv2d_146 (Conv2D)         (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_208 (Ba  (None, 56, 56, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 28, 28, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_147 (Conv2D)         (None, 28, 28, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_209 (Ba  (None, 28, 28, 256)      1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv2d_148 (Conv2D)         (None, 28, 28, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_210 (Ba  (None, 28, 28, 256)      1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_49 (Activation)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 56, 56, 128)      131200    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_149 (Conv2D)         (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_211 (Ba  (None, 56, 56, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_50 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv2d_150 (Conv2D)         (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_212 (Ba  (None, 56, 56, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_51 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 112, 112, 64)     32832     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_151 (Conv2D)         (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_213 (Ba  (None, 112, 112, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_52 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2d_152 (Conv2D)         (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_214 (Ba  (None, 112, 112, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_53 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_8 (Conv2DT  (None, 224, 224, 32)     8224      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_153 (Conv2D)         (None, 224, 224, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_215 (Ba  (None, 224, 224, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_54 (Activation)  (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv2d_154 (Conv2D)         (None, 224, 224, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_216 (Ba  (None, 224, 224, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_55 (Activation)  (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv2d_155 (Conv2D)         (None, 224, 224, 3)       867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,737,955\n",
      "Trainable params: 1,735,139\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (224,224,3)\n",
    "model=build_autoencoder(input_shape)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 1500, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 224 and 32 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_6/conv2d_155/Sigmoid, IteratorGetNext:1)' with input shapes: [32,224,224,3], [32,224,224].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, x_train,\n\u001b[1;32m      2\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m         shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebo___s41.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 1500, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 224 and 32 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_6/conv2d_155/Sigmoid, IteratorGetNext:1)' with input shapes: [32,224,224,3], [32,224,224].\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, x_train,\n",
    "        epochs=10,\n",
    "        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_75 (Bat  (None, 224, 224, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_54 (ReLU)             (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " depthwise_conv2d_52 (Depthw  (None, 224, 224, 32)     320       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_53 (Depthw  (None, 224, 224, 32)     320       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_76 (Bat  (None, 224, 224, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_55 (ReLU)             (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 224, 224, 64)      2112      \n",
      "                                                                 \n",
      " batch_normalization_77 (Bat  (None, 224, 224, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_56 (ReLU)             (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " depthwise_conv2d_54 (Depthw  (None, 224, 224, 64)     640       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_55 (Depthw  (None, 224, 224, 64)     640       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 224, 224, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_57 (ReLU)             (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 224, 224, 128)     8320      \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 224, 224, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_58 (ReLU)             (None, 224, 224, 128)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_56 (Depthw  (None, 224, 224, 128)    1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_57 (Depthw  (None, 224, 224, 128)    1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_80 (Bat  (None, 224, 224, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_59 (ReLU)             (None, 224, 224, 128)     0         \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 224, 224, 128)     16512     \n",
      "                                                                 \n",
      " batch_normalization_81 (Bat  (None, 224, 224, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_60 (ReLU)             (None, 224, 224, 128)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_58 (Depthw  (None, 224, 224, 128)    1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_59 (Depthw  (None, 224, 224, 128)    1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_82 (Bat  (None, 224, 224, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_61 (ReLU)             (None, 224, 224, 128)     0         \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          (None, 224, 224, 256)     33024     \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 224, 224, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_62 (ReLU)             (None, 224, 224, 256)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_60 (Depthw  (None, 224, 224, 256)    2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_61 (Depthw  (None, 224, 224, 256)    2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 224, 224, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_63 (ReLU)             (None, 224, 224, 256)     0         \n",
      "                                                                 \n",
      " conv2d_64 (Conv2D)          (None, 224, 224, 256)     65792     \n",
      "                                                                 \n",
      " batch_normalization_85 (Bat  (None, 224, 224, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_64 (ReLU)             (None, 224, 224, 256)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_62 (Depthw  (None, 224, 224, 256)    2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_63 (Depthw  (None, 224, 224, 256)    2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_86 (Bat  (None, 224, 224, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_65 (ReLU)             (None, 224, 224, 256)     0         \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          (None, 224, 224, 512)     131584    \n",
      "                                                                 \n",
      " batch_normalization_87 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_66 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_64 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_65 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_67 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_66 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_68 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_66 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_67 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_90 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_69 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_91 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_70 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_68 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_69 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_92 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_71 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_68 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_72 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_70 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_71 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_73 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_69 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_95 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_74 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_72 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_73 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_96 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_75 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_70 (Conv2D)          (None, 224, 224, 512)     262656    \n",
      "                                                                 \n",
      " batch_normalization_97 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_76 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " depthwise_conv2d_74 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_75 (Depthw  (None, 224, 224, 512)    5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_98 (Bat  (None, 224, 224, 512)    2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_77 (ReLU)             (None, 224, 224, 512)     0         \n",
      "                                                                 \n",
      " conv2d_71 (Conv2D)          (None, 224, 224, 1024)    525312    \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 224, 224, 1024)   4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_78 (ReLU)             (None, 224, 224, 1024)    0         \n",
      "                                                                 \n",
      " depthwise_conv2d_76 (Depthw  (None, 224, 224, 1024)   10240     \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " depthwise_conv2d_77 (Depthw  (None, 224, 224, 1024)   10240     \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_100 (Ba  (None, 224, 224, 1024)   4096      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_79 (ReLU)             (None, 224, 224, 1024)    0         \n",
      "                                                                 \n",
      " conv2d_72 (Conv2D)          (None, 224, 224, 1024)    1049600   \n",
      "                                                                 \n",
      " batch_normalization_101 (Ba  (None, 224, 224, 1024)   4096      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_80 (ReLU)             (None, 224, 224, 1024)    0         \n",
      "                                                                 \n",
      " average_pooling2d_3 (Averag  (None, 224, 218, 1018)   0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 224, 218, 140)     142660    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,432,068\n",
      "Trainable params: 3,410,180\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#https://journalofcloudcomputing.springeropen.com/counter/pdf/10.1186/s13677-020-00203-9.pdf\n",
    "#https://towardsdatascience.com/building-mobilenet-from-scratch-using-tensorflow-ad009c5dd42c\n",
    "#import all necessary layers\n",
    "from tensorflow.keras.layers import Input, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import ReLU, AvgPool2D, Flatten, Dense\n",
    "from tensorflow.keras import Model\n",
    "# MobileNet block\n",
    "def mobilnet_block (x, filters, strides):\n",
    "    \n",
    "    x = DepthwiseConv2D(kernel_size = 3, strides = strides,dilation_rate=3, padding = 'same')(x)\n",
    "    x = DepthwiseConv2D(kernel_size = 3, strides = strides, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters = filters, kernel_size = 1, strides = 1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "#stem of the model\n",
    "input = Input(shape = (224,224,3))\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "# main part of the model\n",
    "x = mobilnet_block(x, filters = 64, strides = 1)\n",
    "x = mobilnet_block(x, filters = 128, strides = 1)\n",
    "x = mobilnet_block(x, filters = 128, strides = 1)\n",
    "x = mobilnet_block(x, filters = 256, strides = 1)\n",
    "x = mobilnet_block(x, filters = 256, strides = 1)\n",
    "x = mobilnet_block(x, filters = 512, strides = 1)\n",
    "for _ in range (5):\n",
    "     x = mobilnet_block(x, filters = 512, strides = 1)\n",
    "x = mobilnet_block(x, filters = 1024, strides = 1)\n",
    "x = mobilnet_block(x, filters = 1024, strides = 1)\n",
    "x = AvgPool2D (pool_size = 7, strides = 1, data_format='channels_first')(x)\n",
    "output = Dense (units = 140, activation = 'softmax')(x)\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.summary()\n",
    "#plot the model\n",
    "# tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_dtype=False,show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 140) and (32, 224, 218, 140) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train,\n\u001b[1;32m      3\u001b[0m             \n\u001b[1;32m      4\u001b[0m               epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m               validation_data\u001b[39m=\u001b[39;49m(x_test, y_test),\n\u001b[1;32m      6\u001b[0m               shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filef4szjz_s.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 140) and (32, 224, 218, 140) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train,\n",
    "            \n",
    "              epochs=50,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few_shot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
