{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "# Get the path to the images\n",
    "images_path = '/home/rs/21CS91R01/research/2023_ICVGIP-Code/datasets/DB2_A'\n",
    "\n",
    "# List the images in the folder\n",
    "images = os.listdir(images_path)\n",
    "\n",
    "# Iterate over the images\n",
    "for image in images:\n",
    "  # Read the image\n",
    "  image = cv2.imread(os.path.join(images_path, image))\n",
    "\n",
    "  # Display the image\n",
    "  # cv2_imshow(image)\n",
    "  cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:38:56.957508: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images_path = '/home/rs/21CS91R01/research/2023_ICVGIP-Code/datasets/processed_images_path'\n",
    "if not os.path.exists(processed_images_path):\n",
    "    os.mkdir(processed_images_path)\n",
    "\n",
    "# List to store preprocessed images\n",
    "preprocessed_images_train = []\n",
    "labels_train = []\n",
    "\n",
    "preprocessed_images_test = []\n",
    "labels_test = []\n",
    "\n",
    "# Iterate over the images\n",
    "for image_name in os.listdir(images_path):\n",
    "    if image_name.endswith('.bmp'):\n",
    "        # Extract the label from the image name without considering variations\n",
    "        image_name_ex = image_name.split('.')[0]\n",
    "        image_no = image_name_ex.split('_')[0]\n",
    "        index =image_name_ex.split('_')[1]\n",
    "\n",
    "\n",
    "        # Read the image\n",
    "        image_path = os.path.join(images_path, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        # Convert the image to grayscale\n",
    "        # gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Normalize the image\n",
    "        # normalized_image = gray_image\n",
    "        normalized_image = resized_image\n",
    "\n",
    "        # Save the image\n",
    "        processed_image_path = os.path.join(processed_images_path, image_name)\n",
    "        cv2.imwrite(processed_image_path, normalized_image)\n",
    "\n",
    "        # Add the preprocessed image and label to the lists\n",
    "        if (int(index) >= 5 and int(index) <= 12):\n",
    "          preprocessed_images_train.append(normalized_image)\n",
    "          labels_train.append(image_no)\n",
    "        else:\n",
    "          preprocessed_images_test.append(normalized_image)\n",
    "          labels_test.append(image_no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of preprocessed images to a NumPy array\n",
    "preprocessed_images_train = np.array(preprocessed_images_train)\n",
    "preprocessed_images_test = np.array(preprocessed_images_test)\n",
    "print(preprocessed_images_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120,)\n"
     ]
    }
   ],
   "source": [
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "-========\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "numerical_labels_train = label_encoder.fit_transform(labels_train)\n",
    "numerical_labels_test = label_encoder.fit_transform(labels_test)\n",
    "\n",
    "\n",
    "# Convert numerical labels to one-hot encoded format\n",
    "one_hot_labels_train = to_categorical(numerical_labels_train)\n",
    "one_hot_labels_test = to_categorical(numerical_labels_test)\n",
    "\n",
    "print(one_hot_labels_train)\n",
    "print(\"-========\")\n",
    "print(one_hot_labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 16\n",
    "margin = 1  # Margin for constrastive loss.\n",
    "x_train = preprocessed_images_train\n",
    "y_train = one_hot_labels_train\n",
    "x_test = preprocessed_images_test\n",
    "y_test = one_hot_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[42, 42, 42],\n",
       "         [44, 44, 44],\n",
       "         [47, 47, 47],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        [[43, 43, 43],\n",
       "         [46, 46, 46],\n",
       "         [49, 49, 49],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        [[45, 45, 45],\n",
       "         [47, 47, 47],\n",
       "         [50, 50, 50],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[49, 49, 49],\n",
       "         [54, 54, 54],\n",
       "         [57, 57, 57],\n",
       "         ...,\n",
       "         [41, 41, 41],\n",
       "         [46, 46, 46],\n",
       "         [50, 50, 50]],\n",
       "\n",
       "        [[48, 48, 48],\n",
       "         [52, 52, 52],\n",
       "         [57, 57, 57],\n",
       "         ...,\n",
       "         [41, 41, 41],\n",
       "         [44, 44, 44],\n",
       "         [46, 46, 46]],\n",
       "\n",
       "        [[47, 47, 47],\n",
       "         [49, 49, 49],\n",
       "         [55, 55, 55],\n",
       "         ...,\n",
       "         [39, 39, 39],\n",
       "         [41, 41, 41],\n",
       "         [46, 46, 46]]],\n",
       "\n",
       "\n",
       "       [[[37, 37, 37],\n",
       "         [40, 40, 40],\n",
       "         [40, 40, 40],\n",
       "         ...,\n",
       "         [11, 11, 11],\n",
       "         [ 9,  9,  9],\n",
       "         [ 7,  7,  7]],\n",
       "\n",
       "        [[38, 38, 38],\n",
       "         [42, 42, 42],\n",
       "         [42, 42, 42],\n",
       "         ...,\n",
       "         [13, 13, 13],\n",
       "         [ 9,  9,  9],\n",
       "         [ 8,  8,  8]],\n",
       "\n",
       "        [[42, 42, 42],\n",
       "         [42, 42, 42],\n",
       "         [42, 42, 42],\n",
       "         ...,\n",
       "         [13, 13, 13],\n",
       "         [11, 11, 11],\n",
       "         [11, 11, 11]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[67, 67, 67],\n",
       "         [71, 71, 71],\n",
       "         [76, 76, 76],\n",
       "         ...,\n",
       "         [22, 22, 22],\n",
       "         [29, 29, 29],\n",
       "         [37, 37, 37]],\n",
       "\n",
       "        [[67, 67, 67],\n",
       "         [72, 72, 72],\n",
       "         [76, 76, 76],\n",
       "         ...,\n",
       "         [23, 23, 23],\n",
       "         [29, 29, 29],\n",
       "         [40, 40, 40]],\n",
       "\n",
       "        [[64, 64, 64],\n",
       "         [70, 70, 70],\n",
       "         [74, 74, 74],\n",
       "         ...,\n",
       "         [21, 21, 21],\n",
       "         [32, 32, 32],\n",
       "         [41, 41, 41]]],\n",
       "\n",
       "\n",
       "       [[[31, 31, 31],\n",
       "         [34, 34, 34],\n",
       "         [37, 37, 37],\n",
       "         ...,\n",
       "         [ 5,  5,  5],\n",
       "         [ 3,  3,  3],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[33, 33, 33],\n",
       "         [34, 34, 34],\n",
       "         [36, 36, 36],\n",
       "         ...,\n",
       "         [ 6,  6,  6],\n",
       "         [ 4,  4,  4],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[35, 35, 35],\n",
       "         [35, 35, 35],\n",
       "         [38, 38, 38],\n",
       "         ...,\n",
       "         [ 7,  7,  7],\n",
       "         [ 5,  5,  5],\n",
       "         [ 3,  3,  3]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[61, 61, 61],\n",
       "         [64, 64, 64],\n",
       "         [68, 68, 68],\n",
       "         ...,\n",
       "         [15, 15, 15],\n",
       "         [26, 26, 26],\n",
       "         [33, 33, 33]],\n",
       "\n",
       "        [[60, 60, 60],\n",
       "         [64, 64, 64],\n",
       "         [67, 67, 67],\n",
       "         ...,\n",
       "         [17, 17, 17],\n",
       "         [25, 25, 25],\n",
       "         [34, 34, 34]],\n",
       "\n",
       "        [[58, 58, 58],\n",
       "         [61, 61, 61],\n",
       "         [67, 67, 67],\n",
       "         ...,\n",
       "         [17, 17, 17],\n",
       "         [26, 26, 26],\n",
       "         [35, 35, 35]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[32, 32, 32],\n",
       "         [34, 34, 34],\n",
       "         [36, 36, 36],\n",
       "         ...,\n",
       "         [ 6,  6,  6],\n",
       "         [ 3,  3,  3],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[33, 33, 33],\n",
       "         [36, 36, 36],\n",
       "         [36, 36, 36],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 5,  5,  5],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[36, 36, 36],\n",
       "         [38, 38, 38],\n",
       "         [38, 38, 38],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 6,  6,  6],\n",
       "         [ 5,  5,  5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[66, 66, 66],\n",
       "         [68, 68, 68],\n",
       "         [74, 74, 74],\n",
       "         ...,\n",
       "         [18, 18, 18],\n",
       "         [25, 25, 25],\n",
       "         [34, 34, 34]],\n",
       "\n",
       "        [[65, 65, 65],\n",
       "         [68, 68, 68],\n",
       "         [72, 72, 72],\n",
       "         ...,\n",
       "         [18, 18, 18],\n",
       "         [27, 27, 27],\n",
       "         [36, 36, 36]],\n",
       "\n",
       "        [[63, 63, 63],\n",
       "         [68, 68, 68],\n",
       "         [71, 71, 71],\n",
       "         ...,\n",
       "         [18, 18, 18],\n",
       "         [30, 30, 30],\n",
       "         [38, 38, 38]]],\n",
       "\n",
       "\n",
       "       [[[33, 33, 33],\n",
       "         [35, 35, 35],\n",
       "         [38, 38, 38],\n",
       "         ...,\n",
       "         [ 4,  4,  4],\n",
       "         [ 3,  3,  3],\n",
       "         [ 1,  1,  1]],\n",
       "\n",
       "        [[35, 35, 35],\n",
       "         [36, 36, 36],\n",
       "         [37, 37, 37],\n",
       "         ...,\n",
       "         [ 7,  7,  7],\n",
       "         [ 5,  5,  5],\n",
       "         [ 2,  2,  2]],\n",
       "\n",
       "        [[36, 36, 36],\n",
       "         [36, 36, 36],\n",
       "         [38, 38, 38],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 5,  5,  5],\n",
       "         [ 4,  4,  4]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[62, 62, 62],\n",
       "         [64, 64, 64],\n",
       "         [69, 69, 69],\n",
       "         ...,\n",
       "         [20, 20, 20],\n",
       "         [28, 28, 28],\n",
       "         [34, 34, 34]],\n",
       "\n",
       "        [[61, 61, 61],\n",
       "         [65, 65, 65],\n",
       "         [70, 70, 70],\n",
       "         ...,\n",
       "         [20, 20, 20],\n",
       "         [29, 29, 29],\n",
       "         [35, 35, 35]],\n",
       "\n",
       "        [[59, 59, 59],\n",
       "         [64, 64, 64],\n",
       "         [67, 67, 67],\n",
       "         ...,\n",
       "         [21, 21, 21],\n",
       "         [29, 29, 29],\n",
       "         [37, 37, 37]]],\n",
       "\n",
       "\n",
       "       [[[43, 43, 43],\n",
       "         [43, 43, 43],\n",
       "         [47, 47, 47],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        [[45, 45, 45],\n",
       "         [45, 45, 45],\n",
       "         [49, 49, 49],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        [[47, 47, 47],\n",
       "         [47, 47, 47],\n",
       "         [50, 50, 50],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[50, 50, 50],\n",
       "         [53, 53, 53],\n",
       "         [57, 57, 57],\n",
       "         ...,\n",
       "         [28, 28, 28],\n",
       "         [35, 35, 35],\n",
       "         [38, 38, 38]],\n",
       "\n",
       "        [[51, 51, 51],\n",
       "         [55, 55, 55],\n",
       "         [57, 57, 57],\n",
       "         ...,\n",
       "         [30, 30, 30],\n",
       "         [34, 34, 34],\n",
       "         [36, 36, 36]],\n",
       "\n",
       "        [[49, 49, 49],\n",
       "         [52, 52, 52],\n",
       "         [59, 59, 59],\n",
       "         ...,\n",
       "         [29, 29, 29],\n",
       "         [31, 31, 31],\n",
       "         [34, 34, 34]]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape=(1120, 224, 224, 3)\n",
      "y_train.shape=(1120, 140)\n",
      "x_test.shape=(560, 224, 224, 3)\n",
      "y_test.shape=(560, 140)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{x_test.shape=}\")\n",
    "print(f\"{y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.paperspace.com/attention-mechanisms-in-computer-vision-cbam/\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\n",
    "from keras import backend as K\n",
    "from keras.activations import sigmoid\n",
    "\n",
    "def attach_attention_module(net, attention_module):\n",
    "  if attention_module == 'se_block': # SE_block\n",
    "    net = se_block(net)\n",
    "  elif attention_module == 'cbam_block': # CBAM_block\n",
    "    net = cbam_block(net)\n",
    "  else:\n",
    "    raise Exception(\"'{}' is not supported attention module!\".format(attention_module))\n",
    "\n",
    "  return net\n",
    "\n",
    "def se_block(input_feature, ratio=8):\n",
    "\t\"\"\"Contains the implementation of Squeeze-and-Excitation(SE) block.\n",
    "\tAs described in https://arxiv.org/abs/1709.01507.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\n",
    "\tse_feature = GlobalAveragePooling2D()(input_feature)\n",
    "\tse_feature = Reshape((1, 1, channel))(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel)\n",
    "\tse_feature = Dense(channel // ratio,\n",
    "\t\t\t\t\t   activation='relu',\n",
    "\t\t\t\t\t   kernel_initializer='he_normal',\n",
    "\t\t\t\t\t   use_bias=True,\n",
    "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel//ratio)\n",
    "\tse_feature = Dense(channel,\n",
    "\t\t\t\t\t   activation='sigmoid',\n",
    "\t\t\t\t\t   kernel_initializer='he_normal',\n",
    "\t\t\t\t\t   use_bias=True,\n",
    "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel)\n",
    "\tif K.image_data_format() == 'channels_first':\n",
    "\t\tse_feature = Permute((3, 1, 2))(se_feature)\n",
    "\n",
    "\tse_feature = multiply([input_feature, se_feature])\n",
    "\treturn se_feature\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "\t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "\tAs described in https://arxiv.org/abs/1807.06521.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
    "\tcbam_feature = spatial_attention(cbam_feature)\n",
    "\treturn cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\t\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\t\n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\t\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "\tkernel_size = 7\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tchannel = input_feature.shape[1]\n",
    "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
    "\telse:\n",
    "\t\tchannel = input_feature.shape[-1]\n",
    "\t\tcbam_feature = input_feature\n",
    "\t\n",
    "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert avg_pool.shape[-1] == 1\n",
    "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert max_pool.shape[-1] == 1\n",
    "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\tassert concat.shape[-1] == 2\n",
    "\tcbam_feature = Conv2D(filters = 1,\n",
    "\t\t\t\t\tkernel_size=kernel_size,\n",
    "\t\t\t\t\tstrides=1,\n",
    "\t\t\t\t\tpadding='same',\n",
    "\t\t\t\t\tactivation='sigmoid',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tuse_bias=False)(concat)\t\n",
    "\tassert cbam_feature.shape[-1] == 1\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\t\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\t\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages/cv2/../../../../lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting keras_applications\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages (from keras_applications) (1.23.5)\n",
      "Requirement already satisfied: h5py in /home/rs/21CS91R01/anaconda3/envs/few_shot/lib/python3.10/site-packages (from keras_applications) (3.7.0)\n",
      "Installing collected packages: keras_applications\n",
      "Successfully installed keras_applications-1.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MobileNet v1 models for Keras.\n",
    "This is a revised implementation from Somshubra Majumdar's SENet repo:\n",
    "(https://github.com/titu1994/keras-squeeze-excite-network)\n",
    "# Reference\n",
    "- [MobileNets: Efficient Convolutional Neural Networks for\n",
    "   Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf))\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import Conv2D\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils import conv_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "# from keras.engine.topology import get_source_inputs\n",
    "from keras.layers import InputSpec\n",
    "from keras.applications import imagenet_utils\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def relu6(x):\n",
    "    return K.relu(x, max_value=6)\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    \"\"\"Preprocesses a numpy array encoding a batch of images.\n",
    "    # Arguments\n",
    "        x: a 4D numpy array consists of RGB values within [0, 255].\n",
    "    # Returns\n",
    "        Preprocessed array.\n",
    "    \"\"\"\n",
    "    return imagenet_utils.preprocess_input(x, mode='tf')\n",
    "\n",
    "\n",
    "class DepthwiseConv2D(Conv2D):\n",
    "    \"\"\"Depthwise separable 2D convolution.\n",
    "    Depthwise Separable convolutions consists in performing\n",
    "    just the first step in a depthwise spatial convolution\n",
    "    (which acts on each input channel separately).\n",
    "    The `depth_multiplier` argument controls how many\n",
    "    output channels are generated per input channel in the depthwise step.\n",
    "    # Arguments\n",
    "        kernel_size: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "            Specifying any stride value != 1 is incompatible with specifying\n",
    "            any `dilation_rate` value != 1.\n",
    "        padding: one of `'valid'` or `'same'` (case-insensitive).\n",
    "        depth_multiplier: The number of depthwise convolution output channels\n",
    "            for each input channel.\n",
    "            The total number of depthwise convolution output\n",
    "            channels will be equal to `filters_in * depth_multiplier`.\n",
    "        data_format: A string,\n",
    "            one of `channels_last` (default) or `channels_first`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `channels_last` corresponds to inputs with shape\n",
    "            `(batch, height, width, channels)` while `channels_first`\n",
    "            corresponds to inputs with shape\n",
    "            `(batch, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be 'channels_last'.\n",
    "        activation: Activation function to use\n",
    "            (see [activations](../activations.md)).\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. 'linear' activation: `a(x) = x`).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        depthwise_initializer: Initializer for the depthwise kernel matrix\n",
    "            (see [initializers](../initializers.md)).\n",
    "        bias_initializer: Initializer for the bias vector\n",
    "            (see [initializers](../initializers.md)).\n",
    "        depthwise_regularizer: Regularizer function applied to\n",
    "            the depthwise kernel matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        bias_regularizer: Regularizer function applied to the bias vector\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        activity_regularizer: Regularizer function applied to\n",
    "            the output of the layer (its 'activation').\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        depthwise_constraint: Constraint function applied to\n",
    "            the depthwise kernel matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        bias_constraint: Constraint function applied to the bias vector\n",
    "            (see [constraints](../constraints.md)).\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `[batch, channels, rows, cols]` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `[batch, rows, cols, channels]` if data_format='channels_last'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `[batch, filters, new_rows, new_cols]` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `[batch, new_rows, new_cols, filters]` if data_format='channels_last'.\n",
    "        `rows` and `cols` values might have changed due to padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 depth_multiplier=1,\n",
    "                 data_format=None,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 depthwise_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 depthwise_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 depthwise_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(DepthwiseConv2D, self).__init__(\n",
    "            filters=None,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs)\n",
    "        self.depth_multiplier = depth_multiplier\n",
    "        self.depthwise_initializer = initializers.get(depthwise_initializer)\n",
    "        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n",
    "        self.depthwise_constraint = constraints.get(depthwise_constraint)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) < 4:\n",
    "            raise ValueError('Inputs to `DepthwiseConv2D` should have rank 4. '\n",
    "                             'Received input shape:', str(input_shape))\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = 3\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs to '\n",
    "                             '`DepthwiseConv2D` '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = int(input_shape[channel_axis])\n",
    "        depthwise_kernel_shape = (self.kernel_size[0],\n",
    "                                  self.kernel_size[1],\n",
    "                                  input_dim,\n",
    "                                  self.depth_multiplier)\n",
    "\n",
    "        self.depthwise_kernel = self.add_weight(\n",
    "            shape=depthwise_kernel_shape,\n",
    "            initializer=self.depthwise_initializer,\n",
    "            name='depthwise_kernel',\n",
    "            regularizer=self.depthwise_regularizer,\n",
    "            constraint=self.depthwise_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(input_dim * self.depth_multiplier,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        outputs = K.depthwise_conv2d(\n",
    "            inputs,\n",
    "            self.depthwise_kernel,\n",
    "            strides=self.strides,\n",
    "            padding=self.padding,\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            data_format=self.data_format)\n",
    "\n",
    "        if self.bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            rows = input_shape[2]\n",
    "            cols = input_shape[3]\n",
    "            out_filters = input_shape[1] * self.depth_multiplier\n",
    "        elif self.data_format == 'channels_last':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "            out_filters = input_shape[3] * self.depth_multiplier\n",
    "\n",
    "        rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n",
    "                                             self.padding,\n",
    "                                             self.strides[0])\n",
    "        cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\n",
    "                                             self.padding,\n",
    "                                             self.strides[1])\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            return (input_shape[0], out_filters, rows, cols)\n",
    "        elif self.data_format == 'channels_last':\n",
    "            return (input_shape[0], rows, cols, out_filters)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DepthwiseConv2D, self).get_config()\n",
    "        config.pop('filters')\n",
    "        config.pop('kernel_initializer')\n",
    "        config.pop('kernel_regularizer')\n",
    "        config.pop('kernel_constraint')\n",
    "        config['depth_multiplier'] = self.depth_multiplier\n",
    "        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n",
    "        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n",
    "        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n",
    "        return config\n",
    "\n",
    "\n",
    "def MobileNet(input_shape=None,\n",
    "                alpha=1.0,\n",
    "                depth_multiplier=1,\n",
    "                dropout=1e-3,\n",
    "                include_top=True,\n",
    "                weights=None,\n",
    "                input_tensor=None,\n",
    "                pooling=None,\n",
    "                classes=1000,\n",
    "\t\t\t    attention_module=None):\n",
    "    \"\"\"Instantiates the SE-MobileNet architecture.\n",
    "    Note that only TensorFlow is supported for now,\n",
    "    therefore it only works with the data format\n",
    "    `image_data_format='channels_last'` in your Keras config\n",
    "    at `~/.keras/keras.json`.\n",
    "    To load a MobileNet model via `load_model`, import the custom\n",
    "    objects `relu6` and `DepthwiseConv2D` and pass them to the\n",
    "    `custom_objects` parameter.\n",
    "    E.g.\n",
    "    model = load_model('mobilenet.h5', custom_objects={\n",
    "                       'relu6': mobilenet.relu6,\n",
    "                       'DepthwiseConv2D': mobilenet.DepthwiseConv2D})\n",
    "    # Arguments\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or (3, 224, 224) (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        alpha: controls the width of the network.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        depth_multiplier: depth multiplier for depthwise convolution\n",
    "            (also called the resolution multiplier)\n",
    "        dropout: dropout rate\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: `None` (random initialization) or\n",
    "            `imagenet` (ImageNet weights)\n",
    "        input_tensor: optional Keras tensor (i.e. output of\n",
    "            `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model\n",
    "                will be the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a\n",
    "                2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "        RuntimeError: If attempting to run this model with a\n",
    "            backend that does not support separable convolutions.\n",
    "    \"\"\"\n",
    "\n",
    "    if K.backend() != 'tensorflow':\n",
    "        raise RuntimeError('Only TensorFlow backend is currently supported, '\n",
    "                           'as other backends do not support '\n",
    "                           'depthwise convolution.')\n",
    "\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as ImageNet with `include_top` '\n",
    "                         'as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape and default size.\n",
    "    if input_shape is None:\n",
    "        default_size = 224\n",
    "    else:\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "        else:\n",
    "            rows = input_shape[0]\n",
    "            cols = input_shape[1]\n",
    "\n",
    "        if rows == cols and rows in [128, 160, 192, 224]:\n",
    "            default_size = rows\n",
    "        else:\n",
    "            default_size = 224\n",
    "\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=default_size,\n",
    "                                      min_size=32,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        row_axis, col_axis = (0, 1)\n",
    "    else:\n",
    "        row_axis, col_axis = (1, 2)\n",
    "    rows = input_shape[row_axis]\n",
    "    cols = input_shape[col_axis]\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = _conv_block(img_input, 32, alpha, strides=(2, 2))\n",
    "    x = _depthwise_conv_block(x, 64, alpha, depth_multiplier, block_id=1, attention_module=attention_module)\n",
    "\n",
    "    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=2, attention_module=attention_module)\n",
    "    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, block_id=3, attention_module=attention_module)\n",
    "\n",
    "    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=4, attention_module=attention_module)\n",
    "    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, block_id=5, attention_module=attention_module)\n",
    "\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=6, attention_module=attention_module)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=7, attention_module=attention_module)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=8, attention_module=attention_module)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=9, attention_module=attention_module)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=10, attention_module=attention_module)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=11, attention_module=attention_module)\n",
    "\n",
    "    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=12, attention_module=attention_module)\n",
    "    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, block_id=13, attention_module=attention_module)\n",
    "\n",
    "    if include_top:\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            shape = (int(1024 * alpha), 1, 1)\n",
    "        else:\n",
    "            shape = (1, 1, int(1024 * alpha))\n",
    "\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Reshape(shape, name='reshape_n_1')(x)\n",
    "        x = Dropout(dropout, name='dropout')(x)\n",
    "        x = Conv2D(classes, (1, 1),\n",
    "                   padding='same', name='conv_preds')(x)\n",
    "        x = Activation('softmax', name='act_softmax')(x)\n",
    "        x = Reshape((classes,), name='reshape_final')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    # if input_tensor is not None:\n",
    "    #     inputs = get_source_inputs(input_tensor)\n",
    "    # else:\n",
    "    inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='se_mobilenet_%0.2f_%s' % (alpha, rows))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n",
    "    \"\"\"Adds an initial convolution layer (with batch normalization and relu6).\n",
    "    # Arguments\n",
    "        inputs: Input tensor of shape `(rows, cols, 3)`\n",
    "            (with `channels_last` data format) or\n",
    "            (3, rows, cols) (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(224, 224, 3)` would be one valid value.\n",
    "        filters: Integer, the dimensionality of the output space\n",
    "            (i.e. the number output of filters in the convolution).\n",
    "        alpha: controls the width of the network.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "            Specifying any stride value != 1 is incompatible with specifying\n",
    "            any `dilation_rate` value != 1.\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if data_format='channels_last'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n",
    "        `rows` and `cols` values might have changed due to stride.\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    filters = int(filters * alpha)\n",
    "    x = Conv2D(filters, kernel,\n",
    "               padding='same',\n",
    "               use_bias=False,\n",
    "               strides=strides,\n",
    "               name='conv1')(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis, name='conv1_bn')(x)\n",
    "    return Activation(relu6, name='conv1_relu')(x)\n",
    "\n",
    "\n",
    "def _depthwise_conv_block(inputs, pointwise_conv_filters, alpha,\n",
    "                          depth_multiplier=1, strides=(1, 1), block_id=1, attention_module=None):\n",
    "    \"\"\"Adds a depthwise convolution block.\n",
    "    A depthwise convolution block consists of a depthwise conv,\n",
    "    batch normalization, relu6, pointwise convolution,\n",
    "    batch normalization and relu6 activation.\n",
    "    # Arguments\n",
    "        inputs: Input tensor of shape `(rows, cols, channels)`\n",
    "            (with `channels_last` data format) or\n",
    "            (channels, rows, cols) (with `channels_first` data format).\n",
    "        pointwise_conv_filters: Integer, the dimensionality of the output space\n",
    "            (i.e. the number output of filters in the pointwise convolution).\n",
    "        alpha: controls the width of the network.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        depth_multiplier: The number of depthwise convolution output channels\n",
    "            for each input channel.\n",
    "            The total number of depthwise convolution output\n",
    "            channels will be equal to `filters_in * depth_multiplier`.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "            Specifying any stride value != 1 is incompatible with specifying\n",
    "            any `dilation_rate` value != 1.\n",
    "        block_id: Integer, a unique identification designating the block number.\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(batch, channels, rows, cols)` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `(batch, rows, cols, channels)` if data_format='channels_last'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(batch, filters, new_rows, new_cols)` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `(batch, new_rows, new_cols, filters)` if data_format='channels_last'.\n",
    "        `rows` and `cols` values might have changed due to stride.\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    pointwise_conv_filters = int(pointwise_conv_filters * alpha)\n",
    "\n",
    "    x = DepthwiseConv2D((3, 3),\n",
    "                        padding='same',\n",
    "                        depth_multiplier=depth_multiplier,\n",
    "                        strides=strides,\n",
    "                        use_bias=False,\n",
    "                        name='conv_dw_%d' % block_id)(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis, name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)\n",
    "\n",
    "    x = Conv2D(pointwise_conv_filters, (1, 1),\n",
    "               padding='same',\n",
    "               use_bias=False,\n",
    "               strides=(1, 1),\n",
    "               name='conv_pw_%d' % block_id)(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='conv_pw_%d_bn' % block_id)(x)\n",
    "    x = Activation(relu6, name='conv_pw_%d_relu' % block_id)(x)\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\t\t\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet v1\n",
    "This is a revised implementation from Cifar10 ResNet example in Keras:\n",
    "(https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py)\n",
    "[a] Deep Residual Learning for Image Recognition\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=140, attention_module=None):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            # attention_module\n",
    "            if attention_module is not None:\n",
    "                y = attach_attention_module(y, attention_module)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "data_augmentation = False\n",
    "num_classes = 10\n",
    "subtract_pixel_mean = True  # Subtracting pixel mean improves accuracy\n",
    "base_model = 'resnet20'\n",
    "# Choose what attention_module to use: cbam_block / se_block / None\n",
    "attention_module = 'cbam_block'\n",
    "model_type = base_model if attention_module==None else base_model+'_'+attention_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.engine.topology'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregularizers\u001b[39;00m \u001b[39mimport\u001b[39;00m l2\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_file\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtopology\u001b[39;00m \u001b[39mimport\u001b[39;00m get_source_inputs\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimagenet_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m _obtain_input_shape\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimagenet_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m decode_predictions\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine.topology'"
     ]
    }
   ],
   "source": [
    "'''DenseNet models for Keras.\n",
    "This is a revised implementation from Somshubra Majumdar's SENet repo:\n",
    "(https://github.com/titu1994/keras-squeeze-excite-network)\n",
    "# Reference\n",
    "- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)\n",
    "- [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/pdf/1611.09326.pdf)\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers.pooling import AveragePooling2D, MaxPooling2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_input(x, data_format=None):\n",
    "    \"\"\"Preprocesses a tensor encoding a batch of images.\n",
    "\n",
    "    # Arguments\n",
    "        x: input Numpy tensor, 4D.\n",
    "        data_format: data format of the image tensor.\n",
    "\n",
    "    # Returns\n",
    "        Preprocessed tensor.\n",
    "    \"\"\"\n",
    "    if data_format is None:\n",
    "        data_format = K.image_data_format()\n",
    "    assert data_format in {'channels_last', 'channels_first'}\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        if x.ndim == 3:\n",
    "            # 'RGB'->'BGR'\n",
    "            x = x[::-1, ...]\n",
    "            # Zero-center by mean pixel\n",
    "            x[0, :, :] -= 103.939\n",
    "            x[1, :, :] -= 116.779\n",
    "            x[2, :, :] -= 123.68\n",
    "        else:\n",
    "            x = x[:, ::-1, ...]\n",
    "            x[:, 0, :, :] -= 103.939\n",
    "            x[:, 1, :, :] -= 116.779\n",
    "            x[:, 2, :, :] -= 123.68\n",
    "    else:\n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[..., ::-1]\n",
    "        # Zero-center by mean pixel\n",
    "        x[..., 0] -= 103.939\n",
    "        x[..., 1] -= 116.779\n",
    "        x[..., 2] -= 123.68\n",
    "\n",
    "    x *= 0.017  # scale values\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def DenseNet(input_shape=None,\n",
    "               depth=19,\n",
    "               nb_dense_block=3,\n",
    "               growth_rate=12,\n",
    "               nb_filter=-1,\n",
    "               nb_layers_per_block=-1,\n",
    "               bottleneck=False,\n",
    "               reduction=0.0,\n",
    "               dropout_rate=0.0,\n",
    "               weight_decay=1e-4,\n",
    "               subsample_initial_block=False,\n",
    "               include_top=True,\n",
    "               weights=None,\n",
    "               input_tensor=None,\n",
    "               classes=10,\n",
    "               activation='softmax',\n",
    "\t\t\t   attention_module=None):\n",
    "    '''Instantiate the DenseNet architecture\n",
    "        # Arguments\n",
    "            input_shape: optional shape tuple, only to be specified\n",
    "                if `include_top` is False (otherwise the input shape\n",
    "                has to be `(32, 32, 3)` (with `channels_last` dim ordering)\n",
    "                or `(3, 32, 32)` (with `channels_first` dim ordering).\n",
    "                It should have exactly 3 inputs channels,\n",
    "                and width and height should be no smaller than 8.\n",
    "                E.g. `(200, 200, 3)` would be one valid value.\n",
    "            depth: number or layers in the DenseNet\n",
    "            nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
    "            growth_rate: number of filters to add per dense block\n",
    "            nb_filter: initial number of filters. -1 indicates initial\n",
    "                number of filters is 2 * growth_rate\n",
    "            nb_layers_per_block: number of layers in each dense block.\n",
    "                Can be a -1, positive integer or a list.\n",
    "                If -1, calculates nb_layer_per_block from the network depth.\n",
    "                If positive integer, a set number of layers per dense block.\n",
    "                If list, nb_layer is used as provided. Note that list size must\n",
    "                be (nb_dense_block + 1)\n",
    "            bottleneck: flag to add bottleneck blocks in between dense blocks\n",
    "            reduction: reduction factor of transition blocks.\n",
    "                Note : reduction value is inverted to compute compression.\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay rate\n",
    "            subsample_initial_block: Set to True to subsample the initial convolution and\n",
    "                add a MaxPool2D before the dense blocks are added.\n",
    "            include_top: whether to include the fully-connected\n",
    "                layer at the top of the network.\n",
    "            weights: one of `None` (random initialization) or\n",
    "                'imagenet' (pre-training on ImageNet)..\n",
    "            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "                to use as image input for the model.\n",
    "            classes: optional number of classes to classify images\n",
    "                into, only to be specified if `include_top` is True, and\n",
    "                if no `weights` argument is specified.\n",
    "            activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n",
    "                Note that if sigmoid is used, classes must be 1.\n",
    "        # Returns\n",
    "            A Keras model instance.\n",
    "        '''\n",
    "\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `cifar10` '\n",
    "                         '(pre-training on CIFAR-10).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as ImageNet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    if activation not in ['softmax', 'sigmoid']:\n",
    "        raise ValueError('activation must be one of \"softmax\" or \"sigmoid\"')\n",
    "\n",
    "    if activation == 'sigmoid' and classes != 1:\n",
    "        raise ValueError('sigmoid activation can only be used when classes = 1')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=32,\n",
    "                                      min_size=8,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = __create_dense_net(classes, img_input, include_top, depth, nb_dense_block,\n",
    "                           growth_rate, nb_filter, nb_layers_per_block, bottleneck, reduction,\n",
    "                           dropout_rate, weight_decay, subsample_initial_block, activation, attention_module)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='se-densenet')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def DenseNetImageNet121(input_shape=None,\n",
    "                          bottleneck=True,\n",
    "                          reduction=0.5,\n",
    "                          dropout_rate=0.0,\n",
    "                          weight_decay=1e-4,\n",
    "                          include_top=True,\n",
    "                          weights=None,\n",
    "                          input_tensor=None,\n",
    "                          classes=1000,\n",
    "                          activation='softmax',\n",
    "\t\t\t\t\t\t  attention_module=None):\n",
    "    return DenseNet(input_shape, depth=121, nb_dense_block=4, growth_rate=32, nb_filter=64,\n",
    "                      nb_layers_per_block=[6, 12, 24, 16], bottleneck=bottleneck, reduction=reduction,\n",
    "                      dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n",
    "                      include_top=include_top, weights=weights, input_tensor=input_tensor,\n",
    "                      classes=classes, activation=activation, attention_module=attention_module)\n",
    "\n",
    "\n",
    "def DenseNetImageNet169(input_shape=None,\n",
    "                          bottleneck=True,\n",
    "                          reduction=0.5,\n",
    "                          dropout_rate=0.0,\n",
    "                          weight_decay=1e-4,\n",
    "                          include_top=True,\n",
    "                          weights=None,\n",
    "                          input_tensor=None,\n",
    "                          classes=1000,\n",
    "                          activation='softmax',\n",
    "\t\t\t\t\t\t  attention_module=None):\n",
    "    return DenseNet(input_shape, depth=169, nb_dense_block=4, growth_rate=32, nb_filter=64,\n",
    "                      nb_layers_per_block=[6, 12, 32, 32], bottleneck=bottleneck, reduction=reduction,\n",
    "                      dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n",
    "                      include_top=include_top, weights=weights, input_tensor=input_tensor,\n",
    "                      classes=classes, activation=activation, attention_module=attention_module)\n",
    "\n",
    "\n",
    "def DenseNetImageNet201(input_shape=None,\n",
    "                          bottleneck=True,\n",
    "                          reduction=0.5,\n",
    "                          dropout_rate=0.0,\n",
    "                          weight_decay=1e-4,\n",
    "                          include_top=True,\n",
    "                          weights=None,\n",
    "                          input_tensor=None,\n",
    "                          classes=1000,\n",
    "                          activation='softmax',\n",
    "\t\t\t\t\t\t  attention_module=None):\n",
    "    return DenseNet(input_shape, depth=201, nb_dense_block=4, growth_rate=32, nb_filter=64,\n",
    "                      nb_layers_per_block=[6, 12, 48, 32], bottleneck=bottleneck, reduction=reduction,\n",
    "                      dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n",
    "                      include_top=include_top, weights=weights, input_tensor=input_tensor,\n",
    "                      classes=classes, activation=activation, attention_module=attention_module)\n",
    "\n",
    "\n",
    "def DenseNetImageNet264(input_shape=None,\n",
    "                          bottleneck=True,\n",
    "                          reduction=0.5,\n",
    "                          dropout_rate=0.0,\n",
    "                          weight_decay=1e-4,\n",
    "                          include_top=True,\n",
    "                          weights=None,\n",
    "                          input_tensor=None,\n",
    "                          classes=1000,\n",
    "                          activation='softmax',\n",
    "\t\t\t\t\t\t  attention_module=None):\n",
    "    return DenseNet(input_shape, depth=201, nb_dense_block=4, growth_rate=32, nb_filter=64,\n",
    "                      nb_layers_per_block=[6, 12, 64, 48], bottleneck=bottleneck, reduction=reduction,\n",
    "                      dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n",
    "                      include_top=include_top, weights=weights, input_tensor=input_tensor,\n",
    "                      classes=classes, activation=activation, attention_module=attention_module)\n",
    "\n",
    "\n",
    "def DenseNetImageNet161(input_shape=None,\n",
    "                          bottleneck=True,\n",
    "                          reduction=0.5,\n",
    "                          dropout_rate=0.0,\n",
    "                          weight_decay=1e-4,\n",
    "                          include_top=True,\n",
    "                          weights=None,\n",
    "                          input_tensor=None,\n",
    "                          classes=1000,\n",
    "                          activation='softmax',\n",
    "\t\t\t\t\t\t  attention_module=None):\n",
    "    return DenseNet(input_shape, depth=161, nb_dense_block=4, growth_rate=48, nb_filter=96,\n",
    "                      nb_layers_per_block=[6, 12, 36, 24], bottleneck=bottleneck, reduction=reduction,\n",
    "                      dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n",
    "                      include_top=include_top, weights=weights, input_tensor=input_tensor,\n",
    "                      classes=classes, activation=activation, attention_module=attention_module)\n",
    "\n",
    "\n",
    "def __conv_block(ip, nb_filter, bottleneck=False, dropout_rate=None, weight_decay=1e-4):\n",
    "    ''' Apply BatchNorm, Relu, 3x3 Conv2D, optional bottleneck block and dropout\n",
    "    Args:\n",
    "        ip: Input keras tensor\n",
    "        nb_filter: number of filters\n",
    "        bottleneck: add bottleneck block\n",
    "        dropout_rate: dropout rate\n",
    "        weight_decay: weight decay factor\n",
    "    Returns: keras tensor with batch_norm, relu and convolution2d added (optional bottleneck)\n",
    "    '''\n",
    "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(ip)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if bottleneck:\n",
    "        inter_channel = nb_filter * 4  # Obtained from https://github.com/liuzhuang13/DenseNet/blob/master/densenet.lua\n",
    "\n",
    "        x = Conv2D(inter_channel, (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(weight_decay))(x)\n",
    "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(nb_filter, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def __dense_block(x, nb_layers, nb_filter, growth_rate, bottleneck=False, dropout_rate=None, weight_decay=1e-4,\n",
    "                  grow_nb_filters=True, return_concat_list=False, attention_module=None):\n",
    "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
    "    Args:\n",
    "        x: keras tensor\n",
    "        nb_layers: the number of layers of conv_block to append to the model.\n",
    "        nb_filter: number of filters\n",
    "        growth_rate: growth rate\n",
    "        bottleneck: bottleneck block\n",
    "        dropout_rate: dropout rate\n",
    "        weight_decay: weight decay factor\n",
    "        grow_nb_filters: flag to decide to allow number of filters to grow\n",
    "        return_concat_list: return the list of feature maps along with the actual output\n",
    "    Returns: keras tensor with nb_layers of conv_block appended\n",
    "    '''\n",
    "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        cb = __conv_block(x, growth_rate, bottleneck, dropout_rate, weight_decay)\n",
    "        x_list.append(cb)\n",
    "\n",
    "        x = concatenate([x, cb], axis=concat_axis)\n",
    "\n",
    "        if grow_nb_filters:\n",
    "            nb_filter += growth_rate\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    if return_concat_list:\n",
    "        return x, nb_filter, x_list\n",
    "    else:\n",
    "        return x, nb_filter\n",
    "\n",
    "\n",
    "def __transition_block(ip, nb_filter, compression=1.0, weight_decay=1e-4, attention_module=None):\n",
    "    ''' Apply BatchNorm, Relu 1x1, Conv2D, optional compression, dropout and Maxpooling2D\n",
    "    Args:\n",
    "        ip: keras tensor\n",
    "        nb_filter: number of filters\n",
    "        compression: calculated as 1 - reduction. Reduces the number of feature maps\n",
    "                    in the transition block.\n",
    "        dropout_rate: dropout rate\n",
    "        weight_decay: weight decay factor\n",
    "    Returns: keras tensor, after applying batch_norm, relu-conv, dropout, maxpool\n",
    "    '''\n",
    "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(ip)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
    "               kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def __create_dense_net(nb_classes, img_input, include_top, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=-1,\n",
    "                       nb_layers_per_block=-1, bottleneck=False, reduction=0.0, dropout_rate=None, weight_decay=1e-4,\n",
    "                       subsample_initial_block=False, activation='softmax', attention_module=None):\n",
    "    ''' Build the DenseNet model\n",
    "    Args:\n",
    "        nb_classes: number of classes\n",
    "        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
    "        include_top: flag to include the final Dense layer\n",
    "        depth: number or layers\n",
    "        nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
    "        growth_rate: number of filters to add per dense block\n",
    "        nb_filter: initial number of filters. Default -1 indicates initial number of filters is 2 * growth_rate\n",
    "        nb_layers_per_block: number of layers in each dense block.\n",
    "                Can be a -1, positive integer or a list.\n",
    "                If -1, calculates nb_layer_per_block from the depth of the network.\n",
    "                If positive integer, a set number of layers per dense block.\n",
    "                If list, nb_layer is used as provided. Note that list size must\n",
    "                be (nb_dense_block + 1)\n",
    "        bottleneck: add bottleneck blocks\n",
    "        reduction: reduction factor of transition blocks. Note : reduction value is inverted to compute compression\n",
    "        dropout_rate: dropout rate\n",
    "        weight_decay: weight decay rate\n",
    "        subsample_initial_block: Set to True to subsample the initial convolution and\n",
    "                add a MaxPool2D before the dense blocks are added.\n",
    "        subsample_initial:\n",
    "        activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n",
    "                Note that if sigmoid is used, classes must be 1.\n",
    "    Returns: keras tensor with nb_layers of conv_block appended\n",
    "    '''\n",
    "\n",
    "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    if reduction != 0.0:\n",
    "        assert reduction <= 1.0 and reduction > 0.0, 'reduction value must lie between 0.0 and 1.0'\n",
    "\n",
    "    # layers in each dense block\n",
    "    if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n",
    "        nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n",
    "\n",
    "        assert len(nb_layers) == (nb_dense_block), 'If list, nb_layer is used as provided. ' \\\n",
    "                                                   'Note that list size must be (nb_dense_block)'\n",
    "        final_nb_layer = nb_layers[-1]\n",
    "        nb_layers = nb_layers[:-1]\n",
    "    else:\n",
    "        if nb_layers_per_block == -1:\n",
    "            assert (depth - 4) % 3 == 0, 'Depth must be 3 N + 4 if nb_layers_per_block == -1'\n",
    "            count = int((depth - 4) / 3)\n",
    "            nb_layers = [count for _ in range(nb_dense_block)]\n",
    "            final_nb_layer = count\n",
    "        else:\n",
    "            final_nb_layer = nb_layers_per_block\n",
    "            nb_layers = [nb_layers_per_block] * nb_dense_block\n",
    "\n",
    "    # compute initial nb_filter if -1, else accept users initial nb_filter\n",
    "    if nb_filter <= 0:\n",
    "        nb_filter = 2 * growth_rate\n",
    "\n",
    "    # compute compression factor\n",
    "    compression = 1.0 - reduction\n",
    "\n",
    "    # Initial convolution\n",
    "    if subsample_initial_block:\n",
    "        initial_kernel = (7, 7)\n",
    "        initial_strides = (2, 2)\n",
    "    else:\n",
    "        initial_kernel = (3, 3)\n",
    "        initial_strides = (1, 1)\n",
    "\n",
    "    x = Conv2D(nb_filter, initial_kernel, kernel_initializer='he_normal', padding='same',\n",
    "               strides=initial_strides, use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
    "\n",
    "    if subsample_initial_block:\n",
    "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter, growth_rate, bottleneck=bottleneck,\n",
    "                                     dropout_rate=dropout_rate, weight_decay=weight_decay, attention_module=attention_module)\n",
    "        # add transition_block\n",
    "        x = __transition_block(x, nb_filter, compression=compression,\n",
    "\t\t\t\t\t\t\t   weight_decay=weight_decay, attention_module=attention_module)\n",
    "        nb_filter = int(nb_filter * compression)\n",
    "\n",
    "    # The last dense_block does not have a transition_block\n",
    "    x, nb_filter = __dense_block(x, final_nb_layer, nb_filter, growth_rate, bottleneck=bottleneck,\n",
    "                                 dropout_rate=dropout_rate, weight_decay=weight_decay, attention_module=attention_module)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = Dense(nb_classes, activation=activation)(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import warnings\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = ''\n",
    "WEIGHTS_PATH_NO_TOP = ''\n",
    "\n",
    "\n",
    "def _conv2d_bn(x,\n",
    "               filters,\n",
    "               num_row,\n",
    "               num_col,\n",
    "               padding='same',\n",
    "               strides=(1, 1),\n",
    "               name=None):\n",
    "    \"\"\"Utility function to apply conv + BN.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: filters in `Conv2D`.\n",
    "        num_row: height of the convolution kernel.\n",
    "        num_col: width of the convolution kernel.\n",
    "        padding: padding mode in `Conv2D`.\n",
    "        strides: strides in `Conv2D`.\n",
    "        name: name of the ops; will become `name + '_conv'`\n",
    "            for the convolution and `name + '_bn'` for the\n",
    "            batch norm layer.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "    \"\"\"\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        bn_axis = 1\n",
    "    else:\n",
    "        bn_axis = 3\n",
    "    x = Conv2D(\n",
    "        filters, (num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
    "    x = Activation('relu', name=name)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def InceptionV3(include_top=True,\n",
    "                  weights=None,\n",
    "                  input_tensor=None,\n",
    "                  input_shape=None,\n",
    "                  pooling=None,\n",
    "                  classes=1000,\n",
    "                  attention_module=None):\n",
    "    \"\"\"Instantiates the Squeeze and Excite Inception v3 architecture.\n",
    "\n",
    "    # Arguments\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization)\n",
    "            or \"imagenet\" (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(299, 299, 3)` (with `channels_last` data format)\n",
    "            or `(3, 299, 299)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 139.\n",
    "            E.g. `(150, 150, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(\n",
    "        input_shape,\n",
    "        default_size=299,\n",
    "        min_size=139,\n",
    "        data_format=K.image_data_format(),\n",
    "        require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "\n",
    "    x = _conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n",
    "    x = _conv2d_bn(x, 32, 3, 3, padding='valid')\n",
    "    x = _conv2d_bn(x, 64, 3, 3)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = _conv2d_bn(x, 80, 1, 1, padding='valid')\n",
    "    x = _conv2d_bn(x, 192, 3, 3, padding='valid')\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # mixed 0, 1, 2: 35 x 35 x 256\n",
    "    branch1x1 = _conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = _conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = _conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = _conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = _conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = _conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = _conv2d_bn(branch_pool, 32, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed0')\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    # mixed 1: 35 x 35 x 256\n",
    "    branch1x1 = _conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = _conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = _conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = _conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = _conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = _conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = _conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed1')\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    # mixed 2: 35 x 35 x 256\n",
    "    branch1x1 = _conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = _conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = _conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = _conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = _conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = _conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = _conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed2')\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    # mixed 3: 17 x 17 x 768\n",
    "    branch3x3 = _conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch3x3dbl = _conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = _conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = _conv2d_bn(\n",
    "        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = layers.concatenate(\n",
    "        [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed3')\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    # mixed 4: 17 x 17 x 768\n",
    "    branch1x1 = _conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = _conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7 = _conv2d_bn(branch7x7, 128, 1, 7)\n",
    "    branch7x7 = _conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = _conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7dbl = _conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = _conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "    branch7x7dbl = _conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = _conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = _conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed4')\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    # mixed 5, 6: 17 x 17 x 768\n",
    "    for i in range(2):\n",
    "        branch1x1 = _conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "        branch7x7 = _conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7 = _conv2d_bn(branch7x7, 160, 1, 7)\n",
    "        branch7x7 = _conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "        branch7x7dbl = _conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7dbl = _conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = _conv2d_bn(branch7x7dbl, 160, 1, 7)\n",
    "        branch7x7dbl = _conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = _conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "        branch_pool = AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), padding='same')(x)\n",
    "        branch_pool = _conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = layers.concatenate(\n",
    "            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "            axis=channel_axis,\n",
    "            name='mixed' + str(5 + i))\n",
    "\n",
    "        # attention_module\n",
    "        if attention_module is not None:\n",
    "            x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    # mixed 7: 17 x 17 x 768\n",
    "    branch1x1 = _conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = _conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7 = _conv2d_bn(branch7x7, 192, 1, 7)\n",
    "    branch7x7 = _conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = _conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7dbl = _conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = _conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "    branch7x7dbl = _conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = _conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = _conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed7')\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    # mixed 8: 8 x 8 x 1280\n",
    "    branch3x3 = _conv2d_bn(x, 192, 1, 1)\n",
    "    branch3x3 = _conv2d_bn(branch3x3, 320, 3, 3,\n",
    "                           strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch7x7x3 = _conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7x3 = _conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "    branch7x7x3 = _conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "    branch7x7x3 = _conv2d_bn(\n",
    "        branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = layers.concatenate(\n",
    "        [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name='mixed8')\n",
    "\n",
    "    # attention_module\n",
    "    if attention_module is not None:\n",
    "        x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    # mixed 9: 8 x 8 x 2048\n",
    "    for i in range(2):\n",
    "        branch1x1 = _conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "        branch3x3 = _conv2d_bn(x, 384, 1, 1)\n",
    "        branch3x3_1 = _conv2d_bn(branch3x3, 384, 1, 3)\n",
    "        branch3x3_2 = _conv2d_bn(branch3x3, 384, 3, 1)\n",
    "        branch3x3 = layers.concatenate(\n",
    "            [branch3x3_1, branch3x3_2], axis=channel_axis, name='mixed9_' + str(i))\n",
    "\n",
    "        branch3x3dbl = _conv2d_bn(x, 448, 1, 1)\n",
    "        branch3x3dbl = _conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "        branch3x3dbl_1 = _conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "        branch3x3dbl_2 = _conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "        branch3x3dbl = layers.concatenate(\n",
    "            [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n",
    "\n",
    "        branch_pool = AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), padding='same')(x)\n",
    "        branch_pool = _conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = layers.concatenate(\n",
    "            [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
    "            axis=channel_axis,\n",
    "            name='mixed' + str(9 + i))\n",
    "\n",
    "        # attention_module\n",
    "        if attention_module is not None:\n",
    "            x = attach_attention_module(x, attention_module)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='inception_v3')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Input\n",
    "depth = 20 # For ResNet, specify the depth (e.g. ResNet50: depth=50)\n",
    "model = InceptionV3(input_shape=(224,224,3), classes = 140, attention_module=attention_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 111, 111, 32  864         ['input_6[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 111, 111, 32  96         ['conv2d_94[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " activation_80 (Activation)     (None, 111, 111, 32  0           ['batch_normalization_84[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 109, 109, 32  9216        ['activation_80[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 109, 109, 32  96         ['conv2d_95[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 109, 109, 32  0           ['batch_normalization_85[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 109, 109, 64  18432       ['activation_81[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 109, 109, 64  192        ['conv2d_96[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 109, 109, 64  0           ['batch_normalization_86[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 54, 54, 64)   0           ['activation_82[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 54, 54, 80)   5120        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 54, 54, 80)  240         ['conv2d_97[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 54, 54, 80)   0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 52, 52, 192)  138240      ['activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 52, 52, 192)  576        ['conv2d_98[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 52, 52, 192)  0           ['batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0          ['activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 25, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 25, 25, 64)  192         ['conv2d_102[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 25, 25, 48)   9216        ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 25, 25, 48)  144         ['conv2d_100[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 25, 25, 96)  288         ['conv2d_103[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 25, 25, 48)   0           ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (AveragePo  (None, 25, 25, 192)  0          ['max_pooling2d_1[0][0]']        \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 25, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)            (None, 25, 25, 32)   6144        ['average_pooling2d_5[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 25, 25, 64)  192         ['conv2d_99[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 25, 25, 64)  192         ['conv2d_101[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 25, 25, 96)  288         ['conv2d_104[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 25, 25, 32)  96          ['conv2d_105[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 25, 25, 32)   0           ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)           (None, 25, 25, 256)  0           ['activation_85[0][0]',          \n",
      "                                                                  'activation_87[0][0]',          \n",
      "                                                                  'activation_90[0][0]',          \n",
      "                                                                  'activation_91[0][0]']          \n",
      "                                                                                                  \n",
      " global_average_pooling2d_26 (G  (None, 256)         0           ['mixed0[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_23 (Globa  (None, 256)         0           ['mixed0[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_46 (Reshape)           (None, 1, 1, 256)    0           ['global_average_pooling2d_26[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_47 (Reshape)           (None, 1, 1, 256)    0           ['global_max_pooling2d_23[0][0]']\n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 1, 1, 32)     8224        ['reshape_46[0][0]',             \n",
      "                                                                  'reshape_47[0][0]']             \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 1, 1, 256)    8448        ['dense_49[0][0]',               \n",
      "                                                                  'dense_49[1][0]']               \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 1, 1, 256)    0           ['dense_50[0][0]',               \n",
      "                                                                  'dense_50[1][0]']               \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 1, 1, 256)    0           ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_46 (Multiply)         (None, 25, 25, 256)  0           ['mixed0[0][0]',                 \n",
      "                                                                  'activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_46 (Lambda)             (None, 25, 25, 1)    0           ['multiply_46[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_47 (Lambda)             (None, 25, 25, 1)    0           ['multiply_46[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_74 (Concatenate)   (None, 25, 25, 2)    0           ['lambda_46[0][0]',              \n",
      "                                                                  'lambda_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 25, 25, 1)    98          ['concatenate_74[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_47 (Multiply)         (None, 25, 25, 256)  0           ['multiply_46[0][0]',            \n",
      "                                                                  'conv2d_106[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)            (None, 25, 25, 64)   16384       ['multiply_47[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_99 (BatchN  (None, 25, 25, 64)  192         ['conv2d_110[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_96 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_99[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)            (None, 25, 25, 48)   12288       ['multiply_47[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_96[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 25, 25, 48)  144         ['conv2d_108[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 25, 25, 96)  288         ['conv2d_111[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 25, 25, 48)   0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " activation_97 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_6 (AveragePo  (None, 25, 25, 256)  0          ['multiply_47[0][0]']            \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)            (None, 25, 25, 64)   16384       ['multiply_47[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_94[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_97[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)            (None, 25, 25, 64)   16384       ['average_pooling2d_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 25, 25, 64)  192         ['conv2d_107[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 25, 25, 64)  192         ['conv2d_109[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 25, 25, 96)  288         ['conv2d_112[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 25, 25, 64)  192         ['conv2d_113[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " activation_95 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " activation_98 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " activation_99 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)           (None, 25, 25, 288)  0           ['activation_93[0][0]',          \n",
      "                                                                  'activation_95[0][0]',          \n",
      "                                                                  'activation_98[0][0]',          \n",
      "                                                                  'activation_99[0][0]']          \n",
      "                                                                                                  \n",
      " global_average_pooling2d_27 (G  (None, 288)         0           ['mixed1[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_24 (Globa  (None, 288)         0           ['mixed1[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_48 (Reshape)           (None, 1, 1, 288)    0           ['global_average_pooling2d_27[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_49 (Reshape)           (None, 1, 1, 288)    0           ['global_max_pooling2d_24[0][0]']\n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 1, 1, 36)     10404       ['reshape_48[0][0]',             \n",
      "                                                                  'reshape_49[0][0]']             \n",
      "                                                                                                  \n",
      " dense_52 (Dense)               (None, 1, 1, 288)    10656       ['dense_51[0][0]',               \n",
      "                                                                  'dense_51[1][0]']               \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 1, 1, 288)    0           ['dense_52[0][0]',               \n",
      "                                                                  'dense_52[1][0]']               \n",
      "                                                                                                  \n",
      " activation_100 (Activation)    (None, 1, 1, 288)    0           ['add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_48 (Multiply)         (None, 25, 25, 288)  0           ['mixed1[0][0]',                 \n",
      "                                                                  'activation_100[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_48 (Lambda)             (None, 25, 25, 1)    0           ['multiply_48[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_49 (Lambda)             (None, 25, 25, 1)    0           ['multiply_48[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_75 (Concatenate)   (None, 25, 25, 2)    0           ['lambda_48[0][0]',              \n",
      "                                                                  'lambda_49[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)            (None, 25, 25, 1)    98          ['concatenate_75[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_49 (Multiply)         (None, 25, 25, 288)  0           ['multiply_48[0][0]',            \n",
      "                                                                  'conv2d_114[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_118 (Conv2D)            (None, 25, 25, 64)   18432       ['multiply_49[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 25, 25, 64)  192         ['conv2d_118[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_104 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_116 (Conv2D)            (None, 25, 25, 48)   13824       ['multiply_49[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_119 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_104[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_104 (Batch  (None, 25, 25, 48)  144         ['conv2d_116[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 25, 25, 96)  288         ['conv2d_119[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_102 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_104[0][0]']\n",
      "                                                                                                  \n",
      " activation_105 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_7 (AveragePo  (None, 25, 25, 288)  0          ['multiply_49[0][0]']            \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)            (None, 25, 25, 64)   18432       ['multiply_49[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_117 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_102[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_120 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_105[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_121 (Conv2D)            (None, 25, 25, 64)   18432       ['average_pooling2d_7[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_103 (Batch  (None, 25, 25, 64)  192         ['conv2d_115[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 25, 25, 64)  192         ['conv2d_117[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_108 (Batch  (None, 25, 25, 96)  288         ['conv2d_120[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_109 (Batch  (None, 25, 25, 64)  192         ['conv2d_121[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_101 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_103[0][0]']\n",
      "                                                                                                  \n",
      " activation_103 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " activation_106 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_108[0][0]']\n",
      "                                                                                                  \n",
      " activation_107 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_109[0][0]']\n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)           (None, 25, 25, 288)  0           ['activation_101[0][0]',         \n",
      "                                                                  'activation_103[0][0]',         \n",
      "                                                                  'activation_106[0][0]',         \n",
      "                                                                  'activation_107[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling2d_28 (G  (None, 288)         0           ['mixed2[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_25 (Globa  (None, 288)         0           ['mixed2[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_50 (Reshape)           (None, 1, 1, 288)    0           ['global_average_pooling2d_28[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_51 (Reshape)           (None, 1, 1, 288)    0           ['global_max_pooling2d_25[0][0]']\n",
      "                                                                                                  \n",
      " dense_53 (Dense)               (None, 1, 1, 36)     10404       ['reshape_50[0][0]',             \n",
      "                                                                  'reshape_51[0][0]']             \n",
      "                                                                                                  \n",
      " dense_54 (Dense)               (None, 1, 1, 288)    10656       ['dense_53[0][0]',               \n",
      "                                                                  'dense_53[1][0]']               \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 1, 1, 288)    0           ['dense_54[0][0]',               \n",
      "                                                                  'dense_54[1][0]']               \n",
      "                                                                                                  \n",
      " activation_108 (Activation)    (None, 1, 1, 288)    0           ['add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_50 (Multiply)         (None, 25, 25, 288)  0           ['mixed2[0][0]',                 \n",
      "                                                                  'activation_108[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_50 (Lambda)             (None, 25, 25, 1)    0           ['multiply_50[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_51 (Lambda)             (None, 25, 25, 1)    0           ['multiply_50[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_76 (Concatenate)   (None, 25, 25, 2)    0           ['lambda_50[0][0]',              \n",
      "                                                                  'lambda_51[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_122 (Conv2D)            (None, 25, 25, 1)    98          ['concatenate_76[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_51 (Multiply)         (None, 25, 25, 288)  0           ['multiply_50[0][0]',            \n",
      "                                                                  'conv2d_122[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_124 (Conv2D)            (None, 25, 25, 64)   18432       ['multiply_51[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 25, 25, 64)  192         ['conv2d_124[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_110 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_125 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_110[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 25, 25, 96)  288         ['conv2d_125[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_111 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_123 (Conv2D)            (None, 12, 12, 384)  995328      ['multiply_51[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_126 (Conv2D)            (None, 12, 12, 96)   82944       ['activation_111[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 12, 12, 384)  1152       ['conv2d_123[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_113 (Batch  (None, 12, 12, 96)  288         ['conv2d_126[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_109 (Activation)    (None, 12, 12, 384)  0           ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " activation_112 (Activation)    (None, 12, 12, 96)   0           ['batch_normalization_113[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0          ['multiply_51[0][0]']            \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)           (None, 12, 12, 768)  0           ['activation_109[0][0]',         \n",
      "                                                                  'activation_112[0][0]',         \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling2d_29 (G  (None, 768)         0           ['mixed3[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_26 (Globa  (None, 768)         0           ['mixed3[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_52 (Reshape)           (None, 1, 1, 768)    0           ['global_average_pooling2d_29[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_53 (Reshape)           (None, 1, 1, 768)    0           ['global_max_pooling2d_26[0][0]']\n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 1, 1, 96)     73824       ['reshape_52[0][0]',             \n",
      "                                                                  'reshape_53[0][0]']             \n",
      "                                                                                                  \n",
      " dense_56 (Dense)               (None, 1, 1, 768)    74496       ['dense_55[0][0]',               \n",
      "                                                                  'dense_55[1][0]']               \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 1, 1, 768)    0           ['dense_56[0][0]',               \n",
      "                                                                  'dense_56[1][0]']               \n",
      "                                                                                                  \n",
      " activation_113 (Activation)    (None, 1, 1, 768)    0           ['add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_52 (Multiply)         (None, 12, 12, 768)  0           ['mixed3[0][0]',                 \n",
      "                                                                  'activation_113[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_52 (Lambda)             (None, 12, 12, 1)    0           ['multiply_52[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_53 (Lambda)             (None, 12, 12, 1)    0           ['multiply_52[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_77 (Concatenate)   (None, 12, 12, 2)    0           ['lambda_52[0][0]',              \n",
      "                                                                  'lambda_53[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_127 (Conv2D)            (None, 12, 12, 1)    98          ['concatenate_77[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_53 (Multiply)         (None, 12, 12, 768)  0           ['multiply_52[0][0]',            \n",
      "                                                                  'conv2d_127[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_132 (Conv2D)            (None, 12, 12, 128)  98304       ['multiply_53[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_118 (Batch  (None, 12, 12, 128)  384        ['conv2d_132[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_118 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_118[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_133 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_118[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_119 (Batch  (None, 12, 12, 128)  384        ['conv2d_133[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_119 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_119[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_129 (Conv2D)            (None, 12, 12, 128)  98304       ['multiply_53[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_134 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_119[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 12, 12, 128)  384        ['conv2d_129[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 12, 12, 128)  384        ['conv2d_134[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_115 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " activation_120 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_130 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_115[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_135 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_120[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 12, 12, 128)  384        ['conv2d_130[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 12, 12, 128)  384        ['conv2d_135[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_116 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " activation_121 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_8 (AveragePo  (None, 12, 12, 768)  0          ['multiply_53[0][0]']            \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_128 (Conv2D)            (None, 12, 12, 192)  147456      ['multiply_53[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_131 (Conv2D)            (None, 12, 12, 192)  172032      ['activation_116[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_136 (Conv2D)            (None, 12, 12, 192)  172032      ['activation_121[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_137 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_114 (Batch  (None, 12, 12, 192)  576        ['conv2d_128[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 12, 12, 192)  576        ['conv2d_131[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 12, 12, 192)  576        ['conv2d_136[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_123 (Batch  (None, 12, 12, 192)  576        ['conv2d_137[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_114 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_114[0][0]']\n",
      "                                                                                                  \n",
      " activation_117 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " activation_122 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " activation_123 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_123[0][0]']\n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)           (None, 12, 12, 768)  0           ['activation_114[0][0]',         \n",
      "                                                                  'activation_117[0][0]',         \n",
      "                                                                  'activation_122[0][0]',         \n",
      "                                                                  'activation_123[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling2d_30 (G  (None, 768)         0           ['mixed4[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_27 (Globa  (None, 768)         0           ['mixed4[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_54 (Reshape)           (None, 1, 1, 768)    0           ['global_average_pooling2d_30[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_55 (Reshape)           (None, 1, 1, 768)    0           ['global_max_pooling2d_27[0][0]']\n",
      "                                                                                                  \n",
      " dense_57 (Dense)               (None, 1, 1, 96)     73824       ['reshape_54[0][0]',             \n",
      "                                                                  'reshape_55[0][0]']             \n",
      "                                                                                                  \n",
      " dense_58 (Dense)               (None, 1, 1, 768)    74496       ['dense_57[0][0]',               \n",
      "                                                                  'dense_57[1][0]']               \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 1, 1, 768)    0           ['dense_58[0][0]',               \n",
      "                                                                  'dense_58[1][0]']               \n",
      "                                                                                                  \n",
      " activation_124 (Activation)    (None, 1, 1, 768)    0           ['add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_54 (Multiply)         (None, 12, 12, 768)  0           ['mixed4[0][0]',                 \n",
      "                                                                  'activation_124[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_54 (Lambda)             (None, 12, 12, 1)    0           ['multiply_54[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_55 (Lambda)             (None, 12, 12, 1)    0           ['multiply_54[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_78 (Concatenate)   (None, 12, 12, 2)    0           ['lambda_54[0][0]',              \n",
      "                                                                  'lambda_55[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_138 (Conv2D)            (None, 12, 12, 1)    98          ['concatenate_78[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_55 (Multiply)         (None, 12, 12, 768)  0           ['multiply_54[0][0]',            \n",
      "                                                                  'conv2d_138[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_143 (Conv2D)            (None, 12, 12, 160)  122880      ['multiply_55[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_128 (Batch  (None, 12, 12, 160)  480        ['conv2d_143[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_129 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_128[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_144 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_129[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_129 (Batch  (None, 12, 12, 160)  480        ['conv2d_144[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_130 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_129[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_140 (Conv2D)            (None, 12, 12, 160)  122880      ['multiply_55[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_145 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_130[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 12, 12, 160)  480        ['conv2d_140[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 12, 12, 160)  480        ['conv2d_145[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_126 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " activation_131 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_141 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_126[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_146 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_131[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 12, 12, 160)  480        ['conv2d_141[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 12, 12, 160)  480        ['conv2d_146[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_127 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " activation_132 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_9 (AveragePo  (None, 12, 12, 768)  0          ['multiply_55[0][0]']            \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_139 (Conv2D)            (None, 12, 12, 192)  147456      ['multiply_55[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_142 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_127[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_147 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_132[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_148 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_9[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_124 (Batch  (None, 12, 12, 192)  576        ['conv2d_139[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 12, 12, 192)  576        ['conv2d_142[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 12, 12, 192)  576        ['conv2d_147[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_133 (Batch  (None, 12, 12, 192)  576        ['conv2d_148[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_125 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_124[0][0]']\n",
      "                                                                                                  \n",
      " activation_128 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " activation_133 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " activation_134 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_133[0][0]']\n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)           (None, 12, 12, 768)  0           ['activation_125[0][0]',         \n",
      "                                                                  'activation_128[0][0]',         \n",
      "                                                                  'activation_133[0][0]',         \n",
      "                                                                  'activation_134[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling2d_31 (G  (None, 768)         0           ['mixed5[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_28 (Globa  (None, 768)         0           ['mixed5[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_56 (Reshape)           (None, 1, 1, 768)    0           ['global_average_pooling2d_31[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_57 (Reshape)           (None, 1, 1, 768)    0           ['global_max_pooling2d_28[0][0]']\n",
      "                                                                                                  \n",
      " dense_59 (Dense)               (None, 1, 1, 96)     73824       ['reshape_56[0][0]',             \n",
      "                                                                  'reshape_57[0][0]']             \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 1, 1, 768)    74496       ['dense_59[0][0]',               \n",
      "                                                                  'dense_59[1][0]']               \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 1, 1, 768)    0           ['dense_60[0][0]',               \n",
      "                                                                  'dense_60[1][0]']               \n",
      "                                                                                                  \n",
      " activation_135 (Activation)    (None, 1, 1, 768)    0           ['add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_56 (Multiply)         (None, 12, 12, 768)  0           ['mixed5[0][0]',                 \n",
      "                                                                  'activation_135[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_56 (Lambda)             (None, 12, 12, 1)    0           ['multiply_56[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_57 (Lambda)             (None, 12, 12, 1)    0           ['multiply_56[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_79 (Concatenate)   (None, 12, 12, 2)    0           ['lambda_56[0][0]',              \n",
      "                                                                  'lambda_57[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_149 (Conv2D)            (None, 12, 12, 1)    98          ['concatenate_79[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_57 (Multiply)         (None, 12, 12, 768)  0           ['multiply_56[0][0]',            \n",
      "                                                                  'conv2d_149[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_154 (Conv2D)            (None, 12, 12, 160)  122880      ['multiply_57[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_138 (Batch  (None, 12, 12, 160)  480        ['conv2d_154[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_140 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_138[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_155 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_140[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_139 (Batch  (None, 12, 12, 160)  480        ['conv2d_155[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_141 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_139[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_151 (Conv2D)            (None, 12, 12, 160)  122880      ['multiply_57[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_156 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_141[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_135 (Batch  (None, 12, 12, 160)  480        ['conv2d_151[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_140 (Batch  (None, 12, 12, 160)  480        ['conv2d_156[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_137 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " activation_142 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_140[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_152 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_137[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_157 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_142[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_136 (Batch  (None, 12, 12, 160)  480        ['conv2d_152[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_141 (Batch  (None, 12, 12, 160)  480        ['conv2d_157[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_138 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      " activation_143 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_141[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_10 (AverageP  (None, 12, 12, 768)  0          ['multiply_57[0][0]']            \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_150 (Conv2D)            (None, 12, 12, 192)  147456      ['multiply_57[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_153 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_138[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_158 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_143[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_159 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_10[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_134 (Batch  (None, 12, 12, 192)  576        ['conv2d_150[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_137 (Batch  (None, 12, 12, 192)  576        ['conv2d_153[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_142 (Batch  (None, 12, 12, 192)  576        ['conv2d_158[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_143 (Batch  (None, 12, 12, 192)  576        ['conv2d_159[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_136 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_134[0][0]']\n",
      "                                                                                                  \n",
      " activation_139 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_137[0][0]']\n",
      "                                                                                                  \n",
      " activation_144 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_142[0][0]']\n",
      "                                                                                                  \n",
      " activation_145 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_143[0][0]']\n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)           (None, 12, 12, 768)  0           ['activation_136[0][0]',         \n",
      "                                                                  'activation_139[0][0]',         \n",
      "                                                                  'activation_144[0][0]',         \n",
      "                                                                  'activation_145[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling2d_32 (G  (None, 768)         0           ['mixed6[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_29 (Globa  (None, 768)         0           ['mixed6[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_58 (Reshape)           (None, 1, 1, 768)    0           ['global_average_pooling2d_32[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_59 (Reshape)           (None, 1, 1, 768)    0           ['global_max_pooling2d_29[0][0]']\n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 1, 1, 96)     73824       ['reshape_58[0][0]',             \n",
      "                                                                  'reshape_59[0][0]']             \n",
      "                                                                                                  \n",
      " dense_62 (Dense)               (None, 1, 1, 768)    74496       ['dense_61[0][0]',               \n",
      "                                                                  'dense_61[1][0]']               \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 1, 1, 768)    0           ['dense_62[0][0]',               \n",
      "                                                                  'dense_62[1][0]']               \n",
      "                                                                                                  \n",
      " activation_146 (Activation)    (None, 1, 1, 768)    0           ['add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_58 (Multiply)         (None, 12, 12, 768)  0           ['mixed6[0][0]',                 \n",
      "                                                                  'activation_146[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_58 (Lambda)             (None, 12, 12, 1)    0           ['multiply_58[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_59 (Lambda)             (None, 12, 12, 1)    0           ['multiply_58[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_80 (Concatenate)   (None, 12, 12, 2)    0           ['lambda_58[0][0]',              \n",
      "                                                                  'lambda_59[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_160 (Conv2D)            (None, 12, 12, 1)    98          ['concatenate_80[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_59 (Multiply)         (None, 12, 12, 768)  0           ['multiply_58[0][0]',            \n",
      "                                                                  'conv2d_160[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_165 (Conv2D)            (None, 12, 12, 192)  147456      ['multiply_59[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_148 (Batch  (None, 12, 12, 192)  576        ['conv2d_165[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_151 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_148[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_166 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_151[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_149 (Batch  (None, 12, 12, 192)  576        ['conv2d_166[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_152 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_149[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)            (None, 12, 12, 192)  147456      ['multiply_59[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_167 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_152[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 12, 12, 192)  576        ['conv2d_162[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_150 (Batch  (None, 12, 12, 192)  576        ['conv2d_167[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_148 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " activation_153 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_163 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_148[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_153[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 12, 12, 192)  576        ['conv2d_163[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_151 (Batch  (None, 12, 12, 192)  576        ['conv2d_168[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_149 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " activation_154 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_11 (AverageP  (None, 12, 12, 768)  0          ['multiply_59[0][0]']            \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_161 (Conv2D)            (None, 12, 12, 192)  147456      ['multiply_59[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_164 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_149[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_154[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_11[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_144 (Batch  (None, 12, 12, 192)  576        ['conv2d_161[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_147 (Batch  (None, 12, 12, 192)  576        ['conv2d_164[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_152 (Batch  (None, 12, 12, 192)  576        ['conv2d_169[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_153 (Batch  (None, 12, 12, 192)  576        ['conv2d_170[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_147 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_144[0][0]']\n",
      "                                                                                                  \n",
      " activation_150 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_147[0][0]']\n",
      "                                                                                                  \n",
      " activation_155 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " activation_156 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_153[0][0]']\n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)           (None, 12, 12, 768)  0           ['activation_147[0][0]',         \n",
      "                                                                  'activation_150[0][0]',         \n",
      "                                                                  'activation_155[0][0]',         \n",
      "                                                                  'activation_156[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling2d_33 (G  (None, 768)         0           ['mixed7[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_30 (Globa  (None, 768)         0           ['mixed7[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_60 (Reshape)           (None, 1, 1, 768)    0           ['global_average_pooling2d_33[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_61 (Reshape)           (None, 1, 1, 768)    0           ['global_max_pooling2d_30[0][0]']\n",
      "                                                                                                  \n",
      " dense_63 (Dense)               (None, 1, 1, 96)     73824       ['reshape_60[0][0]',             \n",
      "                                                                  'reshape_61[0][0]']             \n",
      "                                                                                                  \n",
      " dense_64 (Dense)               (None, 1, 1, 768)    74496       ['dense_63[0][0]',               \n",
      "                                                                  'dense_63[1][0]']               \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 1, 1, 768)    0           ['dense_64[0][0]',               \n",
      "                                                                  'dense_64[1][0]']               \n",
      "                                                                                                  \n",
      " activation_157 (Activation)    (None, 1, 1, 768)    0           ['add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_60 (Multiply)         (None, 12, 12, 768)  0           ['mixed7[0][0]',                 \n",
      "                                                                  'activation_157[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_60 (Lambda)             (None, 12, 12, 1)    0           ['multiply_60[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_61 (Lambda)             (None, 12, 12, 1)    0           ['multiply_60[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_81 (Concatenate)   (None, 12, 12, 2)    0           ['lambda_60[0][0]',              \n",
      "                                                                  'lambda_61[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)            (None, 12, 12, 1)    98          ['concatenate_81[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_61 (Multiply)         (None, 12, 12, 768)  0           ['multiply_60[0][0]',            \n",
      "                                                                  'conv2d_171[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)            (None, 12, 12, 192)  147456      ['multiply_61[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_156 (Batch  (None, 12, 12, 192)  576        ['conv2d_174[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_160 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_160[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 12, 12, 192)  576        ['conv2d_175[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_161 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)            (None, 12, 12, 192)  147456      ['multiply_61[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_161[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_154 (Batch  (None, 12, 12, 192)  576        ['conv2d_172[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_158 (Batch  (None, 12, 12, 192)  576        ['conv2d_176[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_158 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_154[0][0]']\n",
      "                                                                                                  \n",
      " activation_162 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_158[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)            (None, 5, 5, 320)    552960      ['activation_158[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)            (None, 5, 5, 192)    331776      ['activation_162[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_155 (Batch  (None, 5, 5, 320)   960         ['conv2d_173[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_159 (Batch  (None, 5, 5, 192)   576         ['conv2d_177[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_159 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " activation_163 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_159[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)   0           ['multiply_61[0][0]']            \n",
      "                                                                                                  \n",
      " mixed8 (Concatenate)           (None, 5, 5, 1280)   0           ['activation_159[0][0]',         \n",
      "                                                                  'activation_163[0][0]',         \n",
      "                                                                  'max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling2d_34 (G  (None, 1280)        0           ['mixed8[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_31 (Globa  (None, 1280)        0           ['mixed8[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_62 (Reshape)           (None, 1, 1, 1280)   0           ['global_average_pooling2d_34[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_63 (Reshape)           (None, 1, 1, 1280)   0           ['global_max_pooling2d_31[0][0]']\n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 1, 1, 160)    204960      ['reshape_62[0][0]',             \n",
      "                                                                  'reshape_63[0][0]']             \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 1, 1, 1280)   206080      ['dense_65[0][0]',               \n",
      "                                                                  'dense_65[1][0]']               \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 1, 1, 1280)   0           ['dense_66[0][0]',               \n",
      "                                                                  'dense_66[1][0]']               \n",
      "                                                                                                  \n",
      " activation_164 (Activation)    (None, 1, 1, 1280)   0           ['add_31[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_62 (Multiply)         (None, 5, 5, 1280)   0           ['mixed8[0][0]',                 \n",
      "                                                                  'activation_164[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_62 (Lambda)             (None, 5, 5, 1)      0           ['multiply_62[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_63 (Lambda)             (None, 5, 5, 1)      0           ['multiply_62[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_82 (Concatenate)   (None, 5, 5, 2)      0           ['lambda_62[0][0]',              \n",
      "                                                                  'lambda_63[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)            (None, 5, 5, 1)      98          ['concatenate_82[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_63 (Multiply)         (None, 5, 5, 1280)   0           ['multiply_62[0][0]',            \n",
      "                                                                  'conv2d_178[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_183 (Conv2D)            (None, 5, 5, 448)    573440      ['multiply_63[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_164 (Batch  (None, 5, 5, 448)   1344        ['conv2d_183[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_169 (Activation)    (None, 5, 5, 448)    0           ['batch_normalization_164[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)            (None, 5, 5, 384)    491520      ['multiply_63[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_184 (Conv2D)            (None, 5, 5, 384)    1548288     ['activation_169[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 5, 5, 384)   1152        ['conv2d_180[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 5, 5, 384)   1152        ['conv2d_184[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_166 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " activation_170 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_181 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_166[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_182 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_166[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_185 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_170[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_186 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_170[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_12 (AverageP  (None, 5, 5, 1280)  0           ['multiply_63[0][0]']            \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)            (None, 5, 5, 320)    409600      ['multiply_63[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 5, 5, 384)   1152        ['conv2d_181[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_163 (Batch  (None, 5, 5, 384)   1152        ['conv2d_182[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 5, 5, 384)   1152        ['conv2d_185[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 5, 5, 384)   1152        ['conv2d_186[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_187 (Conv2D)            (None, 5, 5, 192)    245760      ['average_pooling2d_12[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 5, 5, 320)   960         ['conv2d_179[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_167 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " activation_168 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_163[0][0]']\n",
      "                                                                                                  \n",
      " activation_171 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " activation_172 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_168 (Batch  (None, 5, 5, 192)   576         ['conv2d_187[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_165 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " mixed9_0 (Concatenate)         (None, 5, 5, 768)    0           ['activation_167[0][0]',         \n",
      "                                                                  'activation_168[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_83 (Concatenate)   (None, 5, 5, 768)    0           ['activation_171[0][0]',         \n",
      "                                                                  'activation_172[0][0]']         \n",
      "                                                                                                  \n",
      " activation_173 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_168[0][0]']\n",
      "                                                                                                  \n",
      " mixed9 (Concatenate)           (None, 5, 5, 2048)   0           ['activation_165[0][0]',         \n",
      "                                                                  'mixed9_0[0][0]',               \n",
      "                                                                  'concatenate_83[0][0]',         \n",
      "                                                                  'activation_173[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling2d_35 (G  (None, 2048)        0           ['mixed9[0][0]']                 \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_32 (Globa  (None, 2048)        0           ['mixed9[0][0]']                 \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_64 (Reshape)           (None, 1, 1, 2048)   0           ['global_average_pooling2d_35[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_65 (Reshape)           (None, 1, 1, 2048)   0           ['global_max_pooling2d_32[0][0]']\n",
      "                                                                                                  \n",
      " dense_67 (Dense)               (None, 1, 1, 256)    524544      ['reshape_64[0][0]',             \n",
      "                                                                  'reshape_65[0][0]']             \n",
      "                                                                                                  \n",
      " dense_68 (Dense)               (None, 1, 1, 2048)   526336      ['dense_67[0][0]',               \n",
      "                                                                  'dense_67[1][0]']               \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 1, 1, 2048)   0           ['dense_68[0][0]',               \n",
      "                                                                  'dense_68[1][0]']               \n",
      "                                                                                                  \n",
      " activation_174 (Activation)    (None, 1, 1, 2048)   0           ['add_32[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_64 (Multiply)         (None, 5, 5, 2048)   0           ['mixed9[0][0]',                 \n",
      "                                                                  'activation_174[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_64 (Lambda)             (None, 5, 5, 1)      0           ['multiply_64[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_65 (Lambda)             (None, 5, 5, 1)      0           ['multiply_64[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_84 (Concatenate)   (None, 5, 5, 2)      0           ['lambda_64[0][0]',              \n",
      "                                                                  'lambda_65[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_188 (Conv2D)            (None, 5, 5, 1)      98          ['concatenate_84[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_65 (Multiply)         (None, 5, 5, 2048)   0           ['multiply_64[0][0]',            \n",
      "                                                                  'conv2d_188[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_193 (Conv2D)            (None, 5, 5, 448)    917504      ['multiply_65[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_173 (Batch  (None, 5, 5, 448)   1344        ['conv2d_193[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_179 (Activation)    (None, 5, 5, 448)    0           ['batch_normalization_173[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_190 (Conv2D)            (None, 5, 5, 384)    786432      ['multiply_65[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_194 (Conv2D)            (None, 5, 5, 384)    1548288     ['activation_179[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 5, 5, 384)   1152        ['conv2d_190[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_174 (Batch  (None, 5, 5, 384)   1152        ['conv2d_194[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_176 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " activation_180 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_174[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_191 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_176[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_192 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_176[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_195 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_180[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_196 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_180[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_13 (AverageP  (None, 5, 5, 2048)  0           ['multiply_65[0][0]']            \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_189 (Conv2D)            (None, 5, 5, 320)    655360      ['multiply_65[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 5, 5, 384)   1152        ['conv2d_191[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 5, 5, 384)   1152        ['conv2d_192[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 5, 5, 384)   1152        ['conv2d_195[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 5, 5, 384)   1152        ['conv2d_196[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_197 (Conv2D)            (None, 5, 5, 192)    393216      ['average_pooling2d_13[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_169 (Batch  (None, 5, 5, 320)   960         ['conv2d_189[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_177 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " activation_178 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " activation_181 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " activation_182 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 5, 5, 192)   576         ['conv2d_197[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_175 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_169[0][0]']\n",
      "                                                                                                  \n",
      " mixed9_1 (Concatenate)         (None, 5, 5, 768)    0           ['activation_177[0][0]',         \n",
      "                                                                  'activation_178[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_85 (Concatenate)   (None, 5, 5, 768)    0           ['activation_181[0][0]',         \n",
      "                                                                  'activation_182[0][0]']         \n",
      "                                                                                                  \n",
      " activation_183 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " mixed10 (Concatenate)          (None, 5, 5, 2048)   0           ['activation_175[0][0]',         \n",
      "                                                                  'mixed9_1[0][0]',               \n",
      "                                                                  'concatenate_85[0][0]',         \n",
      "                                                                  'activation_183[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling2d_36 (G  (None, 2048)        0           ['mixed10[0][0]']                \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " global_max_pooling2d_33 (Globa  (None, 2048)        0           ['mixed10[0][0]']                \n",
      " lMaxPooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " reshape_66 (Reshape)           (None, 1, 1, 2048)   0           ['global_average_pooling2d_36[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " reshape_67 (Reshape)           (None, 1, 1, 2048)   0           ['global_max_pooling2d_33[0][0]']\n",
      "                                                                                                  \n",
      " dense_69 (Dense)               (None, 1, 1, 256)    524544      ['reshape_66[0][0]',             \n",
      "                                                                  'reshape_67[0][0]']             \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 1, 1, 2048)   526336      ['dense_69[0][0]',               \n",
      "                                                                  'dense_69[1][0]']               \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 1, 1, 2048)   0           ['dense_70[0][0]',               \n",
      "                                                                  'dense_70[1][0]']               \n",
      "                                                                                                  \n",
      " activation_184 (Activation)    (None, 1, 1, 2048)   0           ['add_33[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_66 (Multiply)         (None, 5, 5, 2048)   0           ['mixed10[0][0]',                \n",
      "                                                                  'activation_184[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_66 (Lambda)             (None, 5, 5, 1)      0           ['multiply_66[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_67 (Lambda)             (None, 5, 5, 1)      0           ['multiply_66[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_86 (Concatenate)   (None, 5, 5, 2)      0           ['lambda_66[0][0]',              \n",
      "                                                                  'lambda_67[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_198 (Conv2D)            (None, 5, 5, 1)      98          ['concatenate_86[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_67 (Multiply)         (None, 5, 5, 2048)   0           ['multiply_66[0][0]',            \n",
      "                                                                  'conv2d_198[0][0]']             \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 2048)        0           ['multiply_67[0][0]']            \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " predictions (Dense)            (None, 140)          286860      ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,403,914\n",
      "Trainable params: 25,369,482\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "resnet20_cbam_block\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35/35 [==============================] - 225s 6s/step - loss: 4.8958 - accuracy: 0.0071 - val_loss: 4.9725 - val_accuracy: 0.0071\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 196s 6s/step - loss: 4.5884 - accuracy: 0.0080 - val_loss: 5.0594 - val_accuracy: 0.0125\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 194s 6s/step - loss: 4.4441 - accuracy: 0.0116 - val_loss: 11.7130 - val_accuracy: 0.0071\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 191s 5s/step - loss: 4.3738 - accuracy: 0.0134 - val_loss: 49.7865 - val_accuracy: 0.0071\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 198s 6s/step - loss: 4.2317 - accuracy: 0.0241 - val_loss: 13.3398 - val_accuracy: 0.0071\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 202s 6s/step - loss: 4.0258 - accuracy: 0.0384 - val_loss: 28.6872 - val_accuracy: 0.0071\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 233s 7s/step - loss: 3.9281 - accuracy: 0.0411 - val_loss: 19.3899 - val_accuracy: 0.0071\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 341s 10s/step - loss: 3.7910 - accuracy: 0.0455 - val_loss: 29.9968 - val_accuracy: 0.0125\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 365s 10s/step - loss: 3.6502 - accuracy: 0.0473 - val_loss: 18.4129 - val_accuracy: 0.0161\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 342s 10s/step - loss: 3.5012 - accuracy: 0.0607 - val_loss: 41.0254 - val_accuracy: 0.0196\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 363s 10s/step - loss: 3.4338 - accuracy: 0.0652 - val_loss: 6.3921 - val_accuracy: 0.0411\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 360s 10s/step - loss: 3.2668 - accuracy: 0.1018 - val_loss: 44.4936 - val_accuracy: 0.0161\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 343s 10s/step - loss: 3.0848 - accuracy: 0.1429 - val_loss: 12.9947 - val_accuracy: 0.0321\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 364s 10s/step - loss: 2.7815 - accuracy: 0.1723 - val_loss: 17.9334 - val_accuracy: 0.0357\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 370s 11s/step - loss: 2.7249 - accuracy: 0.2107 - val_loss: 17.4119 - val_accuracy: 0.0304\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 745s 22s/step - loss: 2.4345 - accuracy: 0.2607 - val_loss: 44.0366 - val_accuracy: 0.0089\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 849s 24s/step - loss: 2.2067 - accuracy: 0.3018 - val_loss: 38.3941 - val_accuracy: 0.0107\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 984s 28s/step - loss: 1.9851 - accuracy: 0.3696 - val_loss: 48.4635 - val_accuracy: 0.0161\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 999s 29s/step - loss: 1.8696 - accuracy: 0.4062 - val_loss: 38.6753 - val_accuracy: 0.0196\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 1157s 32s/step - loss: 1.6284 - accuracy: 0.4821 - val_loss: 30.0608 - val_accuracy: 0.0286\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 1461s 42s/step - loss: 1.4415 - accuracy: 0.5518 - val_loss: 10.7262 - val_accuracy: 0.0821\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 1516s 44s/step - loss: 1.3303 - accuracy: 0.5759 - val_loss: 8.1501 - val_accuracy: 0.1054\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 1316s 38s/step - loss: 1.1795 - accuracy: 0.6205 - val_loss: 20.1104 - val_accuracy: 0.0375\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 1526s 43s/step - loss: 0.9191 - accuracy: 0.7134 - val_loss: 9.6969 - val_accuracy: 0.1232\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 1624s 47s/step - loss: 0.9356 - accuracy: 0.6955 - val_loss: 4.6417 - val_accuracy: 0.2429\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 1758s 50s/step - loss: 0.7157 - accuracy: 0.7848 - val_loss: 8.6311 - val_accuracy: 0.1107\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 1859s 54s/step - loss: 0.7075 - accuracy: 0.7795 - val_loss: 6.1949 - val_accuracy: 0.2018\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 1635s 47s/step - loss: 0.6430 - accuracy: 0.8179 - val_loss: 6.4159 - val_accuracy: 0.2286\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 1867s 53s/step - loss: 0.5496 - accuracy: 0.8241 - val_loss: 3.4087 - val_accuracy: 0.3518\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 1830s 52s/step - loss: 0.4175 - accuracy: 0.8759 - val_loss: 3.2982 - val_accuracy: 0.3339\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 1787s 50s/step - loss: 0.3967 - accuracy: 0.8777 - val_loss: 3.8639 - val_accuracy: 0.2911\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 1878s 54s/step - loss: 0.3580 - accuracy: 0.9098 - val_loss: 4.0552 - val_accuracy: 0.3536\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.9125 "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=50,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 347s 19s/step - loss: 2.3496 - accuracy: 0.4464\n",
      "Test loss: 2.3496310710906982\n",
      "Test accuracy: 0.4464285671710968\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few_shot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
